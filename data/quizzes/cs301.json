[
    {
        "question": "Which of the following best describes the concept of an 'Abstract Data Type' (ADT) as presented in the CS301 curriculum?",
        "options": [
            "A specific implementation of a data structure in C++ using classes.",
            "A mathematical model of a data structure that specifies operations but not implementation.",
            "A memory management technique used to allocate space for arrays.",
            "A physical representation of data stored on a secondary storage device."
        ],
        "correct": 1,
        "explanation": "An Abstract Data Type (ADT) is a high-level description of a data structure that focuses on 'what' the data structure does rather than 'how' it is implemented. It defines the data stored and the operations allowed on that data (like push/pop for a stack), but it hides the underlying logic, such as whether an array or a linked list is being used. Options 0 and 2 are incorrect because they refer to specific implementations or memory techniques rather than the conceptual model.",
        "topic": "Introduction to ADTs"
    },
    {
        "question": "In a Singly Linked List, what is the primary disadvantage of using an array-based implementation compared to a pointer-based implementation?",
        "options": [
            "Arrays provide faster access to the 'next' element in the sequence.",
            "Arrays require a fixed size to be defined at compile-time, leading to potential overflow or memory waste.",
            "Arrays cannot store complex data types like objects or structures.",
            "Arrays use less memory because they do not require extra space for pointers."
        ],
        "correct": 1,
        "explanation": "The most significant limitation of array-based lists is their static nature; the size must be predetermined. If the number of elements exceeds the array size, the program may crash or require expensive reallocation. Pointer-based linked lists, however, allocate memory dynamically as needed. While arrays do provide faster O(1) access (Option 0) and use less memory per node (Option 3), these are advantages, not disadvantages, in the specific context of flexibility and growth.",
        "topic": "Linked Lists"
    },
    {
        "question": "What is the time complexity of inserting a new node at the head of a Singly Linked List, assuming we have a pointer to the head?",
        "options": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ],
        "correct": 0,
        "explanation": "Inserting at the head of a linked list is a constant time operation, O(1), because it only involves creating a new node, setting its 'next' pointer to the current head, and updating the head pointer. This process does not depend on the number of elements (n) in the list. O(n) would only be required if we needed to traverse to the end of the list to perform an insertion without a tail pointer.",
        "topic": "Linked List Operations"
    },
    {
        "question": "Which of the following data structures follows the 'Last-In, First-Out' (LIFO) principle?",
        "options": [
            "Queue",
            "Linked List",
            "Stack",
            "Binary Search Tree"
        ],
        "correct": 2,
        "explanation": "A Stack is defined by the LIFO principle, where the most recently added element is the first one to be removed. Think of a stack of plates; you add to the top and remove from the top. A Queue (Option 0) follows FIFO (First-In, First-Out), while Linked Lists and Trees (Options 1 and 3) are more general structures that do not strictly enforce a single entry/exit rule for all operations.",
        "topic": "Stack ADT"
    },
    {
        "question": "Consider a stack implemented using a pointer-based linked list. When performing a 'pop' operation, which of the following steps is logically necessary to prevent a memory leak?",
        "options": [
            "Simply move the head pointer to head->next.",
            "Set the head pointer to NULL.",
            "Store the address of the current top in a temporary pointer and use 'delete' on it after updating head.",
            "The C++ compiler automatically deletes the node when the head pointer moves."
        ],
        "correct": 2,
        "explanation": "In C++, dynamic memory allocated with 'new' must be manually released using 'delete'. If you only move the head pointer (Option 0), the memory occupied by the original top node becomes unreachable but is still allocated, causing a memory leak. Option 3 describes the correct procedure: hold the pointer, move the head, then free the memory. C++ does not have automatic garbage collection for raw pointers (Option 3).",
        "topic": "Stack Implementation"
    },
    {
        "question": "In the context of the Stack ADT, what does the 'peek' (or top) operation return?",
        "options": [
            "It removes and returns the top element of the stack.",
            "It returns the value of the top element without removing it from the stack.",
            "It returns the total number of elements currently in the stack.",
            "It returns the memory address of the bottom-most element."
        ],
        "correct": 1,
        "explanation": "The 'peek' or 'top' operation is designed to inspect the data at the top of the stack without altering the stack's state. The 'pop' operation (Option 0) is what removes the element. Option 2 describes a 'size' operation, and Option 3 is generally not a standard stack operation as stacks only provide access to the top.",
        "topic": "Stack Operations"
    },
    {
        "question": "What happens if a 'pop' operation is called on an empty stack?",
        "options": [
            "The stack size becomes negative.",
            "The operation is ignored by the CPU.",
            "A 'Stack Underflow' condition occurs, which must be handled by the program.",
            "The stack automatically initializes with a default value of zero."
        ],
        "correct": 2,
        "explanation": "Attempting to remove an element from an empty stack is a logical error known as 'Stack Underflow'. Robust ADT implementations should check 'isEmpty()' before popping to prevent accessing null pointers or illegal memory. Option 0 is impossible as size cannot be negative, and Option 3 is incorrect as stacks do not self-initialize during a pop.",
        "topic": "Stack ADT"
    },
    {
        "question": "Which of the following is a primary application of the Stack data structure in computer systems?",
        "options": [
            "Managing print jobs in a printer buffer.",
            "Evaluating arithmetic expressions and managing function calls (activation records).",
            "Broadcasting data packets in a computer network.",
            "Storing records in a database for random access."
        ],
        "correct": 1,
        "explanation": "Stacks are fundamental for expression evaluation (postfix/prefix) and the implementation of function calls. When a function is called, its state is pushed onto the 'call stack' and popped when the function returns. Print jobs (Option 0) use Queues (FIFO), and random access (Option 3) is a characteristic of arrays or hash tables, not stacks.",
        "topic": "Applications of Stacks"
    },
    {
        "question": "In a Doubly Linked List, how many pointer fields are required in each node?",
        "options": [
            "One (Next)",
            "Two (Next and Previous)",
            "Three (Next, Previous, and Data)",
            "None, it uses indices."
        ],
        "correct": 1,
        "explanation": "A Doubly Linked List node contains two pointer fields: one to the next node and one to the previous node. This allows for bidirectional traversal. While the node also contains a 'data' field (Option 2), 'data' is not a pointer field; it's the actual payload. Therefore, specifically regarding 'pointer fields', two is the correct answer.",
        "topic": "Doubly Linked List"
    },
    {
        "question": "When implementing a Queue using a circular array, why is one space often left empty?",
        "options": [
            "To store the total size of the queue.",
            "To distinguish between the 'Queue Full' and 'Queue Empty' conditions.",
            "As a buffer to prevent array indices from becoming negative.",
            "To store the address of the array in memory."
        ],
        "correct": 1,
        "explanation": "In a circular array implementation of a queue, both the 'Full' and 'Empty' states could result in the 'front' and 'rear' indices being equal if all spaces are used. By leaving one slot empty, 'front == rear' uniquely identifies an empty queue, while '(rear + 1) % size == front' identifies a full queue. This avoids the need for an additional boolean flag to track state.",
        "topic": "Queue ADT"
    },
    {
        "question": "Which of the following is a mandatory requirement for a recursive function to terminate and avoid infinite execution?",
        "options": [
            "The function must call itself at least twice.",
            "The function must have a base case that does not involve a recursive call.",
            "The function must be declared with a 'static' return type.",
            "The function must use a 'while' loop instead of an 'if' statement."
        ],
        "correct": 1,
        "explanation": "Recursion works by breaking a problem into smaller sub-problems. A base case is essential because it provides the stopping condition. Without it, the function would continue to push activation records onto the system stack until a stack overflow occurs. Options 0, 2, and 3 are either irrelevant or technically incorrect regarding the fundamental logic of recursion.",
        "topic": "Recursion"
    },
    {
        "question": "What is the maximum number of nodes in a binary tree of height 'h'? (Assume the root is at height 0).",
        "options": [
            "2^h",
            "2^(h+1) - 1",
            "2^h - 1",
            "h^2"
        ],
        "correct": 1,
        "explanation": "In a perfect binary tree, each level 'i' has 2^i nodes. The total number of nodes is the sum of a geometric series: 2^0 + 2^1 + ... + 2^h. This sums to 2^(h+1) - 1. For example, if height is 2, nodes = 2^(2+1)-1 = 7. Option 2 is a common mistake that excludes the root level.",
        "topic": "Binary Trees"
    },
    {
        "question": "In a Binary Search Tree (BST), which traversal method results in the output of node values in a strictly increasing (sorted) order?",
        "options": [
            "Pre-order Traversal",
            "In-order Traversal",
            "Post-order Traversal",
            "Level-order Traversal"
        ],
        "correct": 1,
        "explanation": "In-order traversal follows the pattern: Left -> Root -> Right. Since a BST is structured such that all nodes in the left subtree are smaller than the root and all nodes in the right subtree are larger, visiting them in this sequence naturally produces a sorted list. Pre-order and Post-order are used for expression trees and deletion, respectively.",
        "topic": "Tree Traversals"
    },
    {
        "question": "What is the relationship between the number of leaf nodes (L) and the number of nodes with two children (N2) in a non-empty binary tree?",
        "options": [
            "L = N2",
            "L = N2 + 1",
            "L = N2 - 1",
            "L = 2 * N2"
        ],
        "correct": 1,
        "explanation": "This is a fundamental property of binary trees. For any non-empty binary tree, the number of leaves is always exactly one more than the number of internal nodes with two children. This holds true regardless of the tree's height or balance.",
        "topic": "Binary Tree Properties"
    },
    {
        "question": "During a 'Post-order' traversal of an expression tree, what is processed immediately after the left and right subtrees?",
        "options": [
            "The operand.",
            "The operator (Root).",
            "The leaf node.",
            "The parent of the root."
        ],
        "correct": 1,
        "explanation": "Post-order traversal follows the sequence: Left, Right, Root. In an expression tree, the operands are leaves and the operators are internal nodes. Post-order visits the operands first and then their corresponding operator, which is the root of that subtree. This is useful for converting expressions to Postfix notation.",
        "topic": "Tree Traversals"
    },
    {
        "question": "If a binary tree is implemented using an array, and a parent node is stored at index 'i', at which index is its left child located? (Assume 1-based indexing).",
        "options": [
            "i + 1",
            "2i",
            "2i + 1",
            "i / 2"
        ],
        "correct": 1,
        "explanation": "In an array representation of a binary tree, for any node at position 'i', its left child is at '2i' and its right child is at '2i + 1'. The parent of node 'i' is found at 'floor(i/2)'. This allows for efficient navigation without pointers, though it may waste space in sparse trees.",
        "topic": "Binary Tree Implementation"
    },
    {
        "question": "A binary tree where every node has either zero or two children is known as a:",
        "options": [
            "Complete Binary Tree",
            "Strictly Binary Tree (Full Binary Tree)",
            "Perfect Binary Tree",
            "Balanced Binary Tree"
        ],
        "correct": 1,
        "explanation": "A Strictly Binary Tree (also called a Full Binary Tree) is one where no node has only one child. Each node is either a leaf or has exactly two children. A Complete Binary Tree (Option 0) refers to levels being filled from left to right, which is a different structural property.",
        "topic": "Binary Tree Types"
    },
    {
        "question": "Which data structure is internally used by the system to manage recursive function calls?",
        "options": [
            "Queue",
            "Stack",
            "Array",
            "Priority Queue"
        ],
        "correct": 1,
        "explanation": "Systems use a 'Call Stack' to manage recursion. Every time a function calls itself, a new activation record (containing local variables and return address) is pushed onto the stack. When the base case is reached, the records are popped off in LIFO order to resume execution. Queues (Option 0) would process calls in the wrong order.",
        "topic": "Recursion"
    },
    {
        "question": "In the context of the BST 'delete' operation, if the node to be deleted has two children, which node is typically used as its replacement?",
        "options": [
            "The root of the tree.",
            "The node's left child.",
            "The inorder successor or inorder predecessor.",
            "The right-most leaf node."
        ],
        "correct": 2,
        "explanation": "To maintain the BST property after deleting a node with two children, we must replace it with the smallest value in its right subtree (inorder successor) or the largest value in its left subtree (inorder predecessor). This ensures the relative ordering of all other nodes remains valid.",
        "topic": "Binary Search Tree"
    },
    {
        "question": "What is the time complexity of searching for an element in a balanced Binary Search Tree containing 'n' nodes?",
        "options": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "In a balanced BST, each comparison allows us to discard roughly half of the remaining nodes. This logarithmic behavior results in O(log n) complexity. If the tree were skewed (unbalanced), the complexity would degrade to O(n), similar to a linked list.",
        "topic": "BST Efficiency"
    },
    {
        "question": "Which of the following is true for a 'Complete Binary Tree'?",
        "options": [
            "All nodes must have two children.",
            "All levels are completely filled except possibly the last level, which is filled from left to right.",
            "The height of the left and right subtrees must be identical.",
            "It cannot be implemented using an array."
        ],
        "correct": 1,
        "explanation": "A Complete Binary Tree is structured specifically to be compact, making it ideal for array-based implementations (like Heaps). All levels must be full except the last, and the last level nodes must be as far left as possible. Option 2 describes a balanced tree, not specifically a complete one.",
        "topic": "Binary Tree Types"
    },
    {
        "question": "In a Pre-order traversal, the root node is visited:",
        "options": [
            "After the left subtree.",
            "After the right subtree.",
            "Before both the left and right subtrees.",
            "Only after all leaf nodes are visited."
        ],
        "correct": 2,
        "explanation": "The 'Pre' in Pre-order refers to the root being visited first. The sequence is: Root -> Left -> Right. This is often used to create a copy of a tree or to evaluate prefix expressions.",
        "topic": "Tree Traversals"
    },
    {
        "question": "What is the result of the 'Post-order' traversal for a tree with root 'A', left child 'B', and right child 'C'?",
        "options": [
            "A, B, C",
            "B, A, C",
            "B, C, A",
            "C, B, A"
        ],
        "correct": 2,
        "explanation": "Following the Left -> Right -> Root rule: we visit the left child (B), then the right child (C), and finally the root (A). Thus, the sequence is B, C, A.",
        "topic": "Tree Traversals"
    },
    {
        "question": "In a BST, the 'findMin' operation can be performed by:",
        "options": [
            "Searching the right-most path of the tree.",
            "Searching the left-most path of the tree.",
            "Performing a Level-order traversal.",
            "Checking the root node only."
        ],
        "correct": 1,
        "explanation": "In a BST, for any node, all smaller values are in its left subtree. Therefore, the minimum value in the entire tree is the node reached by following the left-child pointers from the root until a NULL is encountered.",
        "topic": "Binary Search Tree"
    },
    {
        "question": "The number of NULL pointers in a binary tree with 'n' nodes is:",
        "options": [
            "n",
            "n - 1",
            "n + 1",
            "2n"
        ],
        "correct": 2,
        "explanation": "Every node has 2 pointer fields, totaling 2n pointers. In a tree with n nodes, n-1 pointers are used to connect nodes (one for every node except the root). Thus, the remaining pointers are NULL: 2n - (n - 1) = n + 1.",
        "topic": "Binary Tree Properties"
    },
    {
        "question": "What is the primary advantage of a Threaded Binary Tree?",
        "options": [
            "It reduces the height of the tree.",
            "It uses the NULL pointers to facilitate faster traversals without recursion or stacks.",
            "It allows nodes to have more than two children.",
            "It encrypts the data stored in nodes."
        ],
        "correct": 1,
        "explanation": "Threaded binary trees replace NULL pointers with 'threads' that point to the inorder predecessor or successor. This allows for linear in-order traversal without the memory overhead of a stack or the complexity of recursion.",
        "topic": "Threaded Binary Trees"
    },
    {
        "question": "In a 'Level-order' traversal, which data structure is typically used to keep track of nodes to be visited?",
        "options": [
            "Stack",
            "Queue",
            "Linked List",
            "Binary Search Tree"
        ],
        "correct": 1,
        "explanation": "Level-order traversal (Breadth-First Search) visits nodes level by level. To ensure nodes are visited in the order they were discovered, a FIFO structure (Queue) is used. We enqueue the root, then repeatedly dequeue a node, visit it, and enqueue its children.",
        "topic": "Tree Traversals"
    },
    {
        "question": "The height of an empty tree is usually defined as:",
        "options": [
            "0",
            "1",
            "-1",
            "Infinity"
        ],
        "correct": 2,
        "explanation": "By convention in most data structures textbooks (including VU handouts), the height of an empty tree is -1, and the height of a tree with only a root node is 0. This makes the math for tree balance (like AVL trees) consistent.",
        "topic": "Binary Tree Basics"
    },
    {
        "question": "Which of the following is an example of an 'Internal Node'?",
        "options": [
            "A node with no children.",
            "A node with at least one child.",
            "The NULL pointer at the end of a list.",
            "The first node added to the tree."
        ],
        "correct": 1,
        "explanation": "An internal node (or non-leaf node) is any node that has one or more children. Leaf nodes (Option 0) have zero children. The root is also an internal node unless it is the only node in the tree.",
        "topic": "Tree Terminology"
    },
    {
        "question": "How many nodes are in the left subtree of a BST if we insert the keys in the following order: 50, 30, 70, 20, 40?",
        "options": [
            "2",
            "3",
            "4",
            "1"
        ],
        "correct": 1,
        "explanation": "50 is the root. 30 is smaller, so it goes left. 70 is larger, so it goes right. 20 is smaller than 50 and 30, so it goes left of 30. 40 is smaller than 50 but larger than 30, so it goes right of 30. The nodes 30, 20, and 40 are all in the left subtree of 50. Total = 3 nodes.",
        "topic": "Binary Search Tree"
    },
    {
        "question": "Which property must hold for every node in a Binary Search Tree?",
        "options": [
            "left.value > root.value",
            "left.value < root.value < right.value",
            "root.value < left.value",
            "All leaf nodes must be at the same level."
        ],
        "correct": 1,
        "explanation": "The BST property defines that for every node: all values in the left subtree must be less than the node's value, and all values in the right subtree must be greater. This is the basis for its efficient search capabilities.",
        "topic": "BST Property"
    },
    {
        "question": "Which of the following describes 'Depth' of a node?",
        "options": [
            "The number of edges from the node to the deepest leaf.",
            "The number of edges from the root to the node.",
            "The total number of nodes in the tree.",
            "The number of children the node has."
        ],
        "correct": 1,
        "explanation": "Depth is measured from the top down (Root to Node), while Height is measured from the bottom up (Node to deepest leaf). The root has a depth of 0.",
        "topic": "Tree Terminology"
    },
    {
        "question": "If a binary tree is 'Perfect', and its height is 3, how many total nodes does it have?",
        "options": [
            "7",
            "8",
            "15",
            "31"
        ],
        "correct": 2,
        "explanation": "Using the formula 2^(h+1) - 1: height 3 means 2^(3+1) - 1 = 2^4 - 1 = 15 nodes.",
        "topic": "Binary Tree Properties"
    },
    {
        "question": "What is 'Tail Recursion'?",
        "options": [
            "Recursion where the recursive call is the last statement in the function.",
            "Recursion that never terminates.",
            "Recursion used only in linked lists.",
            "Recursion that returns a pointer to the tail of a queue."
        ],
        "correct": 0,
        "explanation": "Tail recursion is a specific form where the return value of the recursive call is immediately returned by the function. Many compilers can optimize tail recursion into a simple loop (Tail Call Optimization), saving stack space.",
        "topic": "Recursion"
    },
    {
        "question": "Which traversal is used to evaluate a Prefix expression?",
        "options": [
            "In-order",
            "Pre-order",
            "Post-order",
            "None of these"
        ],
        "correct": 1,
        "explanation": "Prefix notation (also known as Polish Notation) places the operator before its operands. This maps directly to a Pre-order traversal of an expression tree.",
        "topic": "Expression Trees"
    },
    {
        "question": "In a non-threaded binary tree, how are NULL pointers identified?",
        "options": [
            "By the value -1.",
            "They are simply memory addresses that point to nothing (0x0).",
            "They are replaced by the address of the root.",
            "They are stored in a separate array."
        ],
        "correct": 1,
        "explanation": "In pointer-based structures, a NULL pointer is a special constant (usually 0) that indicates the pointer does not refer to a valid object. In trees, it marks the absence of a child.",
        "topic": "Binary Tree Implementation"
    },
    {
        "question": "What is the result of deleting the root node 50 from a BST with nodes {50, 30, 70} if we use the inorder successor?",
        "options": [
            "30 becomes the root.",
            "70 becomes the root.",
            "The tree becomes empty.",
            "The tree is no longer a BST."
        ],
        "correct": 1,
        "explanation": "The inorder successor is the smallest node in the right subtree. In this case, the right subtree contains only 70. Thus, 70 replaces 50 as the root.",
        "topic": "Binary Search Tree"
    },
    {
        "question": "Binary Search Trees are not suitable for:",
        "options": [
            "Sorted data storage.",
            "Fast searching of elements.",
            "Frequent random access by index like an array.",
            "Maintaining a dynamic set of data."
        ],
        "correct": 2,
        "explanation": "While BSTs are great for searching and sorting, they do not provide O(1) random access by index (e.g., 'give me the 5th element'). To find the 5th element, you would need to perform an in-order traversal, taking O(n) or O(log n) if the tree is augmented with size information.",
        "topic": "BST Limitations"
    },
    {
        "question": "Which node is the 'Ancestor' of all nodes in a tree?",
        "options": [
            "The Leaf node",
            "The Root node",
            "The Parent node",
            "The Sibling node"
        ],
        "correct": 1,
        "explanation": "The root node is the topmost node. Every other node in the tree is descended from the root, making the root the universal ancestor.",
        "topic": "Tree Terminology"
    },
    {
        "question": "In a BST insertion of 10, 20, 30, 40, 50, the tree becomes:",
        "options": [
            "Balanced",
            "Skewed (Right-heavy)",
            "Perfect",
            "Complete"
        ],
        "correct": 1,
        "explanation": "Since each subsequent key is larger than the previous one, every new node will be added as the right child of the previous node. This results in a line-like structure called a right-skewed tree.",
        "topic": "Binary Search Tree"
    },
    {
        "question": "What is 'Degree' of a node?",
        "options": [
            "The height of the node.",
            "The number of children a node has.",
            "The level of the node.",
            "The value stored in the node."
        ],
        "correct": 1,
        "explanation": "In tree theory, the degree of a node is the number of subtrees (children) it has. For a binary tree, the maximum degree of any node is 2.",
        "topic": "Tree Terminology"
    },
    {
        "question": "The time complexity to find the 'successor' of a node in a BST is:",
        "options": [
            "O(1)",
            "O(n)",
            "O(h) where h is height",
            "O(log h)"
        ],
        "correct": 2,
        "explanation": "Finding a successor may require traversing down to the smallest element in the right subtree or moving up the parent pointers. Both paths are bounded by the height of the tree (h).",
        "topic": "BST Operations"
    },
    {
        "question": "A Binary Tree is 'Full' if:",
        "options": [
            "Every node has 2 children.",
            "Every node has 0 or 2 children.",
            "All leaves are at the same level.",
            "It contains 2^h nodes."
        ],
        "correct": 1,
        "explanation": "A full binary tree (or strictly binary tree) is one where no node has exactly one child. This distinguishes it from a 'perfect' tree where all leaves must be at the same level.",
        "topic": "Binary Tree Types"
    },
    {
        "question": "Which of the following is NOT a linear data structure?",
        "options": [
            "Stack",
            "Queue",
            "Binary Tree",
            "Linked List"
        ],
        "correct": 2,
        "explanation": "Linear data structures (Stack, Queue, List) arrange elements in a sequence. A tree is a non-linear (hierarchical) data structure where elements can have multiple relationships.",
        "topic": "Data Structure Classification"
    },
    {
        "question": "In a BST with n nodes, the best-case time complexity for insertion is:",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n log n)"
        ],
        "correct": 1,
        "explanation": "The 'best' case for BST operations occurs when the tree is balanced. In a balanced tree, the path from root to leaf is approximately log n. Therefore, insertion takes O(log n).",
        "topic": "BST Efficiency"
    },
    {
        "question": "What is the primary motivation for using AVL trees instead of standard Binary Search Trees (BST)?",
        "options": [
            "AVL trees use less memory than standard BSTs.",
            "AVL trees ensure O(log n) time complexity for search, insertion, and deletion by maintaining balance.",
            "AVL trees allow nodes to have more than two children.",
            "AVL trees do not require any pointers for implementation."
        ],
        "correct": 1,
        "explanation": "A standard BST can become skewed (like a linked list) in the worst case, leading to O(n) operations. AVL trees are self-balancing BSTs that ensure the height of the tree remains logarithmic, guaranteeing that even in the worst case, operations stay efficient at O(log n). Options 0 and 3 are incorrect as AVL trees actually use slightly more memory for balance factors and are still strictly binary.",
        "topic": "AVL Trees"
    },
    {
        "question": "In an AVL tree, what are the only permissible values for the 'Balance Factor' of any node?",
        "options": [
            "0, 1, 2",
            "-1, 0, 1",
            "Any integer value",
            "Only positive integers"
        ],
        "correct": 1,
        "explanation": "The AVL balance condition states that for every node, the height difference between its left and right subtrees must be at most 1. Therefore, the balance factor (Height_Left - Height_Right) can only be -1, 0, or 1. Any other value (like 2 or -2) triggers a rotation to rebalance the tree.",
        "topic": "AVL Balance Factor"
    },
    {
        "question": "Which type of rotation is required when a new node is inserted into the right subtree of the right child of a node, causing it to become unbalanced?",
        "options": [
            "Single Left Rotation (RR Rotation)",
            "Single Right Rotation (LL Rotation)",
            "Left-Right Double Rotation (LR Rotation)",
            "Right-Left Double Rotation (RL Rotation)"
        ],
        "correct": 0,
        "explanation": "When an insertion occurs in the 'outside' right (right-right case), a Single Left Rotation is performed around the unbalanced node to pull the tree back into balance. Single Right Rotation is for the left-left case, and double rotations are reserved for 'inside' or zigzag imbalances.",
        "topic": "AVL Rotations"
    },
    {
        "question": "In the context of AVL trees, a 'Double Rotation' (LR or RL) is technically a combination of:",
        "options": [
            "Two rotations in the same direction.",
            "Two rotations in opposite directions.",
            "A rotation and a node deletion.",
            "A rotation and a level-order traversal."
        ],
        "correct": 1,
        "explanation": "Double rotations are used for zigzag imbalances. For example, a Left-Right (LR) rotation involves first performing a Left rotation on the child, followed by a Right rotation on the parent. This 'straightens' the tree before bringing the middle value to the top.",
        "topic": "AVL Rotations"
    },
    {
        "question": "What is the maximum height of an AVL tree with 'n' nodes?",
        "options": [
            "Exactly log2(n)",
            "Approximately 1.44 log2(n)",
            "Exactly n - 1",
            "n/2"
        ],
        "correct": 1,
        "explanation": "While a perfectly balanced binary tree has a height of log2(n), AVL trees allow for a slight imbalance (BF of 1). Mathematical analysis shows that the worst-case height of an AVL tree is approximately 1.44 log2(n), which is still strictly O(log n) efficiency.",
        "topic": "AVL Properties"
    },
    {
        "question": "When deleting a node from an AVL tree, how many rotations might be required to restore balance to the entire tree?",
        "options": [
            "At most one single rotation.",
            "At most one double rotation.",
            "Zero or more, potentially O(log n) rotations up to the root.",
            "Deletion never requires rotations."
        ],
        "correct": 2,
        "explanation": "Unlike insertion, where a single or double rotation at the first unbalanced node restores balance for the whole tree, deletion can cause a 'cascade' effect. Rebalancing one node may change the height of its subtree, requiring further checks and rotations at each ancestor up to the root.",
        "topic": "AVL Deletion"
    },
    {
        "question": "Which of the following describes a 'B-Tree'?",
        "options": [
            "A binary tree where every node is colored black.",
            "A self-balancing search tree where nodes can have multiple keys and more than two children.",
            "A tree used only for storing Boolean values.",
            "A tree that is always perfectly balanced and binary."
        ],
        "correct": 1,
        "explanation": "B-Trees are 'multi-way' search trees commonly used in databases and file systems. They allow nodes to store many keys and have many children (m-way), which reduces the height of the tree significantly compared to binary trees, minimizing disk access.",
        "topic": "B-Trees"
    },
    {
        "question": "In a B-Tree of order 'm', what is the maximum number of children any internal node can have?",
        "options": [
            "m - 1",
            "m",
            "2m",
            "m/2"
        ],
        "correct": 1,
        "explanation": "The order 'm' of a B-Tree defines its capacity. An internal node can have a maximum of 'm' children and 'm-1' keys. For example, in a B-Tree of order 4, a node can have at most 4 children.",
        "topic": "B-Tree Properties"
    },
    {
        "question": "Which property of B-Trees makes them particularly suitable for secondary storage (disk) implementation?",
        "options": [
            "They use recursion for all operations.",
            "The large branching factor (order) results in a very shallow tree, reducing disk reads.",
            "They store data only in the leaf nodes.",
            "They are implemented using linked lists exclusively."
        ],
        "correct": 1,
        "explanation": "Disk access is much slower than RAM. By having many children per node, B-Trees keep the height very low (even for millions of records). This ensures that we can find a record with very few disk 'reads', which is the primary bottleneck in large-scale storage.",
        "topic": "B-Trees"
    },
    {
        "question": "In a B-Tree of order 'm', what is the minimum number of keys an internal node (other than the root) must contain?",
        "options": [
            "1",
            "ceil(m/2) - 1",
            "m - 1",
            "ceil(m/2)"
        ],
        "correct": 1,
        "explanation": "To ensure the tree remains dense and efficient, B-Trees enforce a minimum occupancy rule. Every node (except the root) must be at least half full, meaning it must have at least ceil(m/2) - 1 keys.",
        "topic": "B-Tree Properties"
    },
    {
        "question": "What happens in a B-Tree when an insertion causes a node to exceed the maximum number of keys (m-1)?",
        "options": [
            "The node is deleted.",
            "The node is split into two, and the median key is promoted to the parent.",
            "The tree is converted into an AVL tree.",
            "The extra keys are stored in a linked list."
        ],
        "correct": 1,
        "explanation": "B-Trees grow from the bottom up. When a node becomes too full, it splits. The middle (median) value moves up into the parent node, and the remaining values form two new children. This may trigger a split in the parent as well.",
        "topic": "B-Tree Operations"
    },
    {
        "question": "The 'Balance Factor' of a node in a binary tree is defined as:",
        "options": [
            "Left Subtree Height + Right Subtree Height",
            "Left Subtree Height - Right Subtree Height",
            "Number of Left Nodes - Number of Right Nodes",
            "Total nodes / Height"
        ],
        "correct": 1,
        "explanation": "In AVL logic, the balance factor is the difference between the heights of the subtrees. It is calculated as H_left - H_right. Some implementations use H_right - H_left, but the core principle of checking the difference remains the same.",
        "topic": "AVL Balance Factor"
    },
    {
        "question": "How many pointers does a node in an AVL tree contain?",
        "options": [
            "Two (Left and Right)",
            "Three (Left, Right, and Parent)",
            "m (based on the order)",
            "Variable number"
        ],
        "correct": 0,
        "explanation": "Since an AVL tree is a specialized Binary Search Tree, each node contains exactly two child pointers: Left and Right. While some implementations might add a Parent pointer for convenience, the standard definition is a binary structure.",
        "topic": "AVL Trees"
    },
    {
        "question": "In an AVL tree, if a node's left subtree has height 3 and its right subtree has height 5, the node is:",
        "options": [
            "Balanced",
            "Unbalanced (BF = -2)",
            "Unbalanced (BF = 2)",
            "Perfectly balanced"
        ],
        "correct": 1,
        "explanation": "Balance Factor = Height_Left - Height_Right = 3 - 5 = -2. Since the absolute value is greater than 1, the node is considered unbalanced and requires a rotation.",
        "topic": "AVL Balance Factor"
    },
    {
        "question": "Which of the following is TRUE about the root of a B-Tree of order m?",
        "options": [
            "It must always have m-1 keys.",
            "It must have at least ceil(m/2) children.",
            "It can have as few as 2 children (unless the tree is just the root).",
            "It must be a leaf node."
        ],
        "correct": 2,
        "explanation": "The root is the only node in a B-Tree exempt from the 'half-full' rule. If it is not a leaf, it must have at least two children, ensuring the tree can begin to branch out.",
        "topic": "B-Tree Properties"
    },
    {
        "question": "A Single Right Rotation (LL Rotation) is performed when an imbalance occurs in the:",
        "options": [
            "Right child's right subtree.",
            "Left child's left subtree.",
            "Left child's right subtree.",
            "Right child's left subtree."
        ],
        "correct": 1,
        "explanation": "The 'LL' case refers to an insertion in the Left child's Left subtree. This makes the node 'left-heavy' in a straight line, which is corrected by rotating the whole structure to the right.",
        "topic": "AVL Rotations"
    },
    {
        "question": "What is the time complexity of B-Tree operations (search/insert)?",
        "options": [
            "O(n)",
            "O(log m n) where m is the order",
            "O(1)",
            "O(n log n)"
        ],
        "correct": 1,
        "explanation": "Because B-Trees are balanced and have a high branching factor, the height is logarithmic with respect to the base 'm'. This makes operations extremely efficient, especially for very large datasets stored on disk.",
        "topic": "B-Tree Complexity"
    },
    {
        "question": "AVL trees are named after which two mathematicians?",
        "options": [
            "Adelson-Velsky and Landis",
            "Aho and Ullman",
            "Bellman and Ford",
            "Dijkstra and Knuth"
        ],
        "correct": 0,
        "explanation": "The AVL tree was the first self-balancing BST, invented in 1962 by Georgy Adelson-Velsky and Evgenii Landis.",
        "topic": "History"
    },
    {
        "question": "Which of the following is NOT a balanced search tree?",
        "options": [
            "AVL Tree",
            "Red-Black Tree",
            "Standard BST",
            "B-Tree"
        ],
        "correct": 2,
        "explanation": "A standard BST does not have any mechanism to ensure it stays balanced. It can easily degenerate into a chain if data is inserted in sorted order.",
        "topic": "Tree Types"
    },
    {
        "question": "To maintain AVL balance, rotations are performed ________ during the insertion process.",
        "options": [
            "From the root down to the leaf",
            "From the leaf up towards the root",
            "Only on the leaf nodes",
            "Only on the root node"
        ],
        "correct": 1,
        "explanation": "After a node is inserted, the recursion 'unwinds', and we check the balance factor of each ancestor starting from the parent of the new node up towards the root. Rebalancing happens at the first node found to be unbalanced.",
        "topic": "AVL Insertion"
    },
    {
        "question": "A B-Tree of order 3 is also known as a:",
        "options": [
            "2-3 Tree",
            "Binary Tree",
            "2-3-4 Tree",
            "Red-Black Tree"
        ],
        "correct": 0,
        "explanation": "A 2-3 Tree is a B-Tree where each node has either 2 children (1 key) or 3 children (2 keys). This is the simplest form of a B-Tree.",
        "topic": "B-Trees"
    },
    {
        "question": "Which rotation is needed for an 'inside' imbalance on the right side (Right-Left case)?",
        "options": [
            "Single Left Rotation",
            "Single Right Rotation",
            "Right-Left Double Rotation",
            "Left-Right Double Rotation"
        ],
        "correct": 2,
        "explanation": "The RL case happens when a node is added to the left subtree of a right child. This is a zigzag imbalance that requires a Right rotation on the child followed by a Left rotation on the parent.",
        "topic": "AVL Rotations"
    },
    {
        "question": "In a B-Tree, all leaf nodes must be at:",
        "options": [
            "Level 0",
            "The same depth/level",
            "Variable levels",
            "Only the left side"
        ],
        "correct": 1,
        "explanation": "One of the strict properties of a B-Tree is that it is perfectly balanced; all leaf nodes must be at the exact same depth.",
        "topic": "B-Tree Properties"
    },
    {
        "question": "The process of restoring AVL balance after an insertion requires how many passes?",
        "options": [
            "One pass (downward only)",
            "Two passes (downward to insert, upward to rebalance)",
            "Three passes",
            "Zero passes"
        ],
        "correct": 1,
        "explanation": "The algorithm first traverses down to find the insertion point (BST search) and then travels back up the path to update heights and perform rotations.",
        "topic": "AVL Operations"
    },
    {
        "question": "Which of the following is true for B+ Trees (a variation of B-Trees)?",
        "options": [
            "Keys are stored only in internal nodes.",
            "All data is stored in the leaf nodes, and leaves are linked together.",
            "They are not used in databases.",
            "They are unbalanced trees."
        ],
        "correct": 1,
        "explanation": "In a B+ Tree, internal nodes only store keys to guide the search, while the actual data records are in the leaves. A linked list connects the leaves to allow for efficient range searches (e.g., 'find all employees with ID between 100 and 200').",
        "topic": "B+ Trees"
    },
    {
        "question": "What is the maximum number of keys in a B-Tree node of order 5?",
        "options": [
            "5",
            "4",
            "6",
            "2"
        ],
        "correct": 1,
        "explanation": "Max keys = m - 1. For order 5, max keys = 4.",
        "topic": "B-Tree Properties"
    },
    {
        "question": "Which of the following is a disadvantage of AVL trees compared to Red-Black trees?",
        "options": [
            "AVL trees are less balanced.",
            "AVL trees may require more rotations during deletion.",
            "AVL trees have slower search times.",
            "AVL trees are harder to implement."
        ],
        "correct": 1,
        "explanation": "AVL trees are 'more strictly' balanced than Red-Black trees. This makes search faster in AVL, but insertion and deletion can be slower because they might trigger more frequent rebalancing rotations.",
        "topic": "AVL vs Red-Black"
    },
    {
        "question": "In a B-Tree, keys within a single node are stored in ________ order.",
        "options": [
            "Random",
            "Decreasing",
            "Increasing (Sorted)",
            "Level"
        ],
        "correct": 2,
        "explanation": "To allow for binary search within a node, all keys in a B-Tree node must be kept in sorted (increasing) order.",
        "topic": "B-Tree Properties"
    },
    {
        "question": "AVL trees use 'height' for balancing. What does 'height' of a node mean?",
        "options": [
            "The number of ancestors.",
            "The number of edges on the longest path from that node to a leaf.",
            "The total number of children.",
            "The level of the node starting from root."
        ],
        "correct": 1,
        "explanation": "Height is measured from the bottom up. A leaf node has height 0, and the height of an internal node is 1 + max(height of children).",
        "topic": "Tree Terminology"
    },
    {
        "question": "If we insert a key that is between the left child and the root, which rotation type is likely?",
        "options": [
            "LL",
            "RR",
            "LR",
            "RL"
        ],
        "correct": 2,
        "explanation": "If a value is greater than the left child but smaller than the root, it will be placed in the right subtree of the left child, creating a zigzag (LR) imbalance.",
        "topic": "AVL Rotations"
    },
    {
        "question": "Why is the time complexity of B-Tree search O(log n)?",
        "options": [
            "Because it is a binary tree.",
            "Because its height is kept balanced and proportional to log n.",
            "Because it uses hashing.",
            "It is actually O(n)."
        ],
        "correct": 1,
        "explanation": "Like all balanced search trees, B-Trees ensure that the search path never exceeds a logarithmic function of the total number of elements.",
        "topic": "B-Tree Efficiency"
    },
    {
        "question": "A B-Tree of order 'm' is a ________ search tree.",
        "options": [
            "Binary",
            "Multi-way",
            "Ternary",
            "Unary"
        ],
        "correct": 1,
        "explanation": "B-Trees are multi-way trees because they can have more than two children (m children).",
        "topic": "B-Trees"
    },
    {
        "question": "When a B-Tree node splits, which key is promoted?",
        "options": [
            "The smallest key",
            "The largest key",
            "The middle (median) key",
            "A random key"
        ],
        "correct": 2,
        "explanation": "Promoting the median ensures that the remaining keys are split evenly into two new child nodes, maintaining the balance of the tree.",
        "topic": "B-Tree Operations"
    },
    {
        "question": "AVL rebalancing rotations take ________ time.",
        "options": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ],
        "correct": 0,
        "explanation": "A rotation only involves updating a few pointers. It is a constant time operation regardless of how large the tree is.",
        "topic": "AVL Rotations"
    },
    {
        "question": "In a B-Tree, internal nodes guide the search to the ________.",
        "options": [
            "Root",
            "Parent",
            "Leaves",
            "NULL pointers"
        ],
        "correct": 2,
        "explanation": "Just like a BST, we use the keys in the internal nodes to decide which child to follow until we reach the leaf nodes where the search terminates.",
        "topic": "B-Trees"
    },
    {
        "question": "Which of the following properties must a 'Binary Heap' satisfy to be considered valid?",
        "options": [
            "It must be a perfect binary tree and follow the BST property.",
            "It must be a complete binary tree and follow the heap-order property.",
            "It must be a full binary tree where every node has two children.",
            "It must be a threaded binary tree with no NULL pointers."
        ],
        "correct": 1,
        "explanation": "A Binary Heap is defined by two specific constraints: Structural and Heap-order. Structurally, it must be a 'Complete Binary Tree' (filled level by level from left to right). The heap-order property determines if it is a Max-Heap (parent  children) or a Min-Heap (parent  children). It does not follow the BST property where the left child is smaller than the parent and the right is larger.",
        "topic": "Heaps"
    },
    {
        "question": "In a 'Min-Heap' containing 'n' elements, where is the smallest element always located?",
        "options": [
            "In the last leaf node.",
            "At the root node (index 1 in array representation).",
            "In the middle of the array.",
            "It could be anywhere depending on the insertion order."
        ],
        "correct": 1,
        "explanation": "The Min-Heap property dictates that for every node 'p', the value at 'p' is less than or equal to the values of its children. This recursive property ensures that the absolute minimum value of the entire structure is always at the root. In an array-based implementation starting at index 1, the minimum is at `A[1]`.",
        "topic": "Min-Heap"
    },
    {
        "question": "What is the time complexity of the 'deleteMin' operation in a Binary Heap with 'n' nodes?",
        "options": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "To delete the minimum element (the root), we replace it with the last element in the heap and then perform a 'percolate down' (or heapify) operation to restore the heap property. Since the height of a complete binary tree is log n, the element can be moved down at most log n levels, resulting in O(log n) complexity.",
        "topic": "Heap Operations"
    },
    {
        "question": "If a Binary Heap is stored in an array 'A' starting at index 1, where is the parent of the node at index 'i' located?",
        "options": [
            "i / 2 (integer division)",
            "2 * i",
            "2 * i + 1",
            "i - 1"
        ],
        "correct": 0,
        "explanation": "The array representation of a complete binary tree allows for simple arithmetic navigation. For a node at index 'i', its left child is at '2i', its right child is at '2i + 1', and its parent is at 'floor(i/2)'. This avoids the need for explicit pointers and saves memory.",
        "topic": "Heap Implementation"
    },
    {
        "question": "Which data structure is most efficiently used to implement a 'Priority Queue' where elements are retrieved based on their importance?",
        "options": [
            "Linked List",
            "Stack",
            "Binary Heap",
            "Circular Queue"
        ],
        "correct": 2,
        "explanation": "While a sorted array or list could implement a priority queue, insertion would be O(n). A Binary Heap provides an excellent balance, allowing both 'insert' and 'deleteMin' (extract priority) to be performed in O(log n) time. This makes it the standard choice for priority queue implementations in systems like OS schedulers.",
        "topic": "Priority Queues"
    },
    {
        "question": "In the 'Huffman Coding' algorithm, what type of tree is constructed to generate prefix codes?",
        "options": [
            "Binary Search Tree",
            "AVL Tree",
            "Optimal Binary Merge Tree",
            "B-Tree"
        ],
        "correct": 2,
        "explanation": "Huffman coding builds a binary tree where the leaves represent characters and the path from the root determines the code. It is an 'Optimal Binary Merge Tree' because it minimizes the weighted path length, ensuring that frequently occurring characters have shorter codes. This is the basis of lossless data compression.",
        "topic": "Huffman Coding"
    },
    {
        "question": "During the 'buildHeap' process, which converts an unordered array into a heap, what is the total time complexity?",
        "options": [
            "O(n log n)",
            "O(n)",
            "O(n^2)",
            "O(log n)"
        ],
        "correct": 1,
        "explanation": "Although inserting 'n' elements one by one takes O(n log n), the 'buildHeap' algorithm (starting from the last non-leaf node and percolating down) is mathematically proven to take O(n) time. This is because most nodes are near the leaves and only travel a short distance down.",
        "topic": "Heap Operations"
    },
    {
        "question": "In Huffman coding, if character 'A' has a frequency of 50 and character 'B' has a frequency of 20, which statement is true regarding their codes?",
        "options": [
            "Character 'B' will have a shorter binary code than 'A'.",
            "Character 'A' will have a shorter binary code than 'B'.",
            "Both will have codes of equal length.",
            "The length of the code depends on their alphabetical order."
        ],
        "correct": 1,
        "explanation": "The core principle of Huffman coding is 'Variable Length Encoding'. Higher frequency characters are placed closer to the root of the Huffman tree, resulting in shorter bit strings. Lower frequency characters are deeper in the tree and receive longer codes, optimizing overall file size.",
        "topic": "Huffman Coding"
    },
    {
        "question": "What is the maximum number of elements in a Binary Heap of height 'h'?",
        "options": [
            "2^h",
            "2^(h+1) - 1",
            "2^h - 1",
            "h^2"
        ],
        "correct": 1,
        "explanation": "Since a heap must be a complete binary tree, its maximum capacity is reached when it is also a 'Perfect' binary tree. The formula for total nodes in a perfect tree of height 'h' is 2^(h+1) - 1.",
        "topic": "Heap Properties"
    },
    {
        "question": "What happens during the 'percolate up' operation in a Max-Heap?",
        "options": [
            "A large value is moved down from the root to the leaves.",
            "A newly inserted value is compared with its parent and swapped if it is larger.",
            "The root is deleted and replaced by its child.",
            "The entire tree is sorted using in-order traversal."
        ],
        "correct": 1,
        "explanation": "When a new element is added to a Max-Heap, it is placed at the next available leaf position. To restore the Max-Heap property, we compare it with its parent. If the new value is larger, they are swapped. this continues until the value finds its correct level or becomes the root.",
        "topic": "Heap Operations"
    },
    {
        "question": "Huffman codes are 'Prefix Codes'. What does this mean technically?",
        "options": [
            "Every code must start with a '1'.",
            "No code is a prefix of any other code.",
            "All codes must have the same number of bits.",
            "Codes are assigned based on the first letter of the word."
        ],
        "correct": 1,
        "explanation": "Prefix codes (or prefix-free codes) ensure that there is no ambiguity during decoding. Because no code is a start (prefix) of another, the decoder can recognize the end of a character's bitstring immediately without needing a separator like a space or comma.",
        "topic": "Huffman Coding"
    },
    {
        "question": "In an array-based Max-Heap, where is the second-largest element located?",
        "options": [
            "Always at index 2.",
            "Always at index 3.",
            "Either at index 2 or index 3.",
            "At the very last index of the array."
        ],
        "correct": 2,
        "explanation": "In a Max-Heap, the largest element is at the root (index 1). The second-largest must be one of the root's children. Therefore, it will be found at index 2 (left child) or index 3 (right child), depending on which one is greater.",
        "topic": "Max-Heap"
    },
    {
        "question": "Which of the following is NOT a valid application of a Binary Heap?",
        "options": [
            "Heapsort algorithm.",
            "Selection of the 'k' smallest elements in a stream.",
            "Performing a range search (e.g., find all keys between 10 and 50).",
            "Implementing the priority queue in Dijkstra's algorithm."
        ],
        "correct": 2,
        "explanation": "Heaps are optimized for accessing the extreme (min or max) element. They are not 'Search Trees'. Unlike BSTs or AVL trees, heaps do not maintain a total ordering of elements that allows for efficient range searches. Range searching in a heap would require O(n) time.",
        "topic": "Heap Applications"
    },
    {
        "question": "When constructing a Huffman Tree, which two nodes are selected from the priority queue in each step?",
        "options": [
            "The two nodes with the highest frequencies.",
            "The root and a random leaf.",
            "The two nodes with the lowest frequencies.",
            "The left-most and right-most nodes."
        ],
        "correct": 2,
        "explanation": "The Huffman algorithm is 'Greedy'. At each step, it takes the two nodes with the smallest frequencies, creates a new internal node as their parent (with frequency equal to the sum of the two), and puts the new node back into the queue. This builds the tree from the 'bottom up'.",
        "topic": "Huffman Coding"
    },
    {
        "question": "A complete binary tree with 10 nodes has how many leaf nodes?",
        "options": [
            "4",
            "5",
            "6",
            "3"
        ],
        "correct": 1,
        "explanation": "In a complete binary tree with 'n' nodes, the leaf nodes start from index floor(n/2) + 1 to n. For n=10, floor(10/2) + 1 = 6. Nodes at indices 6, 7, 8, 9, and 10 are leaves. Total = 5 leaf nodes.",
        "topic": "Binary Tree Properties"
    },
    {
        "question": "What is the result of a 'percolate down' if the children of a node are both smaller than the node in a Min-Heap?",
        "options": [
            "The node is swapped with the smaller child.",
            "The node is swapped with the larger child.",
            "No swap occurs; the node stays in its current position.",
            "The node is moved to the root."
        ],
        "correct": 2,
        "explanation": "In a Min-Heap, the parent must be smaller than its children. If the parent is already smaller than both children, the Min-Heap property is satisfied at that node, and no further swaps are necessary.",
        "topic": "Heap Operations"
    },
    {
        "question": "Which of the following is true about 'internal nodes' in a Huffman tree?",
        "options": [
            "They store the actual character data.",
            "They only store the sum of the frequencies of their children.",
            "They always have exactly one child.",
            "They are not used during the decoding process."
        ],
        "correct": 1,
        "explanation": "In a Huffman tree, all data (characters) are stored in the leaf nodes. The internal nodes are strictly used for structuring the prefix codes and contain the cumulative frequencies used during tree construction.",
        "topic": "Huffman Coding"
    },
    {
        "question": "What is the time complexity of the 'buildHuffmanTree' algorithm for 'C' characters?",
        "options": [
            "O(C)",
            "O(C log C)",
            "O(C^2)",
            "O(2^C)"
        ],
        "correct": 1,
        "explanation": "The algorithm involves 'C' insertions and deletions from a priority queue (heap). Since each heap operation is O(log C), and we perform roughly 'C' of them, the total complexity is O(C log C).",
        "topic": "Huffman Coding"
    },
    {
        "question": "In a Max-Heap, after performing 'deleteMax', which element is moved to the root before heapifying?",
        "options": [
            "The second largest element.",
            "The smallest child of the root.",
            "The last element in the heap array.",
            "The first leaf node on the left."
        ],
        "correct": 2,
        "explanation": "To maintain the 'Complete Binary Tree' structure, we must remove the last occupied slot in the array. Therefore, the value at the last index is moved to the root's position (index 1), and then it 'bubbles down' to its correct spot.",
        "topic": "Heap Operations"
    },
    {
        "question": "The height of a Binary Heap with 'n' elements is ________.",
        "options": [
            "n",
            "log2 n",
            "floor(log2 n)",
            "n/2"
        ],
        "correct": 2,
        "explanation": "Since a heap is always a complete binary tree, its height is strictly logarithmic. Specifically, a heap with 'n' nodes has a height of floor(log2 n).",
        "topic": "Heap Properties"
    },
    {
        "question": "Which of the following describes the 'Greedy' nature of Huffman's algorithm?",
        "options": [
            "It tries all possible tree combinations to find the best one.",
            "It makes the locally optimal choice at each step (merging the two smallest frequencies).",
            "It always assigns '0' to the left child.",
            "It uses a stack to reverse the binary codes."
        ],
        "correct": 1,
        "explanation": "Greedy algorithms make the best possible choice at the current moment without looking back. By always merging the two smallest frequencies, Huffman's algorithm builds the globally optimal tree for prefix-free encoding.",
        "topic": "Algorithm Design"
    },
    {
        "question": "In a Min-Heap, what is the maximum number of comparisons needed for 'insert'?",
        "options": [
            "n",
            "log n",
            "n/2",
            "1"
        ],
        "correct": 1,
        "explanation": "When inserting into a heap, the value may 'percolate up' from the leaf to the root. The number of levels it can travel is equal to the height of the tree, which is O(log n).",
        "topic": "Heap Operations"
    },
    {
        "question": "How many external nodes (leaves) are in a Huffman tree built for 5 characters?",
        "options": [
            "4",
            "5",
            "9",
            "10"
        ],
        "correct": 1,
        "explanation": "A Huffman tree is a strictly binary tree where every internal node has exactly two children. If there are 'n' characters to be encoded, they will form the 'n' leaf nodes of the tree.",
        "topic": "Huffman Coding"
    },
    {
        "question": "Which property allows us to implement a Binary Heap using a simple array without pointers?",
        "options": [
            "The Max-Heap property.",
            "The Complete Binary Tree property.",
            "The BST property.",
            "The Threading property."
        ],
        "correct": 1,
        "explanation": "Because a complete binary tree has no 'holes' between levels or nodes on the same level, its elements can be mapped directly to array indices (1, 2, 3...) such that parents and children have fixed mathematical relationships.",
        "topic": "Heap Implementation"
    },
    {
        "question": "In Huffman encoding, if the frequency of 'e' is much higher than 'z', the bit string for 'e' will be ________ than for 'z'.",
        "options": [
            "Longer",
            "Shorter",
            "The same length",
            "More complex"
        ],
        "correct": 1,
        "explanation": "Frequently used items get shorter codes to save space. This is the fundamental goal of data compression used in ZIP files or JPEG images.",
        "topic": "Huffman Coding"
    },
    {
        "question": "If a Binary Heap array contains [X, 90, 80, 70, 60, 50, 40], what is the left child of 80? (X is a dummy at index 0).",
        "options": [
            "70",
            "60",
            "50",
            "40"
        ],
        "correct": 2,
        "explanation": "Value 80 is at index 3. Its left child is at index 2 * 3 = 6. Looking at the array, index 6 contains the value 50.",
        "topic": "Heap Implementation"
    },
    {
        "question": "What is the initial step in the 'deleteMin' algorithm for a heap?",
        "options": [
            "Delete the root and leave the space empty.",
            "Swap the root with its largest child.",
            "Store the root value to return later and replace the root with the last element.",
            "Perform an in-order traversal to find the new minimum."
        ],
        "correct": 2,
        "explanation": "We must remove the root but maintain tree completeness. By replacing the root with the last element, we 'shrink' the array correctly, then re-establish order by percolating down.",
        "topic": "Heap Operations"
    },
    {
        "question": "A Priority Queue implemented with a 'Linked List' has which worst-case complexity for 'insert'?",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n^2)"
        ],
        "correct": 2,
        "explanation": "To maintain a priority order in a linked list, you may have to traverse the entire list to find the correct spot for insertion, which takes linear time O(n).",
        "topic": "Data Structure Comparison"
    },
    {
        "question": "In a Min-Heap, where could the 'maximum' element be located?",
        "options": [
            "Always at the root.",
            "In one of the leaf nodes.",
            "Always at the last index of the array.",
            "At index 2."
        ],
        "correct": 1,
        "explanation": "In a Min-Heap, every parent is smaller than its children. Therefore, the maximum value cannot be an internal node (it would have to be smaller than its children). It must be one of the leaf nodes.",
        "topic": "Min-Heap"
    },
    {
        "question": "Which of the following is used as a 'Dummy' value at index 0 of a heap array in some implementations?",
        "options": [
            "A very large value for Max-Heap.",
            "A very small value (Sentinel) for Min-Heap.",
            "The value 0.",
            "Both 0 and 1 are correct."
        ],
        "correct": 1,
        "explanation": "Using a sentinel value at index 0 that is smaller than any possible heap value (for a Min-Heap) simplifies the 'insert' code by eliminating the need to check if the current node has reached the root.",
        "topic": "Heap Implementation"
    },
    {
        "question": "How many children does each internal node have in a Huffman Tree?",
        "options": [
            "At most 2",
            "Exactly 2",
            "Exactly 1",
            "Variable"
        ],
        "correct": 1,
        "explanation": "A Huffman tree is a full binary tree; every node that is not a leaf has exactly two children (representing bits 0 and 1).",
        "topic": "Huffman Coding"
    },
    {
        "question": "Which traversal of a Huffman tree is used to decode a binary string?",
        "options": [
            "Pre-order",
            "Post-order",
            "Root-to-Leaf path traversal",
            "Level-order"
        ],
        "correct": 2,
        "explanation": "To decode, we start at the root. If we see a '0', we go left; if '1', we go right. We continue until we reach a leaf, which tells us the character.",
        "topic": "Huffman Coding"
    },
    {
        "question": "In a heap with 'n' elements, the number of internal nodes is:",
        "options": [
            "n/2",
            "floor(n/2)",
            "log n",
            "n - 1"
        ],
        "correct": 1,
        "explanation": "In any complete binary tree, roughly half the nodes are leaves. Specifically, the internal nodes are at indices 1 to floor(n/2).",
        "topic": "Binary Tree Properties"
    },
    {
        "question": "Which operation is performed to 'Heapify' a subtree?",
        "options": [
            "Percolate Up",
            "Percolate Down",
            "In-order Traversal",
            "Rotation"
        ],
        "correct": 1,
        "explanation": "To heapify a subtree where the root might violate the heap property but the children's subtrees are valid, we use 'Percolate Down' to push the violating value to its correct level.",
        "topic": "Heap Operations"
    },
    {
        "question": "If the frequencies of 'A, B, C' are '10, 15, 30', what is the root frequency of the Huffman tree?",
        "options": [
            "30",
            "55",
            "25",
            "45"
        ],
        "correct": 1,
        "explanation": "The root of a Huffman tree always contains the sum of all frequencies in the set: 10 + 15 + 30 = 55.",
        "topic": "Huffman Coding"
    },
    {
        "question": "What is the primary characteristic of an 'In-place' sorting algorithm?",
        "options": [
            "It requires O(n) additional memory to store a copy of the array.",
            "It sorts the data by using only a small, constant amount of extra storage space.",
            "It can only be implemented using recursive function calls.",
            "It only works on data structures that use pointers, like linked lists."
        ],
        "correct": 1,
        "explanation": "An in-place sorting algorithm (like Bubble Sort or Quicksort) rearranges the elements within the original array itself. It may use a few temporary variables for swapping, but the extra memory required is O(1). This is highly efficient for systems with limited RAM. Options 0 is describes 'out-of-place' algorithms like Merge Sort.",
        "topic": "Sorting Basics"
    },
    {
        "question": "Which of the following sorting algorithms has a best-case time complexity of O(n) when the input array is already sorted?",
        "options": [
            "Selection Sort",
            "Insertion Sort",
            "Standard Quicksort",
            "Heap Sort"
        ],
        "correct": 1,
        "explanation": "Insertion Sort is highly efficient for nearly sorted data. In the best case (already sorted), it simply makes one comparison for each element and finds it is already in the correct position, resulting in O(n) time. Selection Sort (Option 0) always takes O(n^2) because it must scan the remaining unsorted portion regardless of the initial order.",
        "topic": "Insertion Sort"
    },
    {
        "question": "In 'Selection Sort', how many swaps are performed in the worst case for an array of 'n' elements?",
        "options": [
            "n^2",
            "n log n",
            "n - 1",
            "n/2"
        ],
        "correct": 2,
        "explanation": "Selection Sort works by finding the minimum element and swapping it into its correct position once per pass. Since there are n-1 passes to sort n elements, the total number of swaps is at most n-1. This makes Selection Sort useful in scenarios where the cost of writing to memory is much higher than the cost of comparisons.",
        "topic": "Selection Sort"
    },
    {
        "question": "What is the 'Stable' property in sorting algorithms?",
        "options": [
            "The algorithm always takes the same amount of time regardless of input.",
            "The algorithm maintains the relative order of records with equal keys.",
            "The algorithm never crashes during execution.",
            "The algorithm uses a fixed amount of memory."
        ],
        "correct": 1,
        "explanation": "A sorting algorithm is 'Stable' if two elements with equal values appear in the same relative order in the sorted output as they did in the input. This is critical when sorting by multiple criteria (e.g., sorting by name, then by department). Insertion sort is stable, while Quicksort is generally not.",
        "topic": "Sorting Properties"
    },
    {
        "question": "Which sorting algorithm is an improvement over Insertion Sort that reduces the distance elements must move by using a 'gap' sequence?",
        "options": [
            "Bubble Sort",
            "Shell Sort",
            "Quick Sort",
            "Merge Sort"
        ],
        "correct": 1,
        "explanation": "Shell Sort (invented by Donald Shell) is a generalization of Insertion Sort. It compares elements that are far apart (separated by a gap) and gradually reduces the gap. This allows elements to 'jump' toward their correct positions faster than the one-by-one shifts in standard Insertion Sort.",
        "topic": "Shell Sort"
    },
    {
        "question": "What is the worst-case time complexity of Quicksort, and when does it occur?",
        "options": [
            "O(n log n), when the pivot is always the median.",
            "O(n^2), when the input is already sorted and the first or last element is chosen as the pivot.",
            "O(n), when the array contains duplicate elements.",
            "O(n^3), when using a random pivot."
        ],
        "correct": 1,
        "explanation": "Quicksort's efficiency depends on the pivot selection. If the pivot consistently results in highly unbalanced partitions (e.g., one partition has 0 elements and the other has n-1), the recursion depth becomes O(n) and the total work O(n^2). This happens in sorted arrays if the first/last element is always the pivot.",
        "topic": "Quicksort"
    },
    {
        "question": "Which strategy is often used in Quicksort to avoid the O(n^2) worst-case scenario?",
        "options": [
            "Choosing the first element as the pivot.",
            "Median-of-three partitioning.",
            "Always using 0 as the pivot.",
            "Converting the array to a linked list first."
        ],
        "correct": 1,
        "explanation": "Median-of-three involves taking the first, middle, and last elements of the array and using their median as the pivot. This significantly increases the likelihood of a balanced partition, pushing Quicksort toward its O(n log n) average performance even on nearly sorted data.",
        "topic": "Quicksort"
    },
    {
        "question": "In the context of 'Disjoint Sets', what is the purpose of the 'Union' operation?",
        "options": [
            "To find which set a specific element belongs to.",
            "To merge two disjoint sets into a single set.",
            "To delete all elements from a set.",
            "To count the number of elements in a set."
        ],
        "correct": 1,
        "explanation": "The Union-Find ADT manages groups of elements. The 'Union' operation specifically takes two different sets and combines them into one. The 'Find' operation (Option 0) is used to determine the representative (or identity) of the set containing a specific element.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "How are Disjoint Sets typically represented in memory for efficient processing?",
        "options": [
            "A doubly linked list.",
            "A circular queue.",
            "An array where each index represents an element and the value represents its parent.",
            "A multidimensional matrix."
        ],
        "correct": 2,
        "explanation": "Disjoint sets are most efficiently implemented using a 'Parent Array'. If `A[i] = -1`, then element `i` is the root of a tree (a set representative). If `A[i] = j`, then `j` is the parent of `i`. This allows for fast upward traversal to find the root.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "What is the 'Union by Rank' (or Union by Height) optimization in Disjoint Sets?",
        "options": [
            "Always making the smaller set a child of the root of the larger set to keep trees shallow.",
            "Sorting the elements of the set before merging.",
            "Only merging sets that have the same number of elements.",
            "Deleting the root after every Union operation."
        ],
        "correct": 0,
        "explanation": "Without optimization, Union operations could create a very deep tree (like a linked list), making 'Find' operations take O(n). Union by Rank ensures that we always attach the shorter tree to the root of the taller tree, keeping the overall height logarithmic O(log n).",
        "topic": "Disjoint Set Optimizations"
    },
    {
        "question": "Which technique improves the 'Find' operation in Disjoint Sets by making all traversed nodes point directly to the root?",
        "options": [
            "Recursive Descent.",
            "Path Compression.",
            "Binary Search.",
            "Pivot Rotation."
        ],
        "correct": 1,
        "explanation": "Path Compression happens during a 'Find' operation. As we search for the root, we update every node along the path to point directly to that root. This flattens the tree structure significantly, making future 'Find' operations nearly O(1) on average.",
        "topic": "Disjoint Set Optimizations"
    },
    {
        "question": "What is the average time complexity of Quicksort?",
        "options": [
            "O(n)",
            "O(n^2)",
            "O(n log n)",
            "O(log n)"
        ],
        "correct": 2,
        "explanation": "On average, Quicksort partitions the array into two roughly equal halves. The depth of the recursion tree is log n, and at each level, O(n) work is done for partitioning. This results in the highly efficient O(n log n) average time complexity.",
        "topic": "Quicksort"
    },
    {
        "question": "Bubble Sort is generally considered inefficient for large datasets because its average and worst-case complexity is:",
        "options": [
            "O(n)",
            "O(n log n)",
            "O(n^2)",
            "O(1)"
        ],
        "correct": 2,
        "explanation": "Bubble Sort uses nested loops to repeatedly step through the list, compare adjacent elements, and swap them. For n elements, this results in approximately (n^2)/2 comparisons and swaps, making it O(n^2).",
        "topic": "Bubble Sort"
    },
    {
        "question": "In a Disjoint Set array implementation, if `S[i] = -5`, what does the value -5 typically represent?",
        "options": [
            "The element `i` has a parent at index 5.",
            "The element `i` is a root, and its set contains 5 elements.",
            "The element `i` is an invalid node.",
            "The set containing `i` has a height of 5."
        ],
        "correct": 1,
        "explanation": "In many efficient implementations of Disjoint Sets using 'Union by Size', the root node stores the negative of the number of elements in its tree. Thus, -5 indicates that the node is a root and there are 5 elements in that set.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "Which sorting algorithm uses the 'Divide and Conquer' strategy by partitioning around a pivot?",
        "options": [
            "Heap Sort",
            "Insertion Sort",
            "Quick Sort",
            "Selection Sort"
        ],
        "correct": 2,
        "explanation": "Quicksort is the classic 'Divide and Conquer' algorithm. It divides the problem (partitioning around a pivot) and then recursively conquers the two sub-problems (sorting the left and right halves).",
        "topic": "Quicksort"
    },
    {
        "question": "In Insertion Sort, an element is inserted into its correct position within the ________ portion of the array.",
        "options": [
            "Unsorted",
            "Sorted",
            "Random",
            "Heapified"
        ],
        "correct": 1,
        "explanation": "Insertion sort maintains a sorted sub-list at the start of the array. For each new element, it scans the sorted portion from right to left to find the correct insertion point.",
        "topic": "Insertion Sort"
    },
    {
        "question": "What is the maximum number of comparisons in the worst case for Selection Sort on an array of size n?",
        "options": [
            "n",
            "n(n-1)/2",
            "log n",
            "n log n"
        ],
        "correct": 1,
        "explanation": "Selection sort always performs the same number of comparisons regardless of the input order. In the first pass, it does n-1 comparisons, then n-2, and so on. The sum (n-1) + (n-2) + ... + 1 equals n(n-1)/2, which is O(n^2).",
        "topic": "Selection Sort"
    },
    {
        "question": "Which sorting algorithm is often used for small arrays or as a base case for Quicksort because of its low overhead?",
        "options": [
            "Shell Sort",
            "Merge Sort",
            "Insertion Sort",
            "Heap Sort"
        ],
        "correct": 2,
        "explanation": "Despite its O(n^2) worst case, Insertion Sort has very little overhead and performs exceptionally well on very small arrays (usually n < 20). Many high-performance Quicksort implementations switch to Insertion Sort for small partitions.",
        "topic": "Insertion Sort"
    },
    {
        "question": "If we use 'Path Compression' and 'Union by Rank' together, the time complexity per operation in Disjoint Sets is:",
        "options": [
            "O(log n)",
            "O(n)",
            "Nearly O(1) (Inverse Ackermann function)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "The combination of these two optimizations makes the Disjoint Set operations incredibly fast. The amortized time per operation becomes the inverse Ackermann function (n), which is less than 5 for any value of n that can be stored in the known universe.",
        "topic": "Disjoint Set Complexity"
    },
    {
        "question": "Which of the following is NOT true about Shell Sort?",
        "options": [
            "It is an in-place sorting algorithm.",
            "It is a stable sorting algorithm.",
            "Its performance depends on the choice of the increment (gap) sequence.",
            "It is faster than simple Insertion Sort for large n."
        ],
        "correct": 1,
        "explanation": "Shell Sort is not stable. Because it compares and swaps elements across large gaps, the relative order of equal elements can be changed. It is, however, in-place and faster than standard O(n^2) algorithms.",
        "topic": "Shell Sort"
    },
    {
        "question": "In Quicksort, the partitioning step takes ________ time.",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n^2)"
        ],
        "correct": 2,
        "explanation": "Partitioning involves a single pass through the current sub-array to compare every element with the pivot and move it to the left or right side. This linear scan takes O(n) time.",
        "topic": "Quicksort"
    },
    {
        "question": "The 'Find(i)' operation in Disjoint Sets returns:",
        "options": [
            "The value stored at index i.",
            "The total number of sets.",
            "The representative (root) of the set containing element i.",
            "A pointer to the parent of i."
        ],
        "correct": 2,
        "explanation": "The primary goal of 'Find' is to identify which set an element belongs to. Since each set is represented by its root, 'Find' returns that root's identity.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "Which sorting algorithm is often called 'Sinking Sort'?",
        "options": [
            "Selection Sort",
            "Insertion Sort",
            "Bubble Sort",
            "Quick Sort"
        ],
        "correct": 2,
        "explanation": "Bubble Sort is sometimes called sinking sort because the largest elements 'sink' to the bottom (the end of the array) while the smaller ones 'bubble up' to the top.",
        "topic": "Bubble Sort"
    },
    {
        "question": "What is the space complexity of Quicksort in the average case (considering the recursion stack)?",
        "options": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "Although Quicksort sorts 'in-place' (O(1) auxiliary space for data), the recursive calls require space on the system stack. In the average case, the depth of recursion is O(log n).",
        "topic": "Quicksort"
    },
    {
        "question": "A 'Pivot' in Quicksort is used to ________.",
        "options": [
            "Find the maximum element.",
            "Divide the array into two parts based on value.",
            "Merge two sorted arrays.",
            "Calculate the average of the set."
        ],
        "correct": 1,
        "explanation": "The pivot acts as a boundary. After partitioning, all elements to the left are smaller than the pivot, and all elements to the right are larger. This 'divides' the problem.",
        "topic": "Quicksort"
    },
    {
        "question": "If an array is already sorted, Selection Sort performs ________ number of comparisons.",
        "options": [
            "Zero",
            "n",
            "The same",
            "Fewer"
        ],
        "correct": 2,
        "explanation": "Selection sort lacks an internal check to see if the array is sorted. It will always execute its nested loops to find the minimum for every position, resulting in O(n^2) comparisons regardless of input state.",
        "topic": "Selection Sort"
    },
    {
        "question": "Shell Sort's worst-case time complexity with Shell's original increments (n/2, n/4...) is:",
        "options": [
            "O(n log n)",
            "O(n^2)",
            "O(n^1.5)",
            "O(n)"
        ],
        "correct": 1,
        "explanation": "Using Shell's original increments, the worst-case performance is O(n^2). However, other increment sequences (like Hibbard's) can improve the worst case to O(n^1.5).",
        "topic": "Shell Sort"
    },
    {
        "question": "Which operation is performed first in the 'Union-Find' algorithm to initialize n elements?",
        "options": [
            "Union all elements.",
            "Find the root of the array.",
            "Initialize an array of size n where each element is -1.",
            "Perform path compression on all elements."
        ],
        "correct": 2,
        "explanation": "Initialization involves creating n disjoint sets, each containing exactly one element. In an array-based representation, setting all entries to -1 indicates that every element is its own root.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "During Quicksort partitioning, if we use two pointers 'i' and 'j', 'i' stops when it finds an element ________ than the pivot.",
        "options": [
            "Smaller",
            "Equal",
            "Larger",
            "NULL"
        ],
        "correct": 2,
        "explanation": "To partition correctly, the left pointer 'i' moves right until it finds an element that is *larger* than the pivot (out of place), while the right pointer 'j' moves left until it finds an element *smaller* than the pivot. They then swap these two elements.",
        "topic": "Quicksort"
    },
    {
        "question": "Insertion Sort is 'Adaptive'. What does this mean?",
        "options": [
            "It changes its pivot at runtime.",
            "It uses more memory if needed.",
            "Its performance improves if the input is partially sorted.",
            "It can sort strings as well as integers."
        ],
        "correct": 2,
        "explanation": "An adaptive algorithm takes advantage of existing order in the input. Insertion sort is adaptive because it finishes earlier when it finds that elements are already in order.",
        "topic": "Insertion Sort"
    },
    {
        "question": "What happens if we always pick the 'Median' as the pivot in Quicksort?",
        "options": [
            "The complexity becomes O(n).",
            "The complexity becomes O(n^2).",
            "The complexity is guaranteed to be O(n log n).",
            "The algorithm fails to sort."
        ],
        "correct": 2,
        "explanation": "Choosing the median ensures perfectly balanced partitions (n/2 on each side). This results in the shortest possible recursion tree (log n depth), guaranteeing O(n log n) time.",
        "topic": "Quicksort"
    },
    {
        "question": "In a Disjoint Set represented by a tree, the 'Representative' of the set is the ________ node.",
        "options": [
            "Leaf",
            "Middle",
            "Root",
            "Oldest"
        ],
        "correct": 2,
        "explanation": "Every set is uniquely identified by its root node. All elements in the same set will eventually lead to the same root when 'Find' is called.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "Selection Sort is best described as having ________ swaps and ________ comparisons.",
        "options": [
            "O(n), O(n^2)",
            "O(n^2), O(n)",
            "O(n log n), O(n)",
            "O(1), O(n^2)"
        ],
        "correct": 0,
        "explanation": "Selection sort minimizes swaps (at most n-1, which is O(n)) but performs a large number of comparisons (n(n-1)/2, which is O(n^2)).",
        "topic": "Selection Sort"
    },
    {
        "question": "Which of the following sorting algorithms is NOT recursive by default?",
        "options": [
            "Merge Sort",
            "Quick Sort",
            "Insertion Sort",
            "Heap Sort"
        ],
        "correct": 2,
        "explanation": "Insertion sort is typically implemented using nested loops (iterative). While it *can* be written recursively, its natural form is iterative. Merge sort and Quicksort are fundamentally recursive (Divide and Conquer).",
        "topic": "Sorting Basics"
    },
    {
        "question": "Path compression reduces the tree height of a Disjoint Set to ________.",
        "options": [
            "Zero",
            "1 (nearly flat)",
            "log n",
            "n"
        ],
        "correct": 1,
        "explanation": "By pointing nodes directly to the root, path compression flattens the tree so that most nodes are only one edge away from the representative.",
        "topic": "Disjoint Set Optimizations"
    },
    {
        "question": "In the context of the Table ADT, what is the primary purpose of a 'Hashing' function?",
        "options": [
            "To sort the data elements in ascending order.",
            "To map a large range of search keys to a smaller range of array indices.",
            "To compress data files to save disk space.",
            "To provide a backup of the data in case of system failure."
        ],
        "correct": 1,
        "explanation": "Hashing is a technique used to transform a search key (which could be a string or a large integer) into a specific index within a hash table array. This allows for near O(1) average time complexity for insertions and lookups. Options 0, 2, and 3 describe sorting, compression, and recovery, which are not the primary roles of a hash function.",
        "topic": "Hashing Basics"
    },
    {
        "question": "What occurs during a 'Collision' in a Hash Table?",
        "options": [
            "The computer runs out of memory.",
            "Two or more different keys result in the same hash index.",
            "The hash function returns a negative value.",
            "The data structure is automatically deleted."
        ],
        "correct": 1,
        "explanation": "Because a hash function maps a potentially infinite set of keys to a finite set of array indices, it is possible for two distinct keys to hash to the same position. This event is called a collision, and it requires a resolution strategy (like chaining or probing) to manage the data. Option 2 is a technical error, not a collision definition.",
        "topic": "Collision Resolution"
    },
    {
        "question": "Which collision resolution technique stores all elements that hash to the same index in a linked list at that array position?",
        "options": [
            "Linear Probing",
            "Quadratic Probing",
            "Separate Chaining",
            "Double Hashing"
        ],
        "correct": 2,
        "explanation": "Separate Chaining handles collisions by making each cell in the hash table point to a linked list of records. When a collision occurs, the new item is simply appended to the list at that index. This method is robust against 'clustering' which affects probing methods (Options 0 and 1).",
        "topic": "Separate Chaining"
    },
    {
        "question": "In 'Linear Probing', if a collision occurs at index 'h', where does the algorithm search for the next available slot?",
        "options": [
            "(h + 1) % size",
            "(h + i^2) % size",
            "(h + i * h2(key)) % size",
            "h / 2"
        ],
        "correct": 0,
        "explanation": "Linear Probing is an 'Open Addressing' strategy that checks the very next sequential cell when a collision occurs. If 'h' is full, it checks h+1, then h+2, and so on, wrapping around the end of the array using the modulo operator. Option 1 describes Quadratic Probing, and Option 2 describes Double Hashing.",
        "topic": "Linear Probing"
    },
    {
        "question": "What is the phenomenon of 'Primary Clustering' in Hash Tables?",
        "options": [
            "When the hash table is completely empty.",
            "The tendency for long runs of occupied slots to build up in Linear Probing, increasing search time.",
            "When data is stored in multiple different hash tables.",
            "When the root of a tree has too many children."
        ],
        "correct": 1,
        "explanation": "Primary clustering is a major disadvantage of Linear Probing. As collisions occur, blocks of consecutive occupied cells form. New keys that hash into these blocks must probe through the entire cluster, making the cluster even larger and slowing down operations. Separate Chaining and Double Hashing are designed to avoid this.",
        "topic": "Clustering"
    },
    {
        "question": "Which of the following is a requirement for a 'Good' hash function?",
        "options": [
            "It must be slow to calculate to ensure security.",
            "It should distribute keys uniformly across the table to minimize collisions.",
            "It should always return the same index for every key.",
            "It must use as many loops as possible."
        ],
        "correct": 1,
        "explanation": "A high-quality hash function must be efficient to compute (fast) and should distribute keys as evenly as possible (uniform distribution). This prevents certain indices from being overloaded while others remain empty, thus keeping search times close to O(1).",
        "topic": "Hash Functions"
    },
    {
        "question": "In 'Quadratic Probing', the interval between probes increases ________.",
        "options": [
            "Linearly",
            "Quadratically (e.g., 1, 4, 9...)",
            "Randomly",
            "Exponentially"
        ],
        "correct": 1,
        "explanation": "To reduce primary clustering, Quadratic Probing calculates the next index using the formula (hash(key) + i^2) % size. By squaring the increment, the algorithm 'jumps' further away from the original collision site with each failed probe.",
        "topic": "Quadratic Probing"
    },
    {
        "question": "What is 'Secondary Clustering'?",
        "options": [
            "Clustering caused by multiple keys hashing to the same initial index in Quadratic Probing.",
            "When the second half of the hash table is full.",
            "A type of clustering that only occurs in Separate Chaining.",
            "When two different hash functions return the same value."
        ],
        "correct": 0,
        "explanation": "While Quadratic Probing eliminates primary clustering (consecutive blocks), it still suffers from secondary clustering: keys that hash to the same initial index will follow the exact same probe sequence (1, 4, 9...). Double Hashing is the primary way to solve this.",
        "topic": "Clustering"
    },
    {
        "question": "Double Hashing eliminates clustering by using a second hash function to determine the ________.",
        "options": [
            "Initial index.",
            "Probe increment (step size).",
            "Data type of the key.",
            "Size of the array."
        ],
        "correct": 1,
        "explanation": "In Double Hashing, if the first hash function results in a collision, a second hash function `h2(key)` is used to calculate the step size. Since `h2` depends on the key itself, different keys that collided at the same initial index will follow different probe paths.",
        "topic": "Double Hashing"
    },
    {
        "question": "The 'Load Factor' () of a hash table is defined as:",
        "options": [
            "Array Size / Number of Elements",
            "Number of Elements / Array Size",
            "Number of Collisions / Array Size",
            "Hash Value / Key"
        ],
        "correct": 1,
        "explanation": "The Load Factor () represents how full the hash table is. It is the ratio of the number of items (n) currently in the table to the table size (m). As  increases, the probability of collisions increases and performance degrades.",
        "topic": "Load Factor"
    },
    {
        "question": "For a hash table using 'Separate Chaining', what is the average number of nodes in each linked list if the load factor is ?",
        "options": [
            "1",
            "",
            "^2",
            "log "
        ],
        "correct": 1,
        "explanation": "In separate chaining, the elements are distributed among the buckets. On average, the number of elements per bucket is exactly the load factor . If  = 2, it means there are twice as many elements as buckets, so each list has roughly 2 nodes.",
        "topic": "Separate Chaining"
    },
    {
        "question": "What is 'Rehashing'?",
        "options": [
            "Calculating the hash value of a key twice.",
            "The process of building a larger hash table and migrating data when the current table becomes too full.",
            "A way to delete all elements from the table.",
            "Encrypting the hash table for security."
        ],
        "correct": 1,
        "explanation": "When the load factor of a hash table becomes too high (e.g., > 0.5 for probing or > 1.0 for chaining), performance drops. Rehashing involves creating a new table (usually double the size and the next prime number) and re-inserting all existing elements into it.",
        "topic": "Rehashing"
    },
    {
        "question": "Which of the following describes a 'Skip List'?",
        "options": [
            "A stack that skips every other element.",
            "A probabilistic data structure that uses multiple layers of linked lists to allow O(log n) search.",
            "A list that only stores data if the key is a prime number.",
            "An array where every second element is NULL."
        ],
        "correct": 1,
        "explanation": "Skip Lists are an alternative to balanced trees. They consist of a base linked list with additional higher-level 'express' lanes that skip over many elements. By using random level assignments for nodes, they achieve logarithmic performance similar to AVL trees but are often easier to implement.",
        "topic": "Skip Lists"
    },
    {
        "question": "In a Skip List, the probability of a node being promoted to the next higher level is typically:",
        "options": [
            "100%",
            "50% (1/2)",
            "10%",
            "0%"
        ],
        "correct": 1,
        "explanation": "The standard Skip List uses a probability 'p' (usually 0.5). When a node is inserted, we 'flip a coin'. If it's heads, the node rises to level 2. We flip again; if heads, it rises to level 3. This ensures that roughly 1/2 of nodes are at level 2, 1/4 at level 3, etc.",
        "topic": "Skip Lists"
    },
    {
        "question": "What is the worst-case time complexity of searching in a Skip List?",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "While Skip Lists have an *average* case of O(log n), because they are probabilistic, there is a tiny mathematical chance that no 'express lanes' are created, causing the structure to function like a standard linked list (O(n)).",
        "topic": "Skip Lists"
    },
    {
        "question": "Why is the size of a hash table usually chosen to be a prime number?",
        "options": [
            "Prime numbers take up less space in memory.",
            "To improve the distribution of keys and reduce collisions, especially when the hash function uses modulo.",
            "Because computers can only perform modulo with prime numbers.",
            "Prime numbers prevent the table from being resized."
        ],
        "correct": 1,
        "explanation": "Using a prime number for the table size 'm' ensures that the keys are spread more evenly across the indices. If 'm' has many factors, patterns in the keys might cause them to cluster in only a few buckets. Primes minimize this pattern-based collision risk.",
        "topic": "Hashing Design"
    },
    {
        "question": "In the Table ADT, 'Search' complexity using Hashing is ideally ________.",
        "options": [
            "O(n)",
            "O(log n)",
            "O(1)",
            "O(n^2)"
        ],
        "correct": 2,
        "explanation": "The primary advantage of Hashing is its constant time performance. Since we calculate the index directly from the key, we don't need to traverse the data structure sequentially, leading to O(1) time complexity in the best/average case.",
        "topic": "Table ADT"
    },
    {
        "question": "When using Open Addressing, how do we 'Delete' an element without breaking the probe sequence?",
        "options": [
            "Set the index to NULL.",
            "Shift all subsequent elements up by one.",
            "Mark the slot as 'Deleted' (Lazy Deletion) so the probe sequence can continue through it.",
            "Delete the entire hash table and rebuild it."
        ],
        "correct": 2,
        "explanation": "In linear or quadratic probing, if you simply set a cell to NULL, a search for a later item in the same probe sequence will stop prematurely at that NULL, failing to find the item. 'Lazy Deletion' involves using a special flag to tell the search algorithm: 'this spot is empty but was once full; keep looking'.",
        "topic": "Open Addressing"
    },
    {
        "question": "A Skip List with 'n' elements and 'L' levels is similar in structure and performance to a ________.",
        "options": [
            "Stack.",
            "Balanced Binary Search Tree.",
            "Queue.",
            "Unordered Array."
        ],
        "correct": 1,
        "explanation": "Because Skip Lists provide O(log n) search, insertion, and deletion, they are functionally identical to balanced trees like AVL or Red-Black trees. They provide a simpler, pointer-based alternative to tree balancing rotations.",
        "topic": "Skip Lists"
    },
    {
        "question": "Which collision resolution strategy is most sensitive to the hash table becoming full?",
        "options": [
            "Separate Chaining.",
            "Open Addressing (Probing).",
            "Rehashing.",
            "None of these."
        ],
        "correct": 1,
        "explanation": "Open addressing (Linear/Quadratic probing) requires empty slots in the array to resolve collisions. As the table fills ( approaches 1.0), the number of probes increases drastically. Separate Chaining (Option 0) can technically handle a load factor greater than 1.0 by growing the linked lists.",
        "topic": "Collision Resolution"
    },
    {
        "question": "The 'Division Method' for hashing uses which mathematical operator?",
        "options": [
            "Addition (+)",
            "Multiplication (*)",
            "Modulo (%)",
            "Subtraction (-)"
        ],
        "correct": 2,
        "explanation": "The division method calculates the index as `key % table_size`. This is the most common and simplest way to map a large key into the bounds of an array.",
        "topic": "Hash Functions"
    },
    {
        "question": "In a Skip List, the bottom-most level (Level 1) always contains ________.",
        "options": [
            "Only the root.",
            "All the elements in sorted order.",
            "Only the largest element.",
            "NULL pointers only."
        ],
        "correct": 1,
        "explanation": "Level 1 of a Skip List is a complete, sorted linked list containing every single element that has been inserted. Higher levels act as shortcuts to skip over these elements.",
        "topic": "Skip Lists"
    },
    {
        "question": "Double Hashing requires that the value of the second hash function, h2(key), must ________.",
        "options": [
            "Be zero.",
            "Be a prime number.",
            "Never be zero.",
            "Always be 1."
        ],
        "correct": 2,
        "explanation": "The second hash function determines the step size for probing. If `h2(key)` returned zero, the algorithm would stay at the same index forever in an infinite loop. Therefore, it must be designed to return at least 1.",
        "topic": "Double Hashing"
    },
    {
        "question": "Which of the following data structures can be used to implement a 'Symbol Table' in a compiler?",
        "options": [
            "Stack",
            "Queue",
            "Hash Table",
            "Priority Queue"
        ],
        "correct": 2,
        "explanation": "Compilers need to look up variables, functions, and keywords extremely quickly. A Hash Table is the standard implementation for a Symbol Table because it allows for O(1) average lookup time.",
        "topic": "Table Applications"
    },
    {
        "question": "If  = 0.5 in a Linear Probing hash table, the average number of probes for a successful search is approximately:",
        "options": [
            "1.5",
            "2.5",
            "5.0",
            "0.5"
        ],
        "correct": 0,
        "explanation": "The formula for the expected number of probes in a successful search with linear probing is `0.5 * (1 + 1 / (1 - ))`. For  = 0.5, this is `0.5 * (1 + 2) = 1.5`. As  grows, this number increases sharply.",
        "topic": "Hash Table Analysis"
    },
    {
        "question": "Quadratic probing can fail to find an empty slot even if the table is only half full if the ________ is not prime.",
        "options": [
            "Key",
            "Table Size",
            "Data Value",
            "Increment"
        ],
        "correct": 1,
        "explanation": "A known theorem states that if the table size is prime and the table is at least half empty (  0.5), Quadratic Probing is guaranteed to find an empty cell. If the size is not prime, the probe sequence might cycle through only a subset of indices.",
        "topic": "Quadratic Probing"
    },
    {
        "question": "What is the primary trade-off in choosing a larger hash table size?",
        "options": [
            "Faster search time vs. increased memory usage.",
            "Slower search time vs. decreased memory usage.",
            "Better security vs. slower CPU speed.",
            "None of these."
        ],
        "correct": 0,
        "explanation": "Increasing the table size reduces the load factor (), which minimizes collisions and leads to faster O(1) searches. However, it uses more memory because much of the array will be empty.",
        "topic": "Hashing Design"
    },
    {
        "question": "In separate chaining, the time complexity to search for a key in the worst case is ________.",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n^2)"
        ],
        "correct": 2,
        "explanation": "In the absolute worst case (a very bad hash function), all 'n' elements could hash to the same bucket. The structure would then behave like a single linked list of length 'n', requiring O(n) time to search.",
        "topic": "Separate Chaining"
    },
    {
        "question": "Which of the following is an example of a 'String Hashing' technique?",
        "options": [
            "Adding the ASCII values of characters and taking the modulo.",
            "Using only the first letter of the string.",
            "Calculating the string length.",
            "Sorting the string alphabetically."
        ],
        "correct": 0,
        "explanation": "A simple string hash function involves summing the numeric (ASCII/Unicode) values of its characters. More advanced versions use polynomial rolling hashes to ensure that different permutations (like 'abc' and 'cba') result in different hash values.",
        "topic": "Hash Functions"
    },
    {
        "question": "The efficiency of a Skip List search is O(log n) because:",
        "options": [
            "It uses binary search on an array.",
            "The number of levels is approximately log n, and we only do a few comparisons per level.",
            "It is a type of binary tree.",
            "It deletes half the elements every time it is searched."
        ],
        "correct": 1,
        "explanation": "Like a binary tree, each level we move down in a Skip List reduces the remaining search space by a factor (usually 1/2). This geometric progression ensures we only visit O(log n) nodes.",
        "topic": "Skip Lists"
    },
    {
        "question": "In Double Hashing, the formula for the i-th probe is `(h1(key) + i * h2(key)) % size`. What is h1?",
        "options": [
            "The step size function.",
            "The primary hash function.",
            "The key itself.",
            "The number of elements."
        ],
        "correct": 1,
        "explanation": "h1 is the primary hash function that gives the initial 'home' position. If that position is full, h2 is used to determine how far to jump to find the next position.",
        "topic": "Double Hashing"
    },
    {
        "question": "A 'Perfect Hash Function' is one that:",
        "options": [
            "Uses no memory.",
            "Produces zero collisions for a fixed set of keys.",
            "Works on any data type.",
            "Can never be calculated."
        ],
        "correct": 1,
        "explanation": "If the set of keys is known in advance (static), it is possible to construct a perfect hash function that maps each key to a unique index with no collisions at all.",
        "topic": "Hash Functions"
    },
    {
        "question": "In a Skip List, we start searching from the ________ level.",
        "options": [
            "Bottom-most",
            "Top-most",
            "Middle",
            "Random"
        ],
        "correct": 1,
        "explanation": "We always start at the highest level where the 'skips' are largest. We move right until the next element is larger than the search key, then we drop down one level and repeat until we find the key at the bottom level.",
        "topic": "Skip Lists"
    },
    {
        "question": "Hashing is NOT suitable for which of the following operations?",
        "options": [
            "Exact match search.",
            "Finding the maximum or minimum element.",
            "Inserting new records.",
            "Updating existing records."
        ],
        "correct": 1,
        "explanation": "Hash tables are designed for O(1) exact lookups. Because they distribute keys randomly, they do not maintain any order. Finding the min or max requires a full O(n) scan of the table. Trees are better for order-based operations.",
        "topic": "Hashing Limitations"
    },
    {
        "question": "A Skip List node contains ________.",
        "options": [
            "Only one pointer to the next node.",
            "A variable-sized array of pointers (one for each level it belongs to).",
            "A parent pointer and two child pointers.",
            "An index to an array."
        ],
        "correct": 1,
        "explanation": "Each node in a Skip List has a 'height' (randomly assigned). A node of height 'k' contains an array of 'k' pointers, allowing it to be linked into 'k' different levels of the data structure.",
        "topic": "Skip Lists"
    },
    {
        "question": "Which of the following mathematical structures best defines a 'Graph' in data structures?",
        "options": [
            "A linear sequence of nodes connected by pointers.",
            "A collection of vertices (V) and a collection of edges (E) that connect them.",
            "A hierarchical structure where each node has exactly one parent.",
            "A set of keys mapped to indices using a hash function."
        ],
        "correct": 1,
        "explanation": "A Graph is a non-linear data structure consisting of a finite set of vertices (or nodes) and a set of edges that connect pairs of vertices. Unlike trees (Option 2), graphs do not have a strict parent-child hierarchy and can contain cycles. Option 0 describes a linked list, and Option 3 describes a hash table.",
        "topic": "Graph Theory"
    },
    {
        "question": "In a 'Directed Graph' (Digraph), what does an edge (u, v) signify?",
        "options": [
            "There is a bidirectional path between u and v.",
            "There is a path from u to v, but not necessarily from v to u.",
            "Vertex u and vertex v are the same node.",
            "The edge has a specific weight or cost associated with it."
        ],
        "correct": 1,
        "explanation": "In a directed graph, edges have a specific orientation. An edge (u, v) represents a one-way link from the source vertex u to the target vertex v. If a path exists in the opposite direction, it must be explicitly defined as another edge (v, u). Bidirectional paths (Option 0) are characteristic of undirected graphs.",
        "topic": "Directed Graphs"
    },
    {
        "question": "What is the 'Degree' of a vertex in an undirected graph?",
        "options": [
            "The total number of vertices in the graph.",
            "The number of edges incident to the vertex.",
            "The length of the longest path starting from that vertex.",
            "The number of cycles the vertex belongs to."
        ],
        "correct": 1,
        "explanation": "In an undirected graph, the degree of a vertex is simply the count of edges connected to it. For directed graphs, this is further split into 'in-degree' (edges coming in) and 'out-degree' (edges going out).",
        "topic": "Graph Terminology"
    },
    {
        "question": "Which data structure is most space-efficient for representing a 'Sparse Graph' (a graph with relatively few edges)?",
        "options": [
            "Adjacency Matrix",
            "Adjacency List",
            "Two-dimensional Array",
            "Max-Heap"
        ],
        "correct": 1,
        "explanation": "An Adjacency List only stores the edges that actually exist, using an array of linked lists. For a sparse graph with V vertices and E edges, it uses O(V + E) space. An Adjacency Matrix (Option 0) always uses O(V^2) space, which wastes significant memory on zero-entries when edges are few.",
        "topic": "Graph Representations"
    },
    {
        "question": "In an 'Adjacency Matrix' representation of an undirected graph, if there is an edge between vertex 2 and vertex 5, which entries in the matrix will be 1?",
        "options": [
            "Only Matrix[2,5]",
            "Only Matrix[5,2]",
            "Both Matrix[2,5] and Matrix[5,2]",
            "Matrix[2,2] and Matrix[5,5]"
        ],
        "correct": 2,
        "explanation": "The adjacency matrix of an undirected graph is always symmetric across the main diagonal. Because the relationship is bidirectional, if 2 is connected to 5, then 5 is also connected to 2. Therefore, both entries (2,5) and (5,2) must be marked.",
        "topic": "Adjacency Matrix"
    },
    {
        "question": "What is the time complexity to check if an edge exists between two vertices 'u' and 'v' in an Adjacency Matrix?",
        "options": [
            "O(1)",
            "O(V)",
            "O(E)",
            "O(log V)"
        ],
        "correct": 0,
        "explanation": "An adjacency matrix provides instant access via array indexing. To check if vertex u and v are connected, you simply look up Matrix[u,v], which is a constant time operation, O(1). This is the primary advantage of matrices over lists.",
        "topic": "Adjacency Matrix"
    },
    {
        "question": "A graph that contains no cycles is referred to as an:",
        "options": [
            "Undirected Graph",
            "Complete Graph",
            "Acyclic Graph",
            "Weighted Graph"
        ],
        "correct": 2,
        "explanation": "An acyclic graph is one where it is impossible to start at a vertex and follow a sequence of edges that eventually leads back to the same vertex. A common example is a Directed Acyclic Graph (DAG).",
        "topic": "Graph Types"
    },
    {
        "question": "In an Adjacency List, how is the 'out-degree' of a vertex 'u' determined?",
        "options": [
            "By looking at the size of the array.",
            "By counting the number of nodes in the linked list at index 'u'.",
            "By checking if the vertex has a pointer to the root.",
            "It cannot be determined from an adjacency list."
        ],
        "correct": 1,
        "explanation": "In an adjacency list, each index 'u' in the array points to a list of all vertices 'v' such that an edge (u, v) exists. Therefore, the length of that specific linked list is exactly the out-degree of vertex u.",
        "topic": "Adjacency List"
    },
    {
        "question": "What is a 'Path' in a graph?",
        "options": [
            "A set of all vertices in the graph.",
            "A sequence of vertices such that each adjacent pair is connected by an edge.",
            "An edge that connects a vertex to itself.",
            "The total number of edges in a complete graph."
        ],
        "correct": 1,
        "explanation": "A path is a traversal through the graph. It is defined as a sequence of vertices v1, v2, ..., vk where there is an edge between each vi and vi+1. If all vertices in the sequence are distinct, it is called a simple path.",
        "topic": "Graph Terminology"
    },
    {
        "question": "A 'Complete Graph' with 'n' vertices has how many edges in total (undirected)?",
        "options": [
            "n",
            "n - 1",
            "n(n - 1) / 2",
            "n^2"
        ],
        "correct": 2,
        "explanation": "In a complete graph, every vertex is connected to every other vertex. For n vertices, each vertex has n-1 edges. Summing these gives n(n-1), but we divide by 2 because each edge is counted twice in an undirected context. This results in O(V^2) edge density.",
        "topic": "Complete Graphs"
    },
    {
        "question": "Which of the following is an advantage of Adjacency Lists over Adjacency Matrices?",
        "options": [
            "Faster edge lookup between two specific vertices.",
            "Easier to implement using simple 2D arrays.",
            "Efficiency in terms of both space and finding all neighbors of a vertex.",
            "Ability to store weights more effectively."
        ],
        "correct": 2,
        "explanation": "Adjacency lists are superior for sparse graphs because they save space. Furthermore, finding all neighbors of a vertex 'u' takes O(degree(u)) time, whereas a matrix requires scanning the entire row of length V, taking O(V) time regardless of how many neighbors exist.",
        "topic": "Graph Representations"
    },
    {
        "question": "A 'Self-loop' is an edge that:",
        "options": [
            "Connects two different components of a graph.",
            "Connects a vertex to itself.",
            "Is part of a cycle.",
            "Has a weight of zero."
        ],
        "correct": 1,
        "explanation": "A self-loop (or buckle) is an edge (u, u) that starts and ends at the same vertex. Most simple graphs exclude self-loops by definition.",
        "topic": "Graph Terminology"
    },
    {
        "question": "What is the maximum number of edges in a directed graph with 'n' vertices (assuming no self-loops)?",
        "options": [
            "n",
            "n(n - 1)",
            "n(n - 1) / 2",
            "2n"
        ],
        "correct": 1,
        "explanation": "In a directed graph, the edge (u, v) is distinct from (v, u). Each of the 'n' vertices can have a directed edge to the other 'n-1' vertices. Therefore, the total number of possible edges is n * (n-1).",
        "topic": "Directed Graphs"
    },
    {
        "question": "Which graph traversal technique uses a Queue data structure?",
        "options": [
            "Depth First Search (DFS)",
            "Breadth First Search (BFS)",
            "Binary Search",
            "Linear Search"
        ],
        "correct": 1,
        "explanation": "Breadth First Search (BFS) explores the graph layer by layer, visiting all neighbors of a vertex before moving to the next level. To maintain this order, a FIFO (Queue) is used. DFS (Option 0) uses a Stack (or recursion).",
        "topic": "Graph Traversals"
    },
    {
        "question": "In a 'Weighted Graph', the adjacency matrix stores ________ instead of just 1s.",
        "options": [
            "The degree of the vertex.",
            "The value of the weight/cost of the edge.",
            "The number of neighbors.",
            "The total number of vertices."
        ],
        "correct": 1,
        "explanation": "For weighted graphs, the matrix entry Matrix[u,v] typically holds the weight of the edge between u and v. If no edge exists, a special value like Infinity (or -1) is used.",
        "topic": "Weighted Graphs"
    },
    {
        "question": "What is a 'Connected Graph' (undirected)?",
        "options": [
            "A graph where every vertex has at least one edge.",
            "A graph where there is a path between every pair of vertices.",
            "A graph that has no cycles.",
            "A graph where every vertex is connected to itself."
        ],
        "correct": 1,
        "explanation": "An undirected graph is connected if you can travel from any vertex to any other vertex through some sequence of edges. If a graph is not connected, it consists of several 'connected components'.",
        "topic": "Graph Terminology"
    },
    {
        "question": "The Adjacency Matrix of a graph with 100 vertices and 100 edges will have how many total entries?",
        "options": [
            "100",
            "200",
            "10,000",
            "1,000"
        ],
        "correct": 2,
        "explanation": "An adjacency matrix is always a square matrix of size V x V. For 100 vertices, the matrix is 100 * 100 = 10,000 entries, regardless of how many edges exist.",
        "topic": "Adjacency Matrix"
    },
    {
        "question": "Which of the following is true for an Adjacency List representation of a Directed Graph?",
        "options": [
            "The sum of lengths of all linked lists equals the number of edges (E).",
            "The sum of lengths of all linked lists equals 2 * E.",
            "Every vertex must be connected to every other vertex.",
            "The matrix must be symmetric."
        ],
        "correct": 0,
        "explanation": "In a directed graph, each edge (u, v) is stored exactly once (in the list for vertex u). Therefore, the total number of nodes in all lists combined is exactly E. For undirected graphs, each edge is stored twice, making the sum 2E (Option 1).",
        "topic": "Adjacency List"
    },
    {
        "question": "A 'Cycle' in a graph is a path that:",
        "options": [
            "Has no edges.",
            "Starts and ends at the same vertex.",
            "Includes all vertices in the graph.",
            "Is always directed."
        ],
        "correct": 1,
        "explanation": "A cycle is a path of length 1 or more that begins and ends at the same vertex with no repeated edges.",
        "topic": "Graph Terminology"
    },
    {
        "question": "If a graph is represented by an Adjacency Matrix, finding the 'in-degree' of a vertex 'v' requires:",
        "options": [
            "Scanning the row of vertex v.",
            "Scanning the column of vertex v.",
            "Checking the diagonal entry.",
            "It is always constant time."
        ],
        "correct": 1,
        "explanation": "In an adjacency matrix for a directed graph, the row index represents the 'from' vertex and the column represents the 'to' vertex. To see how many edges come 'in' to vertex v, you must count the 1s in column v, which takes O(V) time.",
        "topic": "Adjacency Matrix"
    },
    {
        "question": "In a graph, vertices that are connected by an edge are called:",
        "options": [
            "Ancestors",
            "Adjacent vertices (Neighbors)",
            "Successors",
            "Leaves"
        ],
        "correct": 1,
        "explanation": "Adjacent vertices, also known as neighbors, are nodes that have a direct edge connecting them.",
        "topic": "Graph Terminology"
    },
    {
        "question": "A graph with many edges (close to V^2) is called a ________ graph.",
        "options": [
            "Sparse",
            "Dense",
            "Complete",
            "Balanced"
        ],
        "correct": 1,
        "explanation": "A dense graph is one where the number of edges is close to the maximum possible number of edges. For dense graphs, an adjacency matrix is often preferred over an adjacency list.",
        "topic": "Graph Density"
    },
    {
        "question": "What is a 'Sub-graph'?",
        "options": [
            "A graph that is smaller than another graph.",
            "A graph whose vertices and edges are subsets of another graph.",
            "A graph with negative weights.",
            "The linked list part of an adjacency list."
        ],
        "correct": 1,
        "explanation": "A graph G' = (V', E') is a subgraph of G = (V, E) if V' is a subset of V and E' is a subset of E such that every edge in E' connects vertices in V'.",
        "topic": "Graph Terminology"
    },
    {
        "question": "What happens to an Adjacency Matrix if we add a new vertex to the graph?",
        "options": [
            "One new node is added to a linked list.",
            "A new row and a new column must be added to the matrix.",
            "The matrix stays the same size.",
            "The matrix is cleared and rebuilt."
        ],
        "correct": 1,
        "explanation": "Since a matrix is V x V, increasing V to V+1 requires adding both a new row and a new column, which can be an expensive O(V^2) operation if the array needs to be reallocated.",
        "topic": "Adjacency Matrix"
    },
    {
        "question": "Which of the following is NOT a valid graph representation?",
        "options": [
            "Adjacency Matrix",
            "Adjacency List",
            "Edge List",
            "Binary Search Tree"
        ],
        "correct": 3,
        "explanation": "A Binary Search Tree is a specific type of hierarchical data structure. While all trees are technically graphs, a BST is not a general method for representing any arbitrary graph (which might have cycles or multiple components).",
        "topic": "Graph Representations"
    },
    {
        "question": "The 'In-degree' of a vertex in a directed graph is:",
        "options": [
            "The number of edges pointing away from it.",
            "The number of edges pointing towards it.",
            "The total number of vertices it can reach.",
            "The number of self-loops it has."
        ],
        "correct": 1,
        "explanation": "In-degree refers to incoming edges. In social media terms, it's like the number of 'followers' a user has.",
        "topic": "Graph Terminology"
    },
    {
        "question": "A path that visits every vertex in the graph exactly once is called a:",
        "options": [
            "Eulerian Path",
            "Hamiltonian Path",
            "Shortest Path",
            "Complete Path"
        ],
        "correct": 1,
        "explanation": "A Hamiltonian path is a path in an undirected or directed graph that visits each vertex exactly once.",
        "topic": "Graph Paths"
    },
    {
        "question": "If a graph has 5 vertices, how many entries are on the main diagonal of its adjacency matrix?",
        "options": [
            "5",
            "10",
            "25",
            "0"
        ],
        "correct": 0,
        "explanation": "A V x V matrix has exactly V diagonal entries (where row index = column index). These entries represent self-loops.",
        "topic": "Adjacency Matrix"
    },
    {
        "question": "Which representation is better for an algorithm that frequently needs to find all nodes adjacent to a given node?",
        "options": [
            "Adjacency Matrix",
            "Adjacency List",
            "Sorted Array",
            "Stack"
        ],
        "correct": 1,
        "explanation": "In an adjacency list, all neighbors of a node are stored directly in its associated linked list. You only visit the neighbors. In a matrix, you have to check every single column in that node's row.",
        "topic": "Graph Representations"
    },
    {
        "question": "What is the degree of a vertex in a complete graph with n vertices?",
        "options": [
            "n",
            "n - 1",
            "1",
            "n / 2"
        ],
        "correct": 1,
        "explanation": "In a complete graph, a vertex is connected to all other vertices except itself. Hence, its degree is n - 1.",
        "topic": "Complete Graphs"
    },
    {
        "question": "A directed graph is 'Strongly Connected' if:",
        "options": [
            "Every vertex has an out-degree of 1.",
            "There is a directed path between every pair of vertices in both directions.",
            "It contains at least one cycle.",
            "It can be represented by a symmetric matrix."
        ],
        "correct": 1,
        "explanation": "Strong connectivity in digraphs is the equivalent of 'connectedness' in undirected graphs, but it must account for the direction of edges.",
        "topic": "Graph Connectivity"
    },
    {
        "question": "In an Adjacency Matrix, what does an entry of 0 at Matrix[i,j] mean?",
        "options": [
            "The weight of the edge is zero.",
            "There is no edge from vertex i to vertex j.",
            "Vertex i is the root.",
            "The graph is complete."
        ],
        "correct": 1,
        "explanation": "In a standard unweighted graph matrix, 1 represents an edge and 0 represents the absence of an edge.",
        "topic": "Adjacency Matrix"
    },
    {
        "question": "Which of the following describes a 'Bipartite Graph'?",
        "options": [
            "A graph with two roots.",
            "A graph whose vertices can be divided into two disjoint sets such that no two vertices within the same set are adjacent.",
            "A graph that has two edges between every pair of vertices.",
            "A graph that can be drawn in two dimensions."
        ],
        "correct": 1,
        "explanation": "Bipartite graphs are used to model relationships between two different classes of objects, such as students and courses.",
        "topic": "Graph Types"
    },
    {
        "question": "Space complexity of Adjacency List is:",
        "options": [
            "O(V)",
            "O(V^2)",
            "O(V + E)",
            "O(E^2)"
        ],
        "correct": 2,
        "explanation": "We store V head pointers and E edge nodes across all lists. Total space is O(V + E).",
        "topic": "Graph Representations"
    },
    {
        "question": "Graphs are considered 'Non-Linear' data structures because:",
        "options": [
            "They are stored in non-linear memory.",
            "They allow for multiple paths and cycles between nodes.",
            "They can only store non-numeric data.",
            "They use more than one array."
        ],
        "correct": 1,
        "explanation": "Linear structures have a unique next/previous relationship. In a graph, a node can be connected to any number of other nodes in various patterns.",
        "topic": "Graph Theory"
    },
    {
        "question": "When evaluating the efficiency of an algorithm, why is 'Big-O notation' preferred over measuring execution time in seconds?",
        "options": [
            "Seconds are too small a unit to measure modern CPU speeds.",
            "Execution time depends on specific hardware and compiler, while Big-O measures the growth rate independent of hardware.",
            "Big-O notation is easier to calculate using a stopwatch.",
            "Big-O notation only applies to the Standard Template Library (STL)."
        ],
        "correct": 1,
        "explanation": "Wall-clock time varies significantly based on CPU architecture, RAM, and background processes. Big-O notation provides a mathematical framework to describe how the number of operations scales as the input size (n) increases, making it a universal standard for algorithmic comparison. Options 0 and 2 are scientifically incorrect, and Option 3 is false as Big-O is a general theoretical concept.",
        "topic": "Algorithm Analysis"
    },
    {
        "question": "Which of the following growth rates represents the most efficient (fastest) search performance for large datasets?",
        "options": [
            "O(n)",
            "O(n^2)",
            "O(log n)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "Logarithmic growth O(log n) is significantly more efficient than linear O(n) or quadratic O(n^2) growth for large values of 'n'. For example, if n is 1,000,000, log n is approximately 20, whereas n log n is 20,000,000. Balanced trees and binary search provide this high-performance characteristic.",
        "topic": "Complexity Classes"
    },
    {
        "question": "If an algorithm's running time is described as O(1), what does this imply about the input size 'n'?",
        "options": [
            "The running time increases linearly with n.",
            "The running time is constant and does not depend on n.",
            "The algorithm can only handle one input at a time.",
            "The algorithm is exponentially slow."
        ],
        "correct": 1,
        "explanation": "O(1) denotes 'Constant Time'. This means the time taken to complete the operation remains the same whether the input size is 10 or 10 million. Examples include accessing an element in an array by index or pushing an element onto a stack.",
        "topic": "Big-O Notation"
    },
    {
        "question": "In the summary of Lecture 45, which data structure is described as a 'companion' to an algorithm used for building symbol tables in compilers?",
        "options": [
            "Stack",
            "Queue",
            "Hash Table",
            "Linked List"
        ],
        "correct": 2,
        "explanation": "As noted in the final lecture, a Hash Table is a purely algorithmic procedure that becomes a companion to compilers for building symbol tables. This is because compilers need to look up identifiers (variables/functions) in O(1) average time, which hashing provides efficiently.",
        "topic": "Data Structure Selection"
    },
    {
        "question": "According to the handouts, what is the 'worst-case' complexity of an algorithm?",
        "options": [
            "The minimum time required for any input of size n.",
            "The maximum time required for any input of size n.",
            "The average time taken over many executions.",
            "The time it takes to compile the code."
        ],
        "correct": 1,
        "explanation": "Worst-case analysis provides a guaranteed upper bound on the performance of an algorithm. It ensures that the software will never perform worse than a certain limit, which is critical for real-time and safety-critical systems. Average case (Option 2) is often harder to calculate as it requires knowing the distribution of inputs.",
        "topic": "Algorithm Analysis"
    },
    {
        "question": "Which data structure would be the best choice for an application that requires frequent 'Undo' (Last-In, First-Out) functionality?",
        "options": [
            "Queue",
            "Stack",
            "Heap",
            "Binary Search Tree"
        ],
        "correct": 1,
        "explanation": "The 'Undo' operation requires retrieving the most recent action first, which perfectly matches the LIFO (Last-In, First-Out) property of a Stack. Queues (Option 0) are used for FIFO tasks like print scheduling.",
        "topic": "Data Structure Selection"
    },
    {
        "question": "In Big-O analysis, what is the complexity of a nested loop where the outer loop runs 'n' times and the inner loop runs 'n' times?",
        "options": [
            "O(2n)",
            "O(n^2)",
            "O(log n)",
            "O(n log n)"
        ],
        "correct": 1,
        "explanation": "For every iteration of the outer loop, the inner loop executes 'n' operations. Therefore, the total number of operations is n * n = n^2. This quadratic complexity is typical of simpler sorting algorithms like Bubble or Selection Sort.",
        "topic": "Algorithm Analysis"
    },
    {
        "question": "Which term describes a data structure that allows O(log n) search but is probabilistic and uses multiple layers of linked lists?",
        "options": [
            "AVL Tree",
            "Skip List",
            "Binary Heap",
            "Hash Table"
        ],
        "correct": 1,
        "explanation": "Skip Lists are mentioned in the Table ADT section as a recent data structure that provides the same logarithmic performance as balanced trees but uses randomization (coin flips) to build levels instead of complex rotations.",
        "topic": "Skip Lists"
    },
    {
        "question": "If you need to find the shortest path in a networking application, which data structure is primarily relevant from an 'algorithmic' point of view?",
        "options": [
            "Stack",
            "Binary Search Tree",
            "Graph",
            "Circular List"
        ],
        "correct": 2,
        "explanation": "As discussed in the final lectures, Graphs are critical for algorithmic problems like pathfinding, even if they aren't implemented as a basic ADT as often as stacks or lists. Dijkstras algorithm, for example, operates on a Graph structure.",
        "topic": "Graph ADT"
    },
    {
        "question": "What does the 'Growth Rate' of an algorithm specifically measure?",
        "options": [
            "How the memory usage increases over time.",
            "How the execution time increases as the input size increases.",
            "How many programmers are needed to maintain the code.",
            "The rate at which new nodes are added to a tree."
        ],
        "correct": 1,
        "explanation": "The growth rate (or order of growth) describes the mathematical relationship between the size of the problem (n) and the resources (usually time) required to solve it.",
        "topic": "Algorithm Analysis"
    },
    {
        "question": "Which operation in a 'Binary Search Tree' becomes O(n) in the worst case if the tree is not balanced?",
        "options": [
            "Searching",
            "Insertion",
            "Deletion",
            "All of the above"
        ],
        "correct": 3,
        "explanation": "If a BST is unbalanced (skewed), it effectively becomes a linked list. In this scenario, every operation that requires traversing the tree (Search, Insert, or Delete) must potentially visit every node, leading to linear O(n) complexity.",
        "topic": "BST Analysis"
    },
    {
        "question": "In the Standard Template Library (STL), the 'vector' container is an example of:",
        "options": [
            "A linked list.",
            "A dynamic array.",
            "A balanced tree.",
            "A stack."
        ],
        "correct": 1,
        "explanation": "An STL vector is a template class that manages a dynamic array. It allows for O(1) random access but handles the complexity of resizing and memory management automatically.",
        "topic": "STL Containers"
    },
    {
        "question": "What is the time complexity of a 'Binary Search' on a sorted array of size n?",
        "options": [
            "O(n)",
            "O(log n)",
            "O(n^2)",
            "O(1)"
        ],
        "correct": 1,
        "explanation": "Binary search divides the search space in half with every comparison. This repeated halving is a classic example of logarithmic O(log n) efficiency.",
        "topic": "Searching Algorithms"
    },
    {
        "question": "Which data structure is best suited for a 'First-In, First-Out' (FIFO) process like a keyboard buffer?",
        "options": [
            "Stack",
            "Queue",
            "Binary Tree",
            "Hash Table"
        ],
        "correct": 1,
        "explanation": "In a keyboard buffer, the first key pressed must be the first one processed by the CPU to maintain the correct sequence of text. This is the definition of a FIFO Queue.",
        "topic": "Data Structure Selection"
    },
    {
        "question": "When  (Load Factor) in a hash table exceeds 0.5 for open addressing, what is the recommended action to maintain O(1) performance?",
        "options": [
            "Delete half the elements.",
            "Rehash (create a larger table and move data).",
            "Switch to a linked list.",
            "Ignore it; hashing is always O(1)."
        ],
        "correct": 1,
        "explanation": "As a hash table fills up, the probability of collisions increases exponentially, which slows down search and insertion. Rehashing into a larger table restores the low load factor necessary for constant-time performance.",
        "topic": "Hashing Analysis"
    },
    {
        "question": "Which of the following describes 'Space Complexity'?",
        "options": [
            "The amount of time an algorithm takes to run.",
            "The amount of memory an algorithm uses relative to input size.",
            "The number of lines of code in a program.",
            "The physical size of the computer."
        ],
        "correct": 1,
        "explanation": "Space complexity analysis measures the auxiliary memory (RAM) required by an algorithm to execute, excluding the memory used by the input itself.",
        "topic": "Algorithm Analysis"
    },
    {
        "question": "What is the Big-O complexity of the following code snippet?\n`for(int i=0; i<n; i++) { cout << i; }`",
        "options": [
            "O(1)",
            "O(n)",
            "O(n^2)",
            "O(log n)"
        ],
        "correct": 1,
        "explanation": "The loop executes exactly 'n' times, performing one output operation per iteration. This is a linear relationship between the input 'n' and the work done.",
        "topic": "Big-O Notation"
    },
    {
        "question": "Which data structure is 'Strictly Binary' but not necessarily a 'Binary Search Tree'?",
        "options": [
            "AVL Tree",
            "Binary Heap",
            "Linked List",
            "B-Tree"
        ],
        "correct": 1,
        "explanation": "A Binary Heap is a complete binary tree (which is a form of strict binary structure), but it follows the heap-order property (min/max) rather than the BST search-order property (left < root < right).",
        "topic": "Heaps"
    },
    {
        "question": "According to the final course review, why is the study of data structures and algorithms important for a 'Software Engineer'?",
        "options": [
            "To learn how to use a specific IDE.",
            "To increase domain knowledge of design choices and resolve design problems.",
            "To memorize the C++ syntax.",
            "To understand how to build physical hardware."
        ],
        "correct": 1,
        "explanation": "As stated in Lecture 45, the goal is to equip students with design choices. Choosing the right data structure (like a Hash Table vs. a Tree) allows an engineer to resolve specific performance problems in professional applications.",
        "topic": "Course Summary"
    },
    {
        "question": "Which complexity class represents an algorithm where doubling the input size (n) results in a four-fold increase in execution time?",
        "options": [
            "O(n)",
            "O(n^2)",
            "O(log n)",
            "O(2^n)"
        ],
        "correct": 1,
        "explanation": "In a quadratic O(n^2) algorithm, (2n)^2 = 4n^2. This means that if n doubles, the work quadruples. This is why O(n^2) algorithms are generally unsuitable for massive datasets.",
        "topic": "Complexity Classes"
    },
    {
        "question": "What is the time complexity to access the 'top' element of a stack implemented as a linked list?",
        "options": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ],
        "correct": 0,
        "explanation": "In a linked list implementation of a stack, the 'top' is always the head of the list. Since we maintain a pointer to the head, accessing its value is a direct, constant-time operation.",
        "topic": "Stack Analysis"
    },
    {
        "question": "Which of the following is true regarding 'Generic Programming' in C++?",
        "options": [
            "It allows writing code that works with only one data type.",
            "It uses 'templates' to write code that is independent of specific data types.",
            "It makes the program run slower because of templates.",
            "It is only used for implementing linked lists."
        ],
        "correct": 1,
        "explanation": "Generic programming (via C++ templates) allows developers to define a data structure (like `Stack<T>`) once and then use it with integers, strings, or custom objects without rewriting the logic.",
        "topic": "C++ Templates"
    },
    {
        "question": "If you need to maintain a list of items where you frequently insert and delete from the 'middle', which structure is most efficient?",
        "options": [
            "Array",
            "Linked List",
            "Stack",
            "Queue"
        ],
        "correct": 1,
        "explanation": "While finding the middle takes time, the actual insertion/deletion in a linked list only involves changing pointers O(1). In an array (Option 0), you would have to shift all subsequent elements, which is O(n).",
        "topic": "Data Structure Selection"
    },
    {
        "question": "What is the Big-O complexity of an algorithm that performs a 'Search' in a perfectly balanced Binary Search Tree?",
        "options": [
            "O(n)",
            "O(log n)",
            "O(n^2)",
            "O(1)"
        ],
        "correct": 1,
        "explanation": "A balanced BST (like an AVL tree) ensures that the height of the tree is always logarithmic. Since a search follows a single path from root to leaf, it takes O(log n) time.",
        "topic": "BST Analysis"
    },
    {
        "question": "In the analysis of Disjoint Sets, what is the impact of 'Path Compression' on the 'Find' operation?",
        "options": [
            "It makes the find operation take O(n) every time.",
            "It flattens the tree structure, making subsequent finds faster (nearly O(1)).",
            "It deletes the root node.",
            "It sorts the elements in the set."
        ],
        "correct": 1,
        "explanation": "Path compression updates all nodes along a search path to point directly to the root. This 'flattens' the tree, ensuring future searches for those nodes occur in constant time.",
        "topic": "Disjoint Set Analysis"
    },
    {
        "question": "Which notation provides the 'Lower Bound' of an algorithm's complexity?",
        "options": [
            "Big-O (O)",
            "Omega ()",
            "Theta ()",
            "Little-o (o)"
        ],
        "correct": 1,
        "explanation": "While Big-O provides the upper bound (worst case), Omega () provides the lower bound, describing the best-case performance for any input size.",
        "topic": "Complexity Notations"
    },
    {
        "question": "Which data structure provides the fastest access to the 'minimum' element in a dynamic set of data that changes frequently?",
        "options": [
            "Sorted Array",
            "Min-Heap",
            "Hash Table",
            "Singly Linked List"
        ],
        "correct": 1,
        "explanation": "A Min-Heap is designed specifically to keep the minimum element at the root. Accessing it is O(1), and restoring the property after extraction is O(log n). A sorted array (Option 0) would take O(n) to insert new elements.",
        "topic": "Heap Analysis"
    },
    {
        "question": "What is the Big-O complexity of a 'Linear Search' on an unordered array of size n?",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n^2)"
        ],
        "correct": 2,
        "explanation": "In the worst case of a linear search, the item is at the very end of the array or not present at all, requiring the algorithm to check all 'n' elements.",
        "topic": "Searching Algorithms"
    },
    {
        "question": "Which sorting algorithm is known as 'Divide and Conquer' and has an average complexity of O(n log n)?",
        "options": [
            "Bubble Sort",
            "Quicksort",
            "Insertion Sort",
            "Selection Sort"
        ],
        "correct": 1,
        "explanation": "Quicksort divides the array into sub-problems based on a pivot and sorts them recursively. Its average performance is O(n log n), making it one of the fastest general-purpose sorts.",
        "topic": "Quicksort Analysis"
    },
    {
        "question": "A 'Priority Queue' is logically most similar to which real-world scenario?",
        "options": [
            "A stack of trays in a cafeteria.",
            "An emergency room where patients are treated based on severity of injury.",
            "A line at a movie theater.",
            "A box of random items."
        ],
        "correct": 1,
        "explanation": "In a priority queue, elements are removed based on priority (severity), not just arrival time. This differentiates it from a standard FIFO queue (Option 2).",
        "topic": "Priority Queues"
    },
    {
        "question": "Which of the following is a limitation of 'Array-based' stack implementation?",
        "options": [
            "Accessing the top is slow.",
            "The maximum size must be known in advance.",
            "It uses more memory than a linked list for small n.",
            "It is only usable for integers."
        ],
        "correct": 1,
        "explanation": "Standard arrays are static. When implementing a stack with an array, you must pre-allocate a fixed size, which can lead to 'Stack Overflow' if the limit is reached, or wasted memory if it is not used.",
        "topic": "Stack Implementation"
    },
    {
        "question": "In Huffman coding, why do we use a 'Greedy' approach?",
        "options": [
            "To use as much memory as possible.",
            "To always merge the two lowest frequencies to build an optimal prefix tree.",
            "To sort the characters alphabetically.",
            "To encrypt the data."
        ],
        "correct": 1,
        "explanation": "Huffman's greedy strategy ensures that the most frequent characters have the shortest bit-codes, resulting in the most compressed file possible.",
        "topic": "Huffman Coding Analysis"
    },
    {
        "question": "What is the average time complexity to find an element in a 'Skip List'?",
        "options": [
            "O(n)",
            "O(log n)",
            "O(n log n)",
            "O(1)"
        ],
        "correct": 1,
        "explanation": "Skip Lists are designed to mimic the behavior of binary search over a linked list. By skipping levels, the search time becomes O(log n) on average.",
        "topic": "Skip List Analysis"
    },
    {
        "question": "Which of the following describes the 'Tight Bound' of an algorithm's complexity?",
        "options": [
            "Big-O (O)",
            "Omega ()",
            "Theta ()",
            "None of these"
        ],
        "correct": 2,
        "explanation": "Theta () notation is used when an algorithm's upper bound (O) and lower bound () are the same, providing a 'tight' description of its performance.",
        "topic": "Complexity Notations"
    },
    {
        "question": "In the final summary, what does the examiner suggest 'algorithm brings along'?",
        "options": [
            "A data structure companion.",
            "Higher costs.",
            "More errors.",
            "Physical hardware."
        ],
        "correct": 0,
        "explanation": "Lecture 45 concludes that 'algorithm will bring along some data structure along,' meaning the choice of algorithm often dictates which data structure is needed to solve a problem effectively.",
        "topic": "Course Summary"
    },
    {
        "question": "Which of the following describes the 'Infix to Postfix' conversion algorithm using a stack?",
        "options": [
            "Operands are pushed onto the stack; operators are printed immediately.",
            "Operators are pushed onto the stack based on precedence; operands are printed immediately.",
            "Both operands and operators are pushed onto the stack.",
            "The stack is used to store the final result of the expression."
        ],
        "correct": 1,
        "explanation": "In the Infix to Postfix algorithm, operands (like A, B, 1) are sent directly to the output string. Operators (+, -, *, /) are pushed onto the stack. If an operator with lower or equal precedence than the one on top arrives, the top is popped to the output before the new operator is pushed. This ensures the correct order of operations in Postfix (Reverse Polish) notation.",
        "topic": "Stack Applications"
    },
    {
        "question": "In a Circular Queue implemented with an array of size 'N', what is the formula to calculate the next position for the 'rear' pointer during an enqueue operation?",
        "options": [
            "rear = rear + 1",
            "rear = (rear + 1) % N",
            "rear = (rear - 1) % N",
            "rear = N - rear"
        ],
        "correct": 1,
        "explanation": "To make an array 'circular', we use the modulo operator (%). This allows the 'rear' pointer to wrap around to the first index (0) once it reaches the end of the array (N-1), provided there is space. Without the modulo operator (Option 0), the queue would be limited to a single pass through the array, wasting memory.",
        "topic": "Queue Implementation"
    },
    {
        "question": "What is the result of the following Postfix expression evaluation: 6 2 3 + * ?",
        "options": [
            "11",
            "30",
            "15",
            "12"
        ],
        "correct": 1,
        "explanation": "Using a stack for evaluation: 1. Push 6. 2. Push 2. 3. Push 3. 4. '+' arrives: pop 3 and 2, add them (5), push 5. 5. '*' arrives: pop 5 and 6, multiply them (30), push 30. The final result is 30. Option 3 would be the result if it were evaluated as (6*2)+3.",
        "topic": "Stack Applications"
    },
    {
        "question": "Which complexity class represents an algorithm that reduces the remaining search space by half in every step?",
        "options": [
            "O(n)",
            "O(n log n)",
            "O(log n)",
            "O(1)"
        ],
        "correct": 2,
        "explanation": "The 'Divide and Conquer' approach, such as Binary Search or balanced tree traversal, repeatedly halves the problem size. Mathematically, the number of steps required to reach size 1 from size n is log base 2 of n, denoted as O(log n). This is significantly more efficient than linear O(n) scans.",
        "topic": "Complexity Analysis"
    },
    {
        "question": "In the Standard Template Library (STL), which container is optimized for pushing and popping at both the beginning and the end?",
        "options": [
            "vector",
            "list",
            "deque",
            "stack"
        ],
        "correct": 2,
        "explanation": "The 'deque' (Double-Ended Queue) is specifically designed to allow efficient O(1) insertions and deletions at both ends. While a 'list' (Option 1) also allows this, 'deque' provides better cache locality. A 'vector' (Option 0) is only efficient at the end, and 'stack' (Option 3) only allows operations at one end.",
        "topic": "STL Containers"
    },
    {
        "question": "What is the primary advantage of a 'Priority Queue' over a standard 'Queue'?",
        "options": [
            "It uses less memory.",
            "It allows elements to be processed based on urgency rather than arrival time.",
            "It can only store integers.",
            "It is faster to implement using a simple array."
        ],
        "correct": 1,
        "explanation": "A standard queue is FIFO (First-In, First-Out). A Priority Queue allows elements with higher priority to be dequeued before elements with lower priority, even if the low-priority elements arrived first. This is essential for operating system task scheduling or network packet management.",
        "topic": "Queue ADT"
    },
    {
        "question": "If we use a Singly Linked List to implement a Queue, which pointers should we maintain for O(1) Enqueue and Dequeue operations?",
        "options": [
            "A 'head' pointer only.",
            "A 'tail' pointer only.",
            "Both 'front' (head) and 'rear' (tail) pointers.",
            "A pointer to the middle element."
        ],
        "correct": 2,
        "explanation": "To achieve O(1) efficiency in a linked-list queue, we dequeue from the front and enqueue at the rear. Maintaining both pointers allows us to jump directly to either end without traversing the entire list. If we only had a head pointer (Option 0), enqueuing at the end would take O(n).",
        "topic": "Queue Implementation"
    },
    {
        "question": "Which notation represents the 'tight' bound of an algorithm, where the upper and lower bounds are the same?",
        "options": [
            "O (Big-O)",
            " (Omega)",
            " (Theta)",
            "o (Little-o)"
        ],
        "correct": 2,
        "explanation": "Theta () notation is used when an algorithm's growth rate is bounded both above and below by the same function. It provides a more precise description of performance than Big-O, which only provides the maximum (upper bound).",
        "topic": "Complexity Notations"
    },
    {
        "question": "In a recursive function, what happens to the memory if the base case is never reached?",
        "options": [
            "The program runs faster.",
            "A 'Stack Overflow' error occurs as the call stack fills up with activation records.",
            "The computer automatically stops the recursion after 100 calls.",
            "The memory is cleared automatically by the compiler."
        ],
        "correct": 1,
        "explanation": "Every recursive call creates a new activation record on the system's 'Call Stack'. If there is no base case to stop the calls, the stack will grow until it exceeds the allocated memory limit, resulting in a 'Stack Overflow' crash. This is a common logical error in data structure implementation.",
        "topic": "Recursion"
    },
    {
        "question": "What is the time complexity of accessing an element at a specific index in an STL vector?",
        "options": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ],
        "correct": 0,
        "explanation": "An STL vector is internally implemented as a dynamic array. Because arrays store elements in contiguous memory locations, the address of any element can be calculated directly using the base address and the index, making access a constant-time O(1) operation.",
        "topic": "Algorithm Analysis"
    },
    {
        "question": "A data structure where elements are added at one end and removed from the other is a:",
        "options": [
            "Stack",
            "Queue",
            "Binary Tree",
            "Heap"
        ],
        "correct": 1,
        "explanation": "This is the definition of a Queue (FIFO). A Stack (Option 0) adds and removes from the same end (LIFO). Trees and Heaps are non-linear structures.",
        "topic": "Queue ADT"
    },
    {
        "question": "In the 'Median-of-Three' pivot selection for Quicksort, which three elements are typically compared?",
        "options": [
            "The first three elements of the array.",
            "The last three elements of the array.",
            "The first, middle, and last elements of the array.",
            "Three random elements chosen by the CPU."
        ],
        "correct": 2,
        "explanation": "Median-of-three selection takes the first, middle, and last elements, sorts them, and uses the median as the pivot. This strategy effectively avoids the worst-case O(n^2) performance on sorted or nearly-sorted data by ensuring a more balanced partition.",
        "topic": "Quicksort"
    },
    {
        "question": "Which of the following sorting algorithms is generally NOT 'In-place'?",
        "options": [
            "Bubble Sort",
            "Insertion Sort",
            "Merge Sort",
            "Selection Sort"
        ],
        "correct": 2,
        "explanation": "Merge Sort is a 'Divide and Conquer' algorithm that requires a temporary auxiliary array of size O(n) to merge the sub-arrays. This makes it NOT 'In-place'. The others (Options 0, 1, and 3) rearrange elements within the original array using only O(1) extra space.",
        "topic": "Sorting Algorithms"
    },
    {
        "question": "In a Max-Heap, the parent of the node at index 'i' (1-based) is found at index:",
        "options": [
            "2i",
            "2i + 1",
            "floor(i/2)",
            "i - 1"
        ],
        "correct": 2,
        "explanation": "In an array-based heap, the parent of any node 'i' is located at index i/2 (integer division). For example, nodes at indices 4 and 5 both have their parent at index 2.",
        "topic": "Heap Implementation"
    },
    {
        "question": "What is the purpose of 'Path Compression' in the Union-Find data structure?",
        "options": [
            "To sort the elements of the sets.",
            "To reduce the height of the trees, making future 'Find' operations faster.",
            "To delete sets that are too large.",
            "To merge sets based on alphabetical order."
        ],
        "correct": 1,
        "explanation": "Path compression makes every node on the path to the root point directly to the root during a 'Find' operation. This flattens the tree, ensuring that subsequent operations are extremely fast (nearly O(1) amortized).",
        "topic": "Disjoint Sets"
    },
    {
        "question": "The 'Load Factor' of a hash table is used to decide when to perform which operation?",
        "options": [
            "Insertion",
            "Deletion",
            "Rehashing",
            "Searching"
        ],
        "correct": 2,
        "explanation": "The Load Factor (number of elements / table size) indicates how full the table is. Once it reaches a certain threshold (usually 0.5 for probing or 1.0 for chaining), 'Rehashing' is performed to create a larger table and maintain efficient O(1) access.",
        "topic": "Hashing"
    },
    {
        "question": "Which traversal of a Binary Search Tree (BST) produces nodes in descending order?",
        "options": [
            "In-order (Left, Root, Right)",
            "Reverse In-order (Right, Root, Left)",
            "Pre-order",
            "Post-order"
        ],
        "correct": 1,
        "explanation": "In-order traversal of a BST produces sorted values in ascending order. By reversing the logic (Right subtree first, then Root, then Left subtree), we visit the largest values first, resulting in descending order.",
        "topic": "Tree Traversals"
    },
    {
        "question": "In Huffman Coding, characters with the highest frequency are assigned:",
        "options": [
            "The longest bit codes.",
            "The shortest bit codes.",
            "Only codes starting with '1'.",
            "Codes of equal length to all other characters."
        ],
        "correct": 1,
        "explanation": "Huffman Coding is a variable-length encoding scheme where frequently occurring characters are placed closer to the root of the tree, giving them shorter bit-strings to minimize the overall file size.",
        "topic": "Huffman Coding"
    },
    {
        "question": "Which data structure is most appropriate for checking 'Balanced Parentheses' in an expression?",
        "options": [
            "Queue",
            "Stack",
            "Linked List",
            "Binary Heap"
        ],
        "correct": 1,
        "explanation": "A Stack is used to store opening parentheses. When a closing parenthesis is encountered, we pop from the stack to see if it matches. If the stack is empty at the end and every closing bracket had a matching opening bracket, the expression is balanced.",
        "topic": "Stack Applications"
    },
    {
        "question": "A 'Strictly Binary Tree' with 'N' leaf nodes will always have how many total nodes?",
        "options": [
            "2N",
            "2N - 1",
            "N + 1",
            "log N"
        ],
        "correct": 1,
        "explanation": "In a strictly binary tree (where every node has 0 or 2 children), the number of internal nodes is N-1. Therefore, the total number of nodes is Leaf nodes + Internal nodes = N + (N-1) = 2N - 1.",
        "topic": "Binary Tree Properties"
    },
    {
        "question": "What is the average case time complexity of searching in a Skip List?",
        "options": [
            "O(n)",
            "O(log n)",
            "O(1)",
            "O(n log n)"
        ],
        "correct": 1,
        "explanation": "A Skip List uses multiple layers of linked lists with 'express lanes'. Statistically, it mimics the behavior of a balanced search tree, allowing for O(log n) average search, insert, and delete operations.",
        "topic": "Skip Lists"
    },
    {
        "question": "Which sorting algorithm is known for being 'Stable' by default?",
        "options": [
            "Quicksort",
            "Heapsort",
            "Insertion Sort",
            "Selection Sort"
        ],
        "correct": 2,
        "explanation": "Insertion Sort is stable because it only swaps elements if one is strictly smaller than the other, preserving the relative order of equal elements. Quicksort and Heapsort are generally unstable due to long-distance swaps.",
        "topic": "Sorting Algorithms"
    },
    {
        "question": "The 'degree' of a node in a tree is defined as:",
        "options": [
            "The height of the tree.",
            "The number of children that node has.",
            "The total number of ancestors.",
            "The number of levels below that node."
        ],
        "correct": 1,
        "explanation": "In tree terminology, the degree of a node is simply the number of its subtrees (children). For a binary tree, the maximum degree of any node is 2.",
        "topic": "Tree Terminology"
    },
    {
        "question": "Which pointer-based implementation uses NULL pointers to point to inorder predecessors and successors?",
        "options": [
            "AVL Tree",
            "Threaded Binary Tree",
            "B-Tree",
            "Binary Heap"
        ],
        "correct": 1,
        "explanation": "A Threaded Binary Tree 'threads' its unused NULL pointers to point to the nodes that would come before or after it in an inorder traversal, allowing for faster traversal without using a stack or recursion.",
        "topic": "Tree Variants"
    },
    {
        "question": "What is the time complexity of building a heap from an unordered array of size 'n'?",
        "options": [
            "O(n log n)",
            "O(n)",
            "O(n^2)",
            "O(log n)"
        ],
        "correct": 1,
        "explanation": "The 'Build-Heap' algorithm (using the percolate-down method starting from the last non-leaf node) is mathematically proven to run in linear O(n) time, which is more efficient than n consecutive insertions.",
        "topic": "Heap Operations"
    },
    {
        "question": "In a B-Tree of order 'm', every node except the root must have at least how many keys?",
        "options": [
            "1",
            "m/2",
            "ceil(m/2) - 1",
            "m - 1"
        ],
        "correct": 2,
        "explanation": "To ensure balance and efficient storage, B-Trees require nodes to be at least half-full. A node of order 'm' must have at least ceil(m/2) - 1 keys.",
        "topic": "B-Trees"
    },
    {
        "question": "Which data structure uses the 'Division Method' or 'Multiplication Method' to calculate storage locations?",
        "options": [
            "Binary Search Tree",
            "Skip List",
            "Hash Table",
            "Circular Queue"
        ],
        "correct": 2,
        "explanation": "These are common methods for creating a hash function. The Division Method uses `key % table_size` to map a key to a specific index in a Hash Table.",
        "topic": "Hashing"
    },
    {
        "question": "What is the primary drawback of 'Linear Probing' in collision resolution?",
        "options": [
            "It uses too much memory.",
            "It leads to 'Primary Clustering', where long runs of occupied slots increase search time.",
            "It is impossible to delete elements.",
            "It requires the use of multiple hash functions."
        ],
        "correct": 1,
        "explanation": "Linear Probing checks the next available slot sequentially. This often creates clusters of occupied cells, meaning new items that hash into that area must probe through the entire cluster, degrading performance.",
        "topic": "Hashing"
    },
    {
        "question": "A graph where every pair of distinct vertices is connected by a unique edge is called a:",
        "options": [
            "Connected Graph",
            "Complete Graph",
            "Directed Graph",
            "Acyclic Graph"
        ],
        "correct": 1,
        "explanation": "A Complete Graph is one where every possible edge between all vertices exists. For 'n' vertices, there are n(n-1)/2 edges.",
        "topic": "Graph Theory"
    },
    {
        "question": "Which of the following describes 'Big-O' notation?",
        "options": [
            "The average performance of an algorithm.",
            "The mathematical upper bound (worst-case) of an algorithm's growth rate.",
            "The exact number of milliseconds a program runs.",
            "The space complexity only."
        ],
        "correct": 1,
        "explanation": "Big-O notation describes the limiting behavior of a function. In computer science, it is used to classify algorithms by how their run-time or space requirements grow as the input size 'n' increases.",
        "topic": "Complexity Analysis"
    },
    {
        "question": "Which STL container is the best choice for storing a collection that needs to be sorted automatically at all times?",
        "options": [
            "vector",
            "list",
            "set",
            "deque"
        ],
        "correct": 2,
        "explanation": "The STL 'set' (and 'map') is typically implemented as a balanced Red-Black tree. It automatically maintains all elements in a sorted order and provides O(log n) insertion and search.",
        "topic": "STL Containers"
    },
    {
        "question": "In a Doubly Linked List, deleting a node requires updating how many pointers (excluding the node itself)?",
        "options": [
            "1",
            "2",
            "4",
            "0"
        ],
        "correct": 1,
        "explanation": "To remove a node, you must update the 'next' pointer of the previous node and the 'prev' pointer of the next node so they point to each other, bypassing the deleted node.",
        "topic": "Linked Lists"
    },
    {
        "question": "What is the complexity of finding the 'Inorder Successor' in an AVL tree?",
        "options": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "Finding the inorder successor involves moving to the right child and then following left pointers to the leaf, or moving up the parent pointers. In a balanced AVL tree, this path is limited by the height of the tree, which is O(log n).",
        "topic": "AVL Trees"
    },
    {
        "question": "Which of the following is true about 'Abstract Data Types' (ADTs)?",
        "options": [
            "They are specific to the C++ language.",
            "They define the operations and data but not the implementation details.",
            "They are hardware-level instructions.",
            "They always use arrays for storage."
        ],
        "correct": 1,
        "explanation": "An ADT is a theoretical model. It defines 'what' the data structure does (e.g., a Stack pushes and pops) without specifying 'how' it is implemented (e.g., using an array or a linked list).",
        "topic": "Introduction to ADTs"
    },
    {
        "question": "In the final course summary, what is the 'companion' to an algorithm?",
        "options": [
            "A compiler.",
            "A data structure.",
            "The user.",
            "The operating system."
        ],
        "correct": 1,
        "explanation": "As stated in Lecture 45, 'data structure becomes a companion to an algorithm.' When solving a problem, you choose an algorithm, and that algorithm inherently brings along or requires a specific data structure to function effectively.",
        "topic": "Course Summary"
    },
    {
        "question": "In a B-Tree of order 'm', when a leaf node becomes full (contains m-1 keys) and a new key is inserted, what is the specific algorithmic response to maintain tree properties?",
        "options": [
            "The node is converted into an AVL tree structure.",
            "The node is split into two, and the median key is promoted to the parent node.",
            "The new key is stored in an overflow linked list attached to the leaf.",
            "The tree height is immediately reduced by merging the root with its children."
        ],
        "correct": 1,
        "explanation": "B-Trees maintain balance by growing from the bottom up. When a node exceeds its capacity (m-1 keys), it must split. The median value moves up to the parent to act as a separator between the two new nodes. If the parent is also full, this 'promotion' process continues recursively toward the root. This ensures all leaves remain at the same level.",
        "topic": "B-Trees"
    },
    {
        "question": "Which of the following is a significant mathematical advantage of using 'Quadratic Probing' over 'Linear Probing'?",
        "options": [
            "It completely eliminates the need for a hash function.",
            "It resolves the problem of 'Primary Clustering' by spreading out the probe sequence.",
            "It allows the load factor () to exceed 1.0 without performance degradation.",
            "It guarantees that every cell in the table will be checked even if the table size is not prime."
        ],
        "correct": 1,
        "explanation": "Linear Probing causes primary clustering because it checks adjacent cells, leading to long chains of occupied slots. Quadratic probing uses an i^2 increment, which 'jumps' further away from the original collision spot, effectively breaking up clusters. However, it can still suffer from secondary clustering. ",
        "topic": "Hashing"
    },
    {
        "question": "According to the analysis of 'Separate Chaining', if we have a table of size 100 and 200 elements are inserted, what is the load factor () and the expected average search time?",
        "options": [
            " = 0.5, Search time is O(1)",
            " = 2.0, Search time is proportional to 2 nodes per list",
            " = 200, Search time is O(n)",
            " = 1.0, Search time is O(log n)"
        ],
        "correct": 1,
        "explanation": "The load factor  is calculated as n/m (200/100 = 2.0). In separate chaining,  represents the average number of nodes in each linked list. Therefore, a search will, on average, check 2 nodes, maintaining efficient performance. Unlike probing, chaining can handle  > 1.0.",
        "topic": "Hashing Analysis"
    },
    {
        "question": "In 'Double Hashing', if the primary hash h1(k) results in a collision, the secondary hash h2(k) must satisfy which condition to ensure all slots can be probed?",
        "options": [
            "h2(k) must always return a value of 1.",
            "h2(k) must be relatively prime to the table size 'm'.",
            "h2(k) must be equal to h1(k).",
            "h2(k) must return a negative value."
        ],
        "correct": 1,
        "explanation": "For double hashing to be effective, the step size h2(k) must be able to visit all slots in the table before repeating. This is mathematically guaranteed if h2(k) and the table size 'm' are relatively prime (their greatest common divisor is 1). Often, 'm' is chosen as a prime number to simplify this.",
        "topic": "Double Hashing"
    },
    {
        "question": "What is the worst-case time complexity of an 'Insertion' operation in a Skip List with 'n' elements?",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "While Skip Lists are designed to provide O(log n) performance on average through randomization, there is a statistical worst case where the 'coin flips' do not produce any higher-level nodes. In this degenerate case, the skip list becomes a standard linked list, resulting in O(n) complexity.",
        "topic": "Skip Lists"
    },
    {
        "question": "In the Table ADT, if the requirement is to perform frequent 'Range Searches' (e.g., find all keys between 100 and 500), which implementation is most suitable?",
        "options": [
            "Hash Table",
            "AVL Tree",
            "Unordered Array",
            "Min-Heap"
        ],
        "correct": 1,
        "explanation": "Hash tables (Option 0) distribute keys randomly, making range searches O(n). AVL trees, being balanced BSTs, store data in a sorted order. This allows us to find the start of the range in O(log n) and then perform an in-order traversal to retrieve elements in the range efficiently. ",
        "topic": "Table ADT Selection"
    },
    {
        "question": "Which of the following describes the 'Lazy Deletion' strategy used in Open Addressing hash tables?",
        "options": [
            "The element is physically removed, and the array is re-sorted.",
            "The element is marked with a special flag (e.g., 'AVAILABLE') to keep the probe sequence intact.",
            "Deletion is ignored until the table is completely full.",
            "The key is replaced with the value of the root node."
        ],
        "correct": 1,
        "explanation": "In open addressing, a NULL slot stops a search. If we physically delete a node in the middle of a probe sequence, subsequent searches for items further down that sequence would fail. Lazy deletion marks the slot as 'empty but previously occupied,' allowing search algorithms to pass through it to find later items.",
        "topic": "Hashing"
    },
    {
        "question": "What is the height of a B-Tree of order 'm' containing 'n' elements?",
        "options": [
            "Exactly log2(n)",
            "O(log m n)",
            "O(n)",
            "Exactly m"
        ],
        "correct": 1,
        "explanation": "B-Trees are extremely 'flat' compared to binary trees because they have a high branching factor (m). The height is logarithmic with respect to the base 'm', which is why they are ideal for systems where reading a node (like from a disk) is expensive.",
        "topic": "B-Trees"
    },
    {
        "question": "The 'Division Method' in hashing (h(k) = k % m) is most effective when the table size 'm' is:",
        "options": [
            "A power of 2.",
            "A prime number not close to a power of 2.",
            "An even number.",
            "A very small integer like 10."
        ],
        "correct": 1,
        "explanation": "If 'm' is a power of 2, the hash result depends only on the lower-order bits of the key, which can cause poor distribution. Choosing a prime number for 'm' helps ensure that the keys are spread more uniformly across the table indices, reducing collisions.",
        "topic": "Hashing"
    },
    {
        "question": "In a Skip List, the 'express lanes' (higher levels) are built using ________.",
        "options": [
            "A predetermined mathematical formula.",
            "Randomization (probabilistic choices).",
            "Manual intervention by the programmer.",
            "The height of the parent AVL tree."
        ],
        "correct": 1,
        "explanation": "Skip Lists rely on 'flipping a coin' during insertion to decide how many levels a node should occupy. This probabilistic approach ensures that, on average, each level has half as many nodes as the level below it, mimicking a balanced binary search structure.",
        "topic": "Skip Lists"
    },
    {
        "question": "What is 'Rehashing' technically?",
        "options": [
            "Recalculating the hash of a single corrupted entry.",
            "Expanding the table size and re-inserting all existing keys into the new table.",
            "Changing the hash function from division to multiplication.",
            "Sorting the hash table in descending order."
        ],
        "correct": 1,
        "explanation": "When a hash table's load factor becomes too high, performance degrades. Rehashing involves creating a new, larger array (usually double the size and the next prime) and moving all old data into it. This is an O(n) operation but restores O(1) average performance.",
        "topic": "Hashing Operations"
    },
    {
        "question": "In a B-Tree node of order 4, what is the maximum number of keys it can hold?",
        "options": [
            "4",
            "3",
            "5",
            "2"
        ],
        "correct": 1,
        "explanation": "A B-Tree node of order 'm' can have at most m children and m-1 keys. For order 4, the maximum number of keys is 3.",
        "topic": "B-Tree Properties"
    },
    {
        "question": "Why does 'Secondary Clustering' occur in Quadratic Probing?",
        "options": [
            "Because the step size is always 1.",
            "Because keys that hash to the same initial index will follow the same probe sequence.",
            "Because the table is too small.",
            "Because the hash function is too complex."
        ],
        "correct": 1,
        "explanation": "While Quadratic Probing fixes primary clustering (consecutive blocks), any two keys that hash to the same initial index 'h' will both check h+1, then h+4, then h+9, etc. This shared path is called secondary clustering.",
        "topic": "Hashing"
    },
    {
        "question": "Which data structure is often described as a 'probabilistic alternative' to balanced BSTs?",
        "options": [
            "B-Tree",
            "Skip List",
            "Hash Table",
            "Binary Heap"
        ],
        "correct": 1,
        "explanation": "Skip Lists provide O(log n) average performance like AVL trees but use randomization instead of complex rotations to maintain balance. This makes them easier to implement in concurrent environments.",
        "topic": "Skip Lists"
    },
    {
        "question": "In the context of 'Disjoint Sets', which optimization ensures that the 'Find' operation takes nearly O(1) time?",
        "options": [
            "Union by Rank.",
            "Path Compression.",
            "Median-of-three.",
            "Lazy Deletion."
        ],
        "correct": 1,
        "explanation": "Path compression flattens the tree structure during a 'Find' operation by making all nodes on the path point directly to the root. This significantly reduces the height of the trees for future operations.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "A B-Tree of order 3 is also known as a:",
        "options": [
            "Binary Search Tree",
            "2-3 Tree",
            "Huffman Tree",
            "Red-Black Tree"
        ],
        "correct": 1,
        "explanation": "A 2-3 tree is a B-Tree where every internal node has either 2 children (1 key) or 3 children (2 keys). It is the simplest form of a multi-way balanced search tree.",
        "topic": "B-Trees"
    },
    {
        "question": "When  = 1.0 in a Separate Chaining hash table, what is the status of the table?",
        "options": [
            "The table is full and cannot accept more data.",
            "Every bucket contains exactly one element.",
            "On average, there is one element per bucket (some may have more, some none).",
            "A collision is guaranteed for the next insertion."
        ],
        "correct": 2,
        "explanation": " = n/m. If n=m, the load factor is 1.0. This means the number of items equals the number of slots. In chaining, this is perfectly acceptable and implies that the linked lists are very short on average (1 node).",
        "topic": "Hashing Analysis"
    },
    {
        "question": "Which of the following describes a 'Dense' graph?",
        "options": [
            "A graph with very few edges.",
            "A graph where the number of edges is close to V^2.",
            "A graph that has no cycles.",
            "A graph where every node is a leaf."
        ],
        "correct": 1,
        "explanation": "A dense graph is one where most possible edges between vertices exist. For such graphs, an Adjacency Matrix is a more efficient representation than an Adjacency List.",
        "topic": "Graphs"
    },
    {
        "question": "In a B-Tree, all leaf nodes must be at:",
        "options": [
            "Level 0.",
            "The same depth.",
            "Random levels.",
            "The right side of the tree."
        ],
        "correct": 1,
        "explanation": "A fundamental property of B-Trees is that they are perfectly height-balanced; every leaf node in the tree is at the exact same level.",
        "topic": "B-Trees"
    },
    {
        "question": "Hashing is unsuitable for applications requiring 'Sorted Output' because:",
        "options": [
            "Hashing is too slow.",
            "Hashing does not maintain any relative order between keys.",
            "Hash tables can only store integers.",
            "Collisions delete data permanently."
        ],
        "correct": 1,
        "explanation": "Hash functions are designed to distribute data randomly to avoid collisions. This randomness destroys any natural ordering of the keys, making it impossible to perform an in-order traversal for sorted output without a separate sorting step.",
        "topic": "Hashing"
    },
    {
        "question": "In a Skip List search, we move ________ if the next node's key is smaller than the search key.",
        "options": [
            "Down",
            "Right",
            "Left",
            "To the root"
        ],
        "correct": 1,
        "explanation": "Skip list traversal involves moving 'Right' as long as the keys are smaller than our target, and moving 'Down' to a more detailed level once the next key is larger than our target.",
        "topic": "Skip Lists"
    },
    {
        "question": "What is the primary drawback of using an Adjacency Matrix for a sparse graph?",
        "options": [
            "Search time is O(1).",
            "It wastes a large amount of memory storing zeros for non-existent edges.",
            "It cannot represent directed edges.",
            "It only works for small values of V."
        ],
        "correct": 1,
        "explanation": "An Adjacency Matrix uses V^2 space regardless of the number of edges. In a sparse graph (where E is much smaller than V^2), almost the entire matrix is filled with zeros, wasting significant memory. ",
        "topic": "Graphs"
    },
    {
        "question": "The efficiency of 'Double Hashing' depends on the fact that different keys that collide at the same spot will ________.",
        "options": [
            "Be deleted.",
            "Have different probe sequences due to the second hash function.",
            "Always follow the same path.",
            "Be moved to a separate linked list."
        ],
        "correct": 1,
        "explanation": "Because the step size (probe increment) is a function of the key itself (h2(key)), two keys that happen to land at the same index will jump to different secondary spots, eliminating secondary clustering.",
        "topic": "Double Hashing"
    },
    {
        "question": "Which of the following sorting algorithms is 'Adaptive' (takes advantage of pre-existing order)?",
        "options": [
            "Selection Sort",
            "Insertion Sort",
            "Standard Quicksort",
            "Merge Sort"
        ],
        "correct": 1,
        "explanation": "Insertion sort is adaptive because its best-case complexity is O(n) when the data is already sorted. Selection sort always takes O(n^2) because it must scan for the minimum regardless of the initial order.",
        "topic": "Sorting"
    },
    {
        "question": "In the final summary (Lecture 45), what is the 'importance' of data structures to a Software Engineer?",
        "options": [
            "To learn how to use a specific IDE.",
            "To increase the domain knowledge of design choices and resolve design problems.",
            "To memorize the syntax of C++ templates.",
            "To understand how to build physical hardware."
        ],
        "correct": 1,
        "explanation": "As stated in the handouts, the goal is to provide engineers with a 'domain knowledge of design choices' to resolve professional design problems in software applications.",
        "topic": "Course Summary"
    },
    {
        "question": "What is the time complexity of 'Find' in a Disjoint Set with Path Compression and Union by Rank?",
        "options": [
            "O(n)",
            "O(log n)",
            "Nearly O(1) (Inverse Ackermann Function)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "The combined optimizations make Disjoint Set operations incredibly efficient, resulting in an amortized time complexity that is practically constant for all conceivable values of n.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "A B-Tree of order 'm' must have its root containing at least how many keys?",
        "options": [
            "m/2",
            "1",
            "ceil(m/2) - 1",
            "m-1"
        ],
        "correct": 1,
        "explanation": "The root is the only node in a B-Tree exempt from the 'half-full' rule. It must contain at least 1 key and have at least 2 children (unless the tree consists of only the root).",
        "topic": "B-Trees"
    },
    {
        "question": "Which collision resolution method is least likely to suffer from Clustering?",
        "options": [
            "Linear Probing",
            "Quadratic Probing",
            "Separate Chaining",
            "All are the same"
        ],
        "correct": 2,
        "explanation": "Separate Chaining avoids clustering by using an external structure (linked lists). Even if many items hash to the same spot, they do not block other spots in the main array.",
        "topic": "Hashing"
    },
    {
        "question": "In a Skip List, the probability 'p' for node promotion is typically ________.",
        "options": [
            "0.1",
            "0.5",
            "1.0",
            "Randomized per node"
        ],
        "correct": 1,
        "explanation": "Using p=0.5 ensures that level 2 has half as many nodes as level 1, level 3 has half as many as level 2, and so on, creating a balanced hierarchy.",
        "topic": "Skip Lists"
    },
    {
        "question": "Which of the following is NOT a property of a B-Tree of order m?",
        "options": [
            "All leaves are at the same level.",
            "Every node except root has at least ceil(m/2) children.",
            "Every node follows the BST property (left < root < right).",
            "The root has at least 2 children if it's not a leaf."
        ],
        "correct": 2,
        "explanation": "B-Trees are multi-way trees, so 'left < root < right' is replaced by a range system where keys act as separators for multiple children. It is not a binary search tree property.",
        "topic": "B-Trees"
    },
    {
        "question": "In a hash table,  (lambda) is the ratio of:",
        "options": [
            "Table size to keys.",
            "Keys to table size.",
            "Collisions to table size.",
            "Time to search."
        ],
        "correct": 1,
        "explanation": "The load factor  = n/m, where n is the number of keys and m is the number of slots in the array.",
        "topic": "Hashing"
    },
    {
        "question": "What happens to search time in a hash table as  approaches 1.0 in Linear Probing?",
        "options": [
            "It stays O(1).",
            "It increases sharply (approaches O(n)).",
            "It decreases.",
            "It becomes O(log n)."
        ],
        "correct": 1,
        "explanation": "As the table fills up, the 'gaps' disappear, and the number of probes needed to find an empty slot or a specific key increases drastically.",
        "topic": "Hashing Analysis"
    },
    {
        "question": "Which algorithm 'brings some data structure along' according to the course summary?",
        "options": [
            "Hashing (Compiler symbol table).",
            "Sorting (Array).",
            "Recursion (Stack).",
            "All of the above."
        ],
        "correct": 3,
        "explanation": "The summary emphasizes that data structures are 'companions' to algorithms. For example, building a symbol table brings along hashing; searches bring along trees.",
        "topic": "Course Summary"
    },
    {
        "question": "In the array representation of an AVL tree, rebalancing involves updating pointers. In terms of complexity, a single rotation takes ________.",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n^2)"
        ],
        "correct": 0,
        "explanation": "A rotation is simply a series of pointer updates. It does not depend on the number of elements in the tree, making it a constant-time operation.",
        "topic": "AVL Trees"
    },
    {
        "question": "A 'Strongly Connected' directed graph is one where:",
        "options": [
            "There is an edge between every pair of nodes.",
            "There is a directed path from every vertex to every other vertex.",
            "The graph has no cycles.",
            "The degree of every vertex is 2."
        ],
        "correct": 1,
        "explanation": "Strong connectivity means that you can reach any vertex from any other vertex following the directed edges.",
        "topic": "Graphs"
    },
    {
        "question": "In a hash table using 'Double Hashing', if the primary hash h1(key) results in a collision at index 'H', and the secondary hash h2(key) returns a value of 'S', what is the index of the third probe (i=2)?",
        "options": [
            "(H + S) % size",
            "(H + 2S) % size",
            "(H + S^2) % size",
            "(H + 2^S) % size"
        ],
        "correct": 1,
        "explanation": "Double Hashing uses the formula (h1(key) + i * h2(key)) % size. For the third probe (where the first probe is i=0 and the second is i=1), the index is calculated by adding twice the step size (S) to the initial hash (H). This prevents the primary clustering seen in linear probing because the step size depends on the key itself. ",
        "topic": "Double Hashing"
    },
    {
        "question": "Which of the following conditions must be met to ensure that Quadratic Probing will always find an empty slot if the hash table is at least half empty?",
        "options": [
            "The table size must be an even number.",
            "The table size must be a prime number.",
            "The load factor must be greater than 0.75.",
            "The keys must be inserted in sorted order."
        ],
        "correct": 1,
        "explanation": "A known mathematical theorem in hashing states that if the table size 'm' is a prime number and the load factor  is less than or equal to 0.5, quadratic probing is guaranteed to find an empty cell. If the table size is not prime, the probe sequence might cycle through a limited subset of indices, failing to find an empty slot even if one exists.",
        "topic": "Quadratic Probing"
    },
    {
        "question": "In a Skip List, if the probability of a node being promoted to the next level is 1/2, what is the expected number of nodes at level 'k' in a list containing 'n' elements?",
        "options": [
            "n / k",
            "n / (2^k)",
            "n / (2^(k-1))",
            "log(n) / k"
        ],
        "correct": 2,
        "explanation": "The base level (Level 1) contains all 'n' nodes. If the promotion probability is 1/2, then Level 2 is expected to have n/2 nodes, Level 3 is expected to have n/4 nodes, and so on. Generalizing this, level 'k' is expected to have n / (2^(k-1)) nodes. This geometric progression ensures the O(log n) average search time. ",
        "topic": "Skip Lists"
    },
    {
        "question": "What is the primary drawback of using 'Separate Chaining' when the load factor () becomes very large?",
        "options": [
            "The hash table will overflow and crash.",
            "The time complexity for search degrades from O(1) toward O(n) as linked lists grow long.",
            "The hash function will stop working.",
            "It becomes impossible to delete elements."
        ],
        "correct": 1,
        "explanation": "In separate chaining, the search time is proportional to the length of the linked lists. If  is large, each 'bucket' contains many nodes. In the worst case, if the hash function is poor and  is high, the table behaves like a single long linked list, making operations O(n). ",
        "topic": "Hashing Analysis"
    },
    {
        "question": "In a B-Tree of order 'm', what is the minimum number of children allowed for any internal node except the root?",
        "options": [
            "2",
            "m / 2",
            "ceil(m / 2)",
            "m - 1"
        ],
        "correct": 2,
        "explanation": "A fundamental property of B-Trees is that they must be at least half-full. Except for the root, every internal node in a B-Tree of order 'm' must have at least ceil(m/2) children to maintain the efficiency of the multi-way search structure.",
        "topic": "B-Trees"
    },
    {
        "question": "Which collision resolution technique is most susceptible to 'Secondary Clustering'?",
        "options": [
            "Linear Probing",
            "Quadratic Probing",
            "Separate Chaining",
            "Double Hashing"
        ],
        "correct": 1,
        "explanation": "Secondary clustering occurs when multiple keys that hash to the same initial index follow the exact same probe sequence. While Quadratic Probing eliminates primary clustering (consecutive blocks), it still suffers from secondary clustering because the probe sequence depends only on the initial hash. Double Hashing solves this by making the step size also dependent on the key.",
        "topic": "Clustering"
    },
    {
        "question": "In the Standard Template Library (STL), the 'priority_queue' is typically implemented using which underlying data structure?",
        "options": [
            "Linked List",
            "Vector (as a Binary Heap)",
            "Binary Search Tree",
            "Stack"
        ],
        "correct": 1,
        "explanation": "The STL `priority_queue` is a container adapter that usually uses a `vector` as its underlying storage, organized as a Binary Heap. This provides O(log n) insertion and O(log n) extraction of the top priority element.",
        "topic": "STL and Heaps"
    },
    {
        "question": "What is the 'Lazy Deletion' marker used for in Open Addressing hash tables?",
        "options": [
            "To save memory by deleting nodes immediately.",
            "To indicate a slot is empty but should not terminate a search probe sequence.",
            "To count the number of deleted items.",
            "To trigger a rehash operation."
        ],
        "correct": 1,
        "explanation": "Physical deletion in probing can break the search path for later keys. Lazy deletion uses a flag to indicate that a cell is empty for insertions but 'active' for continuing a search, ensuring the integrity of the probe sequence. ",
        "topic": "Hashing Deletion"
    },
    {
        "question": "If an AVL tree node has a balance factor of -2, which subtrees are involved in the imbalance?",
        "options": [
            "The left subtree is two levels higher than the right.",
            "The right subtree is two levels higher than the left.",
            "The node is perfectly balanced.",
            "The node is a leaf node."
        ],
        "correct": 1,
        "explanation": "Balance Factor is defined as Height(Left) - Height(Right). A value of -2 indicates that the right subtree is significantly taller than the left, requiring a rotation (Single Left or Right-Left) to restore balance.",
        "topic": "AVL Trees"
    },
    {
        "question": "Which sorting algorithm's worst-case performance is identical to its average-case performance (O(n log n))?",
        "options": [
            "Quicksort",
            "Selection Sort",
            "Merge Sort",
            "Bubble Sort"
        ],
        "correct": 2,
        "explanation": "Merge Sort consistently divides the array in half and performs linear-time merging, resulting in O(n log n) in all cases (Best, Average, and Worst). Quicksort degrades to O(n^2) in its worst case.",
        "topic": "Sorting Complexity"
    },
    {
        "question": "In a B-Tree, if a split occurs at the root, what happens to the height of the tree?",
        "options": [
            "The height decreases.",
            "The height remains the same.",
            "The height increases by 1.",
            "The tree is destroyed."
        ],
        "correct": 2,
        "explanation": "B-Trees are the only trees that grow 'upward'. When the root splits, a new root is created above the old one, increasing the height of the entire tree by exactly one level.",
        "topic": "B-Trees"
    },
    {
        "question": "What is the amortized complexity of 'Find' in a Disjoint Set using both Path Compression and Union by Rank?",
        "options": [
            "O(1)",
            "O(log n)",
            "O((n)) - Inverse Ackermann function",
            "O(n)"
        ],
        "correct": 2,
        "explanation": "The combination of these two optimizations makes the complexity nearly constant. The inverse Ackermann function (n) grows so slowly that for all practical values of n, it is less than 5.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "A 'Perfect' Binary Tree of height 4 has how many leaf nodes?",
        "options": [
            "4",
            "8",
            "16",
            "31"
        ],
        "correct": 2,
        "explanation": "In a perfect binary tree, the number of leaves is 2^h. For height 4 (where root is height 0), the number of leaves is 2^4 = 16 nodes. Total nodes would be 2^(h+1)-1 = 31.",
        "topic": "Binary Tree Properties"
    },
    {
        "question": "Which hashing method is best for eliminating both Primary and Secondary Clustering?",
        "options": [
            "Linear Probing",
            "Quadratic Probing",
            "Double Hashing",
            "Separate Chaining"
        ],
        "correct": 2,
        "explanation": "Double Hashing uses a second hash function to calculate the step size, ensuring that keys hashing to the same spot follow different paths, thus eliminating both types of clustering.",
        "topic": "Double Hashing"
    },
    {
        "question": "In a Min-Heap implemented as an array, where is the second smallest element located?",
        "options": [
            "Always at index 2.",
            "Always at index 3.",
            "Either at index 2 or index 3.",
            "At the last index."
        ],
        "correct": 2,
        "explanation": "The smallest is at index 1 (root). The next smallest must be one of its immediate children, which are at indices 2 and 3. ",
        "topic": "Heaps"
    },
    {
        "question": "What is the primary advantage of using a 'Threaded' Binary Tree?",
        "options": [
            "Faster insertion.",
            "Inorder traversal without the use of a stack or recursion.",
            "Requires less memory per node.",
            "Automatically stays balanced."
        ],
        "correct": 1,
        "explanation": "By utilizing NULL pointers to point to predecessors/successors, threaded trees allow for linear-time inorder traversal using only a few extra bits per node, saving stack space.",
        "topic": "Threaded Binary Trees"
    },
    {
        "question": "Which data structure is most suitable for 'Expression Evaluation'?",
        "options": [
            "Queue",
            "Stack",
            "Hash Table",
            "Skip List"
        ],
        "correct": 1,
        "explanation": "Stacks are the standard ADT for evaluating postfix/prefix expressions and converting infix expressions due to their LIFO nature.",
        "topic": "Stack Applications"
    },
    {
        "question": "A B-Tree of order 5 can have a maximum of how many keys per node?",
        "options": [
            "5",
            "4",
            "6",
            "2"
        ],
        "correct": 1,
        "explanation": "The maximum number of keys is m-1. For order 5, the maximum keys allowed in any node is 4.",
        "topic": "B-Tree Properties"
    },
    {
        "question": "Which complexity class represents the growth of an AVL tree's height relative to its nodes?",
        "options": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "AVL trees are self-balancing, ensuring that the height remains logarithmic relative to the number of nodes (h  1.44 log n).",
        "topic": "AVL Trees"
    },
    {
        "question": "In Huffman Coding, the 'weighted path length' is minimized to achieve ________.",
        "options": [
            "Security",
            "Optimal data compression",
            "Faster sorting",
            "Random access"
        ],
        "correct": 1,
        "explanation": "By giving shorter codes to high-frequency characters, Huffman Coding minimizes the total bits required to represent a file, achieving optimal lossless compression.",
        "topic": "Huffman Coding"
    },
    {
        "question": "What is the time complexity to find an element in a Skip List in the best case?",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n log n)"
        ],
        "correct": 0,
        "explanation": "In the best case, the element you are looking for is the very first node at the highest 'express' level, allowing for O(1) access.",
        "topic": "Skip Lists"
    },
    {
        "question": "A 'Complete' Binary Tree with 15 nodes is also a ________ binary tree.",
        "options": [
            "Skewed",
            "Perfect",
            "Threaded",
            "B-Tree"
        ],
        "correct": 1,
        "explanation": "A complete binary tree with 15 nodes (2^4 - 1) is perfectly filled at all levels, making it a perfect binary tree.",
        "topic": "Binary Tree Types"
    },
    {
        "question": "Rehashing is typically triggered when the load factor  reaches ________.",
        "options": [
            "0.1",
            "0.5 (for Open Addressing)",
            "1.0",
            "Infinity"
        ],
        "correct": 1,
        "explanation": "For probing methods, performance degrades significantly as  exceeds 0.5, making this the standard threshold for rehashing.",
        "topic": "Hashing"
    },
    {
        "question": "Which rotation is needed for an 'inside' imbalance on the left side (Left-Right case)?",
        "options": [
            "Single Left",
            "Single Right",
            "Double LR Rotation",
            "Double RL Rotation"
        ],
        "correct": 2,
        "explanation": "The LR case is a 'zigzag' imbalance where a node is added to the right subtree of a left child. It requires a Left rotation followed by a Right rotation. ",
        "topic": "AVL Rotations"
    },
    {
        "question": "In a Disjoint Set array, if `S[i] = -1`, what is element 'i'?",
        "options": [
            "A leaf node.",
            "The parent of node 1.",
            "A root representing a set of size 1.",
            "An invalid node."
        ],
        "correct": 2,
        "explanation": "In Union-by-Size, the root stores the negative of the set size. -1 indicates 'i' is the root and the set currently contains only itself.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "Which data structure is used to implement BFS in a graph?",
        "options": [
            "Stack",
            "Queue",
            "Heap",
            "BST"
        ],
        "correct": 1,
        "explanation": "BFS explores neighbors level by level, requiring a FIFO (Queue) structure to maintain the correct exploration order.",
        "topic": "Graphs"
    },
    {
        "question": "The 'Division Method' hash function is `index = key % size`. Why is 'size' often prime?",
        "options": [
            "Primes are faster for computers.",
            "To ensure better distribution of keys and reduce collisions.",
            "Primes use less memory.",
            "Modulo only works with primes."
        ],
        "correct": 1,
        "explanation": "Choosing a prime table size helps spread keys more evenly, especially if the keys have mathematical patterns (like being multiples of some number).",
        "topic": "Hashing"
    },
    {
        "question": "What is the degree of a node in a Binary Tree?",
        "options": [
            "Number of ancestors.",
            "Number of children (0, 1, or 2).",
            "Total number of nodes.",
            "Depth of the node."
        ],
        "correct": 1,
        "explanation": "In tree theory, degree refers to the number of children (subtrees) associated with a node.",
        "topic": "Tree Terminology"
    },
    {
        "question": "Which of the following is NOT a linear data structure?",
        "options": [
            "Stack",
            "Queue",
            "Graph",
            "Linked List"
        ],
        "correct": 2,
        "explanation": "Graphs (and Trees) are non-linear data structures because they represent hierarchical or complex networked relationships.",
        "topic": "Data Structure Classification"
    },
    {
        "question": "In a B-Tree, keys in a single node must be stored in ________.",
        "options": [
            "Random order.",
            "Sorted order.",
            "Level order.",
            "Reverse order."
        ],
        "correct": 1,
        "explanation": "B-Tree nodes require keys to be sorted to allow for binary search within the node to determine which child pointer to follow.",
        "topic": "B-Trees"
    },
    {
        "question": "What is the time complexity to insert into a Heap?",
        "options": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "Insertion involves adding a leaf and 'percolating up'. The path from leaf to root is O(log n).",
        "topic": "Heaps"
    },
    {
        "question": "The Table ADT 'Search' using Hashing is ________ in the average case.",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n log n)"
        ],
        "correct": 0,
        "explanation": "Hashing allows for direct index calculation, providing constant-time search on average.",
        "topic": "Hashing"
    },
    {
        "question": "In separate chaining, the space complexity is ________.",
        "options": [
            "O(V)",
            "O(V^2)",
            "O(N + M)",
            "O(log N)"
        ],
        "correct": 2,
        "explanation": "We store N elements in M buckets; total space is the sum of the table size and the nodes in the lists.",
        "topic": "Hashing"
    },
    {
        "question": "Which 'Design Choice' is a companion to an algorithm for building symbol tables?",
        "options": [
            "Stack",
            "Hashing",
            "Linked List",
            "Queue"
        ],
        "correct": 1,
        "explanation": "As noted in the final summary (Lecture 45), Hashing is the primary design choice for compiler symbol tables.",
        "topic": "Course Summary"
    },
    {
        "question": "What is the height of an empty tree?",
        "options": [
            "0",
            "-1",
            "1",
            "NULL"
        ],
        "correct": 1,
        "explanation": "By convention in CS301 handouts, an empty tree has height -1, and a single-node tree has height 0.",
        "topic": "Binary Trees"
    },
    {
        "question": "Which of the following describes the 'Topological Sort' of a Directed Acyclic Graph (DAG)?",
        "options": [
            "A linear ordering of vertices such that for every directed edge (u, v), vertex u comes before v.",
            "A method to find the shortest path between all pairs of vertices in a weighted graph.",
            "A sorting algorithm that arranges vertices based on their numerical IDs in descending order.",
            "A traversal that visits all leaf nodes before visiting internal nodes."
        ],
        "correct": 0,
        "explanation": "Topological sorting is a fundamental graph algorithm used for scheduling tasks with dependencies. It produces a linear sequence where every dependency (edge) is respected. For instance, if task 'u' must be completed before 'v', u will always appear earlier in the sorted list. This is only possible in a Directed Acyclic Graph (DAG) because cycles would create impossible circular dependencies. ",
        "topic": "Graph Algorithms"
    },
    {
        "question": "What is the primary condition required for a graph to have a valid 'Topological Sort'?",
        "options": [
            "The graph must be undirected and connected.",
            "The graph must be a Directed Acyclic Graph (DAG).",
            "The graph must be complete (every node connected to every other node).",
            "The graph must contain at least one self-loop."
        ],
        "correct": 1,
        "explanation": "If a graph contains a cycle (e.g., A points to B, B points to C, and C points to A), no linear ordering can satisfy the edge requirements because each node in the cycle is a dependency for the other. Therefore, only Directed Acyclic Graphs (DAGs) can be topologically sorted.",
        "topic": "Graph Theory"
    },
    {
        "question": "In the context of the Table ADT, what is the 'probabilistic' nature of a Skip List search complexity?",
        "options": [
            "It is always O(n) because it uses linked lists.",
            "The O(log n) performance is expected on average based on random level assignments.",
            "The search time depends on the hardware's clock speed.",
            "The search algorithm randomly deletes nodes to speed up the process."
        ],
        "correct": 1,
        "explanation": "Skip Lists are unique because they don't use rigid balancing rules like AVL trees. Instead, they use 'coin flips' to determine node heights. Mathematically, this randomness results in a high probability that the structure will behave like a balanced tree, yielding O(log n) average complexity for search, insert, and delete operations. ",
        "topic": "Skip Lists"
    },
    {
        "question": "According to Lecture 45, which data structure is described as being primarily important from an 'algorithmic' point of view, even if it was not discussed extensively in the basic ADT implementation?",
        "options": [
            "Stack",
            "Queue",
            "Graph",
            "Circular Linked List"
        ],
        "correct": 2,
        "explanation": "The final lecture of CS301 explicitly notes that while Stacks, Queues, and Trees were the focus of ADT implementation, Graphs are essential specifically from an algorithmic perspective (e.g., for networking and pathfinding), marking them as a critical area for further study.",
        "topic": "Course Summary"
    },
    {
        "question": "What is the 'Indegree' of a vertex in a directed graph, and how is it used in topological sorting?",
        "options": [
            "The number of edges leaving the vertex; used to find the end of the list.",
            "The number of edges entering the vertex; vertices with indegree 0 are processed first.",
            "The total number of neighbors; used to calculate the weight of the vertex.",
            "The depth of the vertex from the root; used to balance the graph."
        ],
        "correct": 1,
        "explanation": "In topological sorting (Kahn's algorithm), we track the 'indegree' (incoming edges). A vertex with an indegree of 0 has no dependencies and can be safely placed in the sorted list. Once placed, its outgoing edges are 'removed,' potentially reducing the indegree of its neighbors to 0, allowing them to be processed next.",
        "topic": "Graph Algorithms"
    },
    {
        "question": "Which of the following data structures is most appropriate for a system requiring 'Random Access' by index in O(1) time?",
        "options": [
            "Singly Linked List",
            "Stack",
            "Binary Search Tree",
            "Array (Vector)"
        ],
        "correct": 3,
        "explanation": "Arrays provide O(1) random access because elements are stored contiguously in memory, allowing the CPU to calculate the address of any index instantly. Linked lists and Trees require traversal, taking O(n) or O(log n) respectively to reach a specific index.",
        "topic": "Data Structure Selection"
    },
    {
        "question": "When implementing an Adjacency Matrix for a directed graph with V vertices, what does the sum of the values in Row 'i' represent?",
        "options": [
            "The Outdegree of vertex i.",
            "The Indegree of vertex i.",
            "The total number of cycles containing i.",
            "The shortest path from i to the root."
        ],
        "correct": 0,
        "explanation": "In an adjacency matrix, Matrix[i,j] = 1 represents an edge 'from' i 'to' j. Therefore, summing all entries in Row i counts all edges leaving that vertex, which is the definition of Outdegree. Summing Column i would give the Indegree. ",
        "topic": "Adjacency Matrix"
    },
    {
        "question": "In 'Separate Chaining' hashing, what is the impact of a 'Poor Hash Function' that maps many keys to the same index?",
        "options": [
            "The table will throw a 'Stack Overflow' error.",
            "The linked lists at that index will become very long, degrading search time to O(n).",
            "The hash table will automatically re-sort the data.",
            "The memory will be released by the garbage collector."
        ],
        "correct": 1,
        "explanation": "The efficiency of hashing relies on a uniform distribution. If many keys hash to the same 'bucket,' the O(1) advantage is lost as the algorithm must perform a linear search through a long linked list, effectively reverting to O(n) complexity.",
        "topic": "Hashing Analysis"
    },
    {
        "question": "Which specific approach to collision resolution is most likely to suffer from 'Primary Clustering'?",
        "options": [
            "Separate Chaining",
            "Linear Probing",
            "Quadratic Probing",
            "Double Hashing"
        ],
        "correct": 1,
        "explanation": "Linear probing checks the next sequential index (H+1, H+2...). This creates 'clusters' of occupied slots. As these clusters grow, the probability of hashing into one increases, making the clusters grow even faster and slowing down the search process significantly.",
        "topic": "Hashing"
    },
    {
        "question": "What is the time complexity to find the 'Minimum' value in a Max-Heap containing n elements?",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "A Max-Heap is optimized to find the maximum (at the root) in O(1). However, it does not maintain a sorted order for other elements. The minimum value could be in any of the leaf nodes, which constitute roughly half the tree (n/2). Therefore, finding it requires a linear scan of the leaves, resulting in O(n) time.",
        "topic": "Heap Analysis"
    },
    {
        "question": "In a B-Tree of order m, what is the maximum number of children any node can have?",
        "options": [
            "m - 1",
            "m",
            "2m",
            "log m"
        ],
        "correct": 1,
        "explanation": "The 'order' m of a B-Tree explicitly defines the maximum branching factor. A node can have at most m children and m-1 keys. This high branching factor keeps the tree height low, which is ideal for disk storage. ",
        "topic": "B-Trees"
    },
    {
        "question": "Which of the following is true for 'Double Hashing'?",
        "options": [
            "It uses two hash tables to store data.",
            "It uses a second hash function to calculate the step size for probing.",
            "It doubles the size of the table whenever a collision occurs.",
            "It is only used for string-based keys."
        ],
        "correct": 1,
        "explanation": "Double hashing resolves collisions by applying a second hash function `h2(key)` to determine the probe increment. Since the step size depends on the key itself, it avoids the clustering issues found in linear and quadratic probing.",
        "topic": "Double Hashing"
    },
    {
        "question": "In the Standard Template Library (STL), which container would you choose if you need to maintain elements in a sorted order and perform fast O(log n) searches?",
        "options": [
            "std::vector",
            "std::list",
            "std::set",
            "std::stack"
        ],
        "correct": 2,
        "explanation": "The STL `std::set` (and `std::map`) is typically implemented as a balanced Red-Black tree (a variant of a balanced BST). It automatically keeps data sorted and provides logarithmic time complexity for all standard operations.",
        "topic": "STL Containers"
    },
    {
        "question": "What happens if you perform a 'Find' operation on two elements that are in the same set in a Disjoint Set structure?",
        "options": [
            "It returns different root values.",
            "It returns the same root value (representative).",
            "It merges the sets automatically.",
            "It returns an error."
        ],
        "correct": 1,
        "explanation": "The 'Find' operation identifies the unique representative (root) of a set. If two elements belong to the same set, following their parent pointers will eventually lead to the exact same root node.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "In a 'Complete Binary Tree' stored in an array starting at index 1, where is the right child of the node at index 'i'?",
        "options": [
            "2i",
            "2i + 1",
            "i / 2",
            "i + 1"
        ],
        "correct": 1,
        "explanation": "Array-based mapping for complete trees uses simple arithmetic: Left child = 2i, Right child = 2i + 1, and Parent = floor(i/2). This allows navigation without pointers. ",
        "topic": "Heap Implementation"
    },
    {
        "question": "A graph with V vertices and E edges is considered 'Sparse' if:",
        "options": [
            "E is much less than V^2.",
            "E is approximately equal to V^2.",
            "The graph contains no edges.",
            "The graph is a complete graph."
        ],
        "correct": 0,
        "explanation": "A sparse graph has relatively few edges compared to the maximum possible number of edges (V^2). For such graphs, an Adjacency List is far more space-efficient than an Adjacency Matrix.",
        "topic": "Graph Representations"
    },
    {
        "question": "Which complexity class represents an algorithm whose execution time doubles when the input size 'n' increases by just one unit?",
        "options": [
            "O(n^2)",
            "O(log n)",
            "O(2^n)",
            "O(n!)"
        ],
        "correct": 2,
        "explanation": "O(2^n) represents exponential growth. In this class, adding one to 'n' multiplies the work by the base (2). This is characteristic of many 'brute force' solutions to hard problems and becomes impractical very quickly as n grows.",
        "topic": "Complexity Analysis"
    },
    {
        "question": "In 'Path Compression', when is the optimization performed?",
        "options": [
            "During the Union operation.",
            "During the Find operation.",
            "Only during initialization.",
            "When the tree is deleted."
        ],
        "correct": 1,
        "explanation": "Path compression is a 'lazy' optimization. As the 'Find' algorithm traverses up to the root, it updates every node along the way to point directly to the root, flattening the tree for all future operations.",
        "topic": "Disjoint Set Optimizations"
    },
    {
        "question": "What is the 'Amortized' complexity of Disjoint Set operations when both Union-by-Rank and Path Compression are used?",
        "options": [
            "O(log n)",
            "Nearly O(1) (Inverse Ackermann function)",
            "O(n)",
            "O(1)"
        ],
        "correct": 1,
        "explanation": "While a single operation could technically take longer, the average (amortized) cost over a sequence of operations is nearly constant. The Inverse Ackermann function (n) is used to describe this, which is practically less than 5 for all realistic values of n.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "In a B-Tree of order 4, what is the maximum number of keys allowed in a single node?",
        "options": [
            "4",
            "3",
            "5",
            "2"
        ],
        "correct": 1,
        "explanation": "Max keys per node = m - 1. For order 4, a node can hold a maximum of 3 keys. Once a 4th key is attempted, the node must split.",
        "topic": "B-Tree Properties"
    },
    {
        "question": "Which of the following traversal methods is used to print the data of an expression tree in 'Infix' notation?",
        "options": [
            "Pre-order",
            "Post-order",
            "In-order",
            "Level-order"
        ],
        "correct": 2,
        "explanation": "In-order traversal visits nodes in the order: Left Subtree -> Root -> Right Subtree. In an expression tree, this corresponds to putting the operator between the operands, which is the standard infix form (e.g., A + B).",
        "topic": "Tree Traversals"
    },
    {
        "question": "What is the 'Load Factor' () threshold usually recommended to trigger rehashing in an Open Addressing hash table?",
        "options": [
            " = 0.1",
            " = 0.5",
            " = 1.0",
            " = 2.0"
        ],
        "correct": 1,
        "explanation": "In probing methods, the number of collisions increases drastically as the table becomes more than half full. Thus, rehashing is typically performed when  exceeds 0.5 to maintain near O(1) performance.",
        "topic": "Hashing Analysis"
    },
    {
        "question": "Which data structure provides the fastest way to extract the element with the 'highest priority' in a dynamic system?",
        "options": [
            "Sorted Linked List",
            "Hash Table",
            "Binary Heap",
            "Standard Queue"
        ],
        "correct": 2,
        "explanation": "A Binary Heap provides O(log n) for both insertion and extraction of the priority element, making it the most efficient choice for a Priority Queue ADT.",
        "topic": "Heaps"
    },
    {
        "question": "In the summary of Lecture 45, the examiner states that choosing an algorithm often 'brings along' a specific ________.",
        "options": [
            "Compiler",
            "Data structure companion",
            "Programming language",
            "Hardware requirement"
        ],
        "correct": 1,
        "explanation": "The course concludes that algorithms and data structures are inseparable; the choice of an algorithm (like searching) naturally requires a specific data structure (like a tree) to be effective.",
        "topic": "Course Summary"
    },
    {
        "question": "Which of the following is NOT a linear data structure?",
        "options": [
            "Stack",
            "Queue",
            "Graph",
            "Vector"
        ],
        "correct": 2,
        "explanation": "Linear structures (Stack, Queue, Vector) have elements arranged in a sequence. Graphs are non-linear because they allow multiple paths and complex networking between nodes.",
        "topic": "Data Structure Classification"
    },
    {
        "question": "What is the height of a Binary Search Tree with n nodes in the absolute worst case?",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "In the worst case, nodes are inserted in sorted order (e.g., 1, 2, 3, 4, 5). This creates a 'skewed' tree that is effectively a linked list, with a height of n-1, or O(n).",
        "topic": "BST Analysis"
    },
    {
        "question": "In 'Huffman Coding', where is the actual data (characters) stored in the tree?",
        "options": [
            "In the root node only.",
            "In all internal nodes.",
            "In the leaf nodes only.",
            "In a separate array, not in the tree."
        ],
        "correct": 2,
        "explanation": "Huffman trees are prefix trees where all encoded characters are located at the leaves. The path from the root to the leaf determines the character's unique binary code.",
        "topic": "Huffman Coding"
    },
    {
        "question": "Which algorithm is best for finding all nodes reachable from a starting vertex in a graph?",
        "options": [
            "Binary Search",
            "Topological Sort",
            "Breadth First Search (BFS)",
            "Quicksort"
        ],
        "correct": 2,
        "explanation": "BFS (and DFS) are graph traversal algorithms designed to visit every reachable vertex systematically. ",
        "topic": "Graph Traversals"
    },
    {
        "question": "A 'Perfect Binary Tree' of height h has how many total nodes?",
        "options": [
            "2^h",
            "2^(h+1) - 1",
            "h^2",
            "2h + 1"
        ],
        "correct": 1,
        "explanation": "In a perfect binary tree, every level is full. The total number of nodes is the sum of a geometric series: 1+2+4...+2^h = 2^(h+1) - 1.",
        "topic": "Binary Trees"
    },
    {
        "question": "Which notation provides the strictly 'Lower Bound' for the complexity of an algorithm?",
        "options": [
            "O (Big-O)",
            " (Omega)",
            " (Theta)",
            "f(n)"
        ],
        "correct": 1,
        "explanation": "Omega notation () describes the best-case growth rate or the minimum amount of work an algorithm will do for any input of size n.",
        "topic": "Complexity Notations"
    },
    {
        "question": "In a B-Tree, all leaf nodes must be at:",
        "options": [
            "The same level.",
            "Variable levels based on key values.",
            "Level 0 only.",
            "The left side of the tree only."
        ],
        "correct": 0,
        "explanation": "A fundamental property of B-Trees is that they are perfectly height-balanced. All leaf nodes are required to be at the exact same depth from the root.",
        "topic": "B-Trees"
    },
    {
        "question": "Which data structure uses a 'Front' and 'Rear' pointer to manage elements?",
        "options": [
            "Stack",
            "Queue",
            "Binary Tree",
            "Skip List"
        ],
        "correct": 1,
        "explanation": "A Queue uses two pointers to maintain FIFO order: Front (where elements are removed) and Rear (where elements are added).",
        "topic": "Queue ADT"
    },
    {
        "question": "What is the time complexity to perform a 'Rotation' in an AVL tree?",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n log n)"
        ],
        "correct": 0,
        "explanation": "A rotation only involves updating a fixed number of pointers (2 or 3) regardless of the size of the tree, making it a constant-time O(1) operation.",
        "topic": "AVL Trees"
    },
    {
        "question": "In C++, which keyword is used to create a 'Generic' data structure that works with any data type?",
        "options": [
            "struct",
            "template",
            "virtual",
            "generic"
        ],
        "correct": 1,
        "explanation": "The `template` keyword in C++ allows programmers to write classes and functions where the data type is a parameter, enabling high code reusability.",
        "topic": "Generic Programming"
    },
    {
        "question": "Which data structure is the 'companion' to a compiler for building its 'Symbol Table'?",
        "options": [
            "Stack",
            "Linked List",
            "Hash Table",
            "Binary Heap"
        ],
        "correct": 2,
        "explanation": "As discussed in the final lecture summary, compilers use Hashing to implement symbol tables because they need to look up variables and functions instantly during compilation.",
        "topic": "Course Summary"
    },
    {
        "question": "Which of the following growth rates characterizes an algorithm where the number of operations remains constant regardless of the input size (n)?",
        "options": [
            "O(log n)",
            "O(1)",
            "O(n)",
            "O(n log n)"
        ],
        "correct": 1,
        "explanation": "O(1) represents 'Constant Time' complexity. This implies that the algorithm's performance is independent of the size of the data set. Common examples include accessing an array element by index or pushing a value onto a stack. Logarithmic and linear growth (Options 0 and 2) still increase as n grows, albeit at different rates.",
        "topic": "Complexity Analysis"
    },
    {
        "question": "In the context of Hashing, what is the mathematical purpose of the 'Modulo' (%) operator in the division method?",
        "options": [
            "To ensure the hash value is always a prime number.",
            "To map a large key value into a valid range of array indices (0 to size-1).",
            "To resolve secondary clustering in quadratic probing.",
            "To calculate the load factor () of the hash table."
        ],
        "correct": 1,
        "explanation": "The division method uses the formula 'index = key % table_size'. This arithmetic operation ensures that no matter how large the input key is, the resulting index will fall within the legal bounds of the hash table array. It does not guarantee primality or inherently calculate the load factor.",
        "topic": "Hashing"
    },
    {
        "question": "According to the final lectures of CS301, which data structure is described as a 'very recent' development within the Table ADT topic?",
        "options": [
            "Binary Search Tree",
            "AVL Tree",
            "Skip List",
            "B-Tree"
        ],
        "correct": 2,
        "explanation": "The handouts explicitly mention the Skip List as a 'very recent' data structure studied within the Table ADT. It provides a probabilistic alternative to balanced trees, offering O(log n) average performance for search, insertion, and deletion using multiple linked list layers. ",
        "topic": "Table ADT"
    },
    {
        "question": "What is the Big-O complexity of a 'Successive Deletion' process in a Max-Heap to obtain a sorted list of all 'n' elements?",
        "options": [
            "O(n)",
            "O(n log n)",
            "O(n^2)",
            "O(log n)"
        ],
        "correct": 1,
        "explanation": "Deleting the maximum element (the root) from a heap takes O(log n) time to restore the heap property. To sort the entire set, this operation must be performed 'n' times. Therefore, the total time complexity is O(n * log n). This is the underlying logic of the Heapsort algorithm.",
        "topic": "Heap Analysis"
    },
    {
        "question": "When  (Load Factor) approaches 1.0 in a Linear Probing hash table, the average search time becomes closer to ________.",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "As the load factor  approaches 1.0, the hash table becomes nearly full, and 'gaps' disappear. In Linear Probing, this leads to primary clustering where the algorithm must probe through almost every occupied slot to find an empty one or a specific key, resulting in linear O(n) performance.",
        "topic": "Hashing Performance"
    },
    {
        "question": "Which of the following is a characteristic of 'B-Trees' that makes them ideal for file systems and databases?",
        "options": [
            "They are always strictly binary trees.",
            "Each node can have many children, resulting in a very shallow tree (low height).",
            "They are implemented using recursion on a system stack.",
            "They do not allow duplicate keys."
        ],
        "correct": 1,
        "explanation": "B-Trees are multi-way search trees. Because each node has a high branching factor (many children), the total height of the tree is kept extremely low even for millions of records. This minimizes disk I/O operations, which are much slower than memory access. ",
        "topic": "B-Trees"
    },
    {
        "question": "In 'Separate Chaining', if the hash function is perfectly uniform, the average length of the linked list in each bucket is equal to ________.",
        "options": [
            "The table size (m)",
            "The number of elements (n)",
            "The load factor ( = n/m)",
            "log(n)"
        ],
        "correct": 2,
        "explanation": "Separate Chaining distributes n elements across m buckets. Under uniform hashing, each bucket is expected to hold an equal share of elements, which is exactly the load factor (). If  is small (e.g., 1.0), search time remains O(1) on average.",
        "topic": "Hashing Analysis"
    },
    {
        "question": "What happens in an AVL tree if an insertion creates a balance factor of +2 and the new node was added to the left subtree of the left child?",
        "options": [
            "A Single Left Rotation is performed.",
            "A Single Right Rotation is performed.",
            "A Double Left-Right Rotation is performed.",
            "The node is deleted to maintain balance."
        ],
        "correct": 1,
        "explanation": "An imbalance where the left child's left subtree is too tall (the LL case) is corrected by a Single Right Rotation around the unbalanced node. This 'pulls' the structure back into the permissible -1, 0, +1 range. ",
        "topic": "AVL Trees"
    },
    {
        "question": "Which specific optimization in Disjoint Sets ensures that all nodes in a path point directly to the root during a 'Find' operation?",
        "options": [
            "Union by Rank",
            "Union by Height",
            "Path Compression",
            "Pivot Selection"
        ],
        "correct": 2,
        "explanation": "Path Compression is a technique applied during the 'Find' operation. It updates the parent pointers of all nodes visited during the search to point directly to the root, effectively flattening the tree and speeding up all future 'Find' requests for those nodes.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "If you need a data structure to implement a 'Priority Queue' where extraction of the highest priority element is frequent, which is the most efficient choice?",
        "options": [
            "Unordered Array",
            "Sorted Linked List",
            "Binary Heap",
            "Binary Search Tree"
        ],
        "correct": 2,
        "explanation": "A Binary Heap provides an ideal balance for priority queues: O(log n) for both insertion and extraction (deleteMin/deleteMax). While a sorted list allows O(1) extraction, its O(n) insertion makes it overall less efficient than a heap for dynamic data.",
        "topic": "Data Structure Selection"
    },
    {
        "question": "A 'Strongly Connected' directed graph is defined as a graph where:",
        "options": [
            "There is an edge between every pair of vertices.",
            "Every vertex has an out-degree of at least 1.",
            "There is a directed path from every vertex to every other vertex.",
            "The graph is a Directed Acyclic Graph (DAG)."
        ],
        "correct": 2,
        "explanation": "Strong connectivity is a property of directed graphs where any node 'u' can reach any node 'v' through some sequence of directed edges. If this is only true when directions are ignored, the graph is called 'weakly connected'.",
        "topic": "Graph Theory"
    },
    {
        "question": "In the final summary (Lecture 45), the examiner notes that 'hashing' is essentially a ________.",
        "options": [
            "Complex data structure",
            "Purely algorithmic procedure",
            "Type of linked list",
            "Hardware optimization"
        ],
        "correct": 1,
        "explanation": "As noted on page 505, 'Hashing was a purely algorithmic procedure and there was nothing much as a data structure.' It focuses on the logic of index calculation rather than the physical arrangement of pointers used in trees or lists.",
        "topic": "Course Summary"
    },
    {
        "question": "Which of the following describes the 'Inverse Ackermann Function' (n)?",
        "options": [
            "A function that grows linearly with n.",
            "A function used to measure the height of a B-Tree.",
            "A function that grows so slowly it is practically constant (less than 5) for all realistic n.",
            "A function that represents the worst-case of Quicksort."
        ],
        "correct": 2,
        "explanation": "The amortized complexity of Disjoint Set operations using both Path Compression and Union by Rank is O((n)). This function is legendary for how slowly it grows; even for n equal to the number of atoms in the universe, (n) remains less than 5, making the operations nearly O(1).",
        "topic": "Complexity Classes"
    },
    {
        "question": "In a B-Tree of order 4, what is the minimum number of keys allowed in a non-root internal node?",
        "options": [
            "1",
            "2",
            "3",
            "4"
        ],
        "correct": 0,
        "explanation": "The minimum number of keys in a B-Tree node of order 'm' is ceil(m/2) - 1. For order 4, this is ceil(4/2) - 1 = 2 - 1 = 1 key. This ensures the tree maintains a high level of occupancy while remaining flexible.",
        "topic": "B-Trees"
    },
    {
        "question": "The time complexity to search for a key in an AVL tree with 'n' nodes is ________.",
        "options": [
            "O(n)",
            "O(log n)",
            "O(1)",
            "O(n^2)"
        ],
        "correct": 1,
        "explanation": "AVL trees are self-balancing binary search trees. Their strict balancing rule ensures that the height is always logarithmic (O(log n)). Since a search only follows one path from root to leaf, it is guaranteed to take logarithmic time.",
        "topic": "AVL Analysis"
    },
    {
        "question": "Which property of a 'Complete Binary Tree' allows it to be represented efficiently as an array?",
        "options": [
            "Every node must have two children.",
            "The keys are stored in sorted order.",
            "There are no 'holes' in the node levels except possibly the last level (filled left to right).",
            "It contains a path to every leaf of the same length."
        ],
        "correct": 2,
        "explanation": "Because a complete tree is filled level by level from left to right, we can map node positions to array indices (1, 2, 3...) without wasting space. This is why heaps (which are complete trees) are almost always implemented with arrays. ",
        "topic": "Heap Implementation"
    },
    {
        "question": "Topological sort is used to find a linear ordering of vertices in which type of graph?",
        "options": [
            "Undirected Graph",
            "Complete Graph",
            "Directed Acyclic Graph (DAG)",
            "Connected Graph"
        ],
        "correct": 2,
        "explanation": "Topological sorting requires a Directed Acyclic Graph. If a graph has a cycle, no linear order is possible because the nodes in the cycle would depend on each other, creating a deadlock. It is commonly used for prerequisite task scheduling.",
        "topic": "Graph Algorithms"
    },
    {
        "question": "According to the handouts, what is the role of the 'Examiner' or 'Senior Examiner' during evaluation?",
        "options": [
            "To provide the code for students.",
            "To test domain knowledge of design choices.",
            "To teach C++ syntax basics.",
            "To explain the physical components of a PC."
        ],
        "correct": 1,
        "explanation": "As stated in the final summary of Lecture 45, the goal of the course is to increase the student's 'domain knowledge of design choices' so they can resolve design problems in professional life.",
        "topic": "Course Summary"
    },
    {
        "question": "What is 'Secondary Clustering' in the context of Quadratic Probing?",
        "options": [
            "The tendency for different hash values to end up in the same sequence of probes.",
            "The tendency for identical hash values to end up in the same sequence of probes.",
            "When the hash table size is not prime.",
            "When the load factor exceeds 1.0."
        ],
        "correct": 1,
        "explanation": "Primary clustering is the formation of large blocks of occupied slots. Secondary clustering is more subtle; if two keys have the same initial hash, they will follow the exact same probe path in quadratic probing. Double hashing solves this because the step size depends on the key itself.",
        "topic": "Hashing Analysis"
    },
    {
        "question": "In a 'Skip List', the height of a node is determined ________.",
        "options": [
            "By the height of the AVL tree.",
            "By the value of the key.",
            "Probabilistically (randomly) during insertion.",
            "By the order of the B-Tree."
        ],
        "correct": 2,
        "explanation": "Skip Lists use 'randomization' to maintain balance. When a node is inserted, a 'coin flip' logic determines how many levels (express lanes) that node will occupy, avoiding complex rebalancing rotations.",
        "topic": "Skip Lists"
    },
    {
        "question": "The amortized complexity of 'Build-Heap' (creating a heap from an unordered array) is ________.",
        "options": [
            "O(n)",
            "O(n log n)",
            "O(log n)",
            "O(1)"
        ],
        "correct": 0,
        "explanation": "While it seems like it should be O(n log n), the mathematically correct complexity for Build-Heap using the bottom-up percolate-down method is linear O(n). This is because most nodes are near the bottom and travel short distances.",
        "topic": "Heap Operations"
    },
    {
        "question": "Which of the following is true for an Adjacency Matrix representing an undirected graph?",
        "options": [
            "The matrix is always square (V x V).",
            "The matrix is symmetric along the main diagonal.",
            "Both A and B.",
            "None of the above."
        ],
        "correct": 2,
        "explanation": "For an undirected graph, an edge between u and v implies a relationship in both directions. Therefore, Matrix[u,v] and Matrix[v,u] will both be 1. This makes the matrix symmetric and, like all adjacency matrices, it must be square (V x V).",
        "topic": "Graph Representations"
    },
    {
        "question": "In Double Hashing, if h1(k) results in a collision, the second hash function h2(k) must ________.",
        "options": [
            "Always return 0.",
            "Never return 0.",
            "Be equal to the table size.",
            "Be a prime number greater than the table size."
        ],
        "correct": 1,
        "explanation": "The second hash function h2(k) determines the 'step size' for probing. If it returned 0, the algorithm would stay at the same index in an infinite loop. Thus, h2(k) must be designed to return a non-zero value.",
        "topic": "Double Hashing"
    },
    {
        "question": "What is the time complexity of 'Binary Search' on a sorted array of size n?",
        "options": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "Binary search works by repeatedly halving the search space. The number of times you can halve 'n' before reaching 1 is log2(n), making it O(log n).",
        "topic": "Searching Algorithms"
    },
    {
        "question": "A complete binary tree with 7 nodes has how many internal nodes?",
        "options": [
            "3",
            "4",
            "7",
            "1"
        ],
        "correct": 0,
        "explanation": "In a perfect/complete binary tree with 7 nodes, the nodes are at index 1, 2, 3 (internal) and 4, 5, 6, 7 (leaves). Indices 1, 2, and 3 have children, so they are internal nodes. ",
        "topic": "Binary Tree Terminology"
    },
    {
        "question": "Which complexity class is considered 'Exponential'?",
        "options": [
            "O(n^2)",
            "O(2^n)",
            "O(n!)",
            "O(log n)"
        ],
        "correct": 1,
        "explanation": "O(2^n) is exponential growth, where the time required doubles with every addition to the input size n. O(n^2) is polynomial (quadratic), and O(n!) is factorial.",
        "topic": "Complexity Classes"
    },
    {
        "question": "In the final lecture, what is described as a 'companion' to an algorithm?",
        "options": [
            "The Compiler",
            "The User",
            "The Data Structure",
            "The Operating System"
        ],
        "correct": 2,
        "explanation": "As quoted from page 505: 'Actually, data structure becomes a companion to an algorithm.' Choosing an algorithm (like searching) naturally forces the choice of an appropriate data structure (like a tree).",
        "topic": "Course Summary"
    },
    {
        "question": "What is the maximum number of children any internal node in a B-Tree of order 'm' can have?",
        "options": [
            "m-1",
            "m",
            "m/2",
            "2m"
        ],
        "correct": 1,
        "explanation": "The 'order' m of a B-Tree directly specifies the maximum branching factor. An internal node can have at most m children and m-1 keys.",
        "topic": "B-Trees"
    },
    {
        "question": "Which sorting algorithm is characterized by its average-case performance of O(n log n) but worst-case of O(n^2)?",
        "options": [
            "Merge Sort",
            "Heap Sort",
            "Quick Sort",
            "Selection Sort"
        ],
        "correct": 2,
        "explanation": "Quicksort is extremely fast on average (O(n log n)), but if the pivot is consistently chosen poorly (like the smallest/largest element in a sorted array), it degrades to quadratic O(n^2) time.",
        "topic": "Sorting Analysis"
    },
    {
        "question": "The memory overhead of 'Separate Chaining' compared to 'Open Addressing' is due to ________.",
        "options": [
            "Using larger arrays.",
            "The creation of individual node objects for the linked lists.",
            "The complexity of the hash function.",
            "Storing keys as strings instead of integers."
        ],
        "correct": 1,
        "explanation": "Separate chaining requires dynamic memory allocation for each new node in the linked lists (the pointers and the node objects themselves), whereas Open Addressing stores everything within the pre-allocated array indices.",
        "topic": "Hashing Analysis"
    },
    {
        "question": "What is the height of an empty binary tree?",
        "options": [
            "0",
            "1",
            "-1",
            "NULL"
        ],
        "correct": 2,
        "explanation": "By standard convention used in data structure textbooks (including VU handouts), the height of an empty tree is -1, and the height of a tree with only a root node is 0.",
        "topic": "Binary Tree Basics"
    },
    {
        "question": "Rehashing is often performed when the load factor () of an Open Addressing table exceeds ________.",
        "options": [
            "0.1",
            "0.5",
            "1.0",
            "2.0"
        ],
        "correct": 1,
        "explanation": "Empirical analysis shows that for probing methods (linear/quadratic), performance drops off sharply once the table is more than half full ( > 0.5). Rehashing into a larger table restores constant-time performance.",
        "topic": "Hashing Operations"
    },
    {
        "question": "Which operation in a 'Disjoint Set' is used to determine which group an element belongs to?",
        "options": [
            "Union(x, y)",
            "Find(x)",
            "Rehash(x)",
            "Sort(x)"
        ],
        "correct": 1,
        "explanation": "The 'Find' operation identifies the representative (root) of the set containing the element x. If two elements have the same representative, they belong to the same set.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "In an Adjacency List for a graph with V vertices and E edges, what is the space complexity?",
        "options": [
            "O(V^2)",
            "O(V + E)",
            "O(E log V)",
            "O(1)"
        ],
        "correct": 1,
        "explanation": "An Adjacency List stores an array of size V (the vertices) and a linked list node for every edge (E). Therefore, the total space required is proportional to the number of vertices plus the number of edges.",
        "topic": "Graph Representations"
    },
    {
        "question": "Which of the following describes 'Big-O' notation's primary use?",
        "options": [
            "To calculate the exact number of milliseconds a program runs.",
            "To provide an upper bound on the growth rate of an algorithm.",
            "To determine the memory capacity of the hard drive.",
            "To identify logic errors in C++ templates."
        ],
        "correct": 1,
        "explanation": "Big-O provides a mathematical 'worst-case' ceiling. It tells you that the algorithm will never grow faster than the specified rate as the input size n increases.",
        "topic": "Complexity Analysis"
    },
    {
        "question": "In a directed graph (digraph), if the sum of the 'indegrees' of all vertices is denoted by S_in and the sum of the 'outdegrees' is denoted by S_out, which of the following is mathematically true?",
        "options": [
            "S_in > S_out",
            "S_in < S_out",
            "S_in = S_out = Total number of edges (E)",
            "S_in + S_out = Total number of vertices (V)"
        ],
        "correct": 2,
        "explanation": "In any directed graph, every edge has exactly one starting vertex and one ending vertex. Therefore, each edge contributes exactly 1 to the total outdegree count and exactly 1 to the total indegree count. This means the sum of indegrees must equal the sum of outdegrees, and both must equal the total number of edges in the graph. ",
        "topic": "Graph Theory"
    },
    {
        "question": "Which of the following describes the 'Adjacency Matrix' of an undirected graph with 'V' vertices and no self-loops?",
        "options": [
            "A square matrix where the main diagonal contains only 1s.",
            "A symmetric matrix where Matrix[i,j] = 1 if there is an edge between vertex i and j.",
            "A rectangular matrix of size V x E.",
            "A matrix where the sum of each row is always equal to V."
        ],
        "correct": 1,
        "explanation": "An adjacency matrix for an undirected graph is always symmetric because an edge between i and j implies a connection from j to i as well (Matrix[i,j] = Matrix[j,i]). Without self-loops, the main diagonal (where i=j) must contain only 0s. Option 2 describes an Incidence Matrix, not an Adjacency Matrix. ",
        "topic": "Graph Representations"
    },
    {
        "question": "What is the time complexity to find all neighbors of a specific vertex 'u' in a graph represented by an 'Adjacency List'?",
        "options": [
            "O(1)",
            "O(V)",
            "O(degree(u))",
            "O(V^2)"
        ],
        "correct": 2,
        "explanation": "In an adjacency list, all neighbors of vertex 'u' are stored in a linked list at index 'u' of the array. To find all neighbors, the algorithm simply traverses that specific list. The time taken is proportional to the number of nodes in that list, which is the degree of the vertex. In an adjacency matrix, this would take O(V) because the entire row must be scanned.",
        "topic": "Graph Representations"
    },
    {
        "question": "Which complexity class represents the most efficient 'Growth Rate' for an algorithm dealing with extremely large input sizes (n  )?",
        "options": [
            "O(n)",
            "O(n log n)",
            "O(log n)",
            "O(2^n)"
        ],
        "correct": 2,
        "explanation": "Logarithmic growth O(log n) is significantly slower than linear O(n) or linear-logarithmic O(n log n) growth. For a dataset of 1 billion items, log(n) is approximately 30, while n is 1 billion. O(2^n) is exponential and the least efficient. ",
        "topic": "Algorithm Analysis"
    },
    {
        "question": "According to the summary in Lecture 45, why is a 'Hash Table' often referred to as a 'companion' to a compiler?",
        "options": [
            "It is used to sort the source code alphabetically.",
            "It is the primary tool for building and maintaining the 'Symbol Table'.",
            "It is used to manage the physical memory of the CPU.",
            "It allows the compiler to run without using a stack."
        ],
        "correct": 1,
        "explanation": "The handouts conclude that data structures are companions to algorithms. Specifically, hashing is the purely algorithmic procedure used to build 'symbol tables' in compilers, which store information about variables and functions for O(1) average-time lookup.",
        "topic": "Course Summary"
    },
    {
        "question": "A 'Directed Acyclic Graph' (DAG) is a directed graph that ________.",
        "options": [
            "Contains at least one cycle.",
            "Has no directed cycles.",
            "Is always a complete graph.",
            "Has exactly two connected components."
        ],
        "correct": 1,
        "explanation": "By definition, a DAG is a directed graph that does not contain any path starting at a vertex and returning to it. This property is crucial for algorithms like Topological Sort and represents systems with clear precedence or dependencies. ",
        "topic": "Graph Types"
    },
    {
        "question": "In the 'Table ADT' section, which data structure is identified as providing a 'probabilistic' alternative to AVL trees?",
        "options": [
            "B-Tree",
            "Skip List",
            "Binary Heap",
            "Circular Queue"
        ],
        "correct": 1,
        "explanation": "Skip Lists are probabilistic data structures that use multiple layers of linked lists. By assigning levels to nodes using random 'coin flips' during insertion, they achieve O(log n) average performance for search and update, similar to balanced BSTs but without rigid rotation rules.",
        "topic": "Skip Lists"
    },
    {
        "question": "What is the maximum height of a 'Binary Search Tree' with 'n' nodes, and when does it occur?",
        "options": [
            "O(log n), when the tree is perfectly balanced.",
            "O(n), when the elements are inserted in strictly sorted order.",
            "O(1), when the tree only contains a root.",
            "O(n^2), when the tree is skewed."
        ],
        "correct": 1,
        "explanation": "In the worst case, a BST becomes 'skewed' (like a linked list) if keys are inserted in sorted (ascending or descending) order. In this scenario, every node has only one child, and the height becomes n-1, resulting in O(n) search time.",
        "topic": "BST Analysis"
    },
    {
        "question": "Which of the following is a 'greedy' algorithm used for optimal prefix-free data compression?",
        "options": [
            "Quicksort",
            "Huffman Coding",
            "Binary Search",
            "Dijkstra's Algorithm"
        ],
        "correct": 1,
        "explanation": "Huffman coding is a greedy algorithm that builds an optimal binary merge tree by repeatedly combining the two lowest-frequency nodes. This results in the shortest possible bit-lengths for the most frequent characters. ",
        "topic": "Huffman Coding"
    },
    {
        "question": "The 'Handshaking Lemma' states that in any undirected graph, the sum of the degrees of all vertices is equal to ________.",
        "options": [
            "The number of vertices (V)",
            "The number of edges (E)",
            "Twice the number of edges (2E)",
            "The square of the number of edges (E^2)"
        ],
        "correct": 2,
        "explanation": "Since every edge in an undirected graph connects two vertices, it contributes exactly 1 to the degree of each of those two vertices. Therefore, the total sum of degrees across the entire graph is exactly double the number of edges.",
        "topic": "Graph Theory"
    },
    {
        "question": "In 'Quadratic Probing', if the table size is prime and the load factor  is ________, it is guaranteed to find an empty slot.",
        "options": [
            " = 1.0",
            " > 0.5",
            "  0.5",
            " = 0.75"
        ],
        "correct": 2,
        "explanation": "Mathematical proofs provided in the Hashing lectures show that Quadratic Probing will always succeed in finding an empty cell if the table size is a prime number and the table is no more than half full (  0.5). Beyond this point, the probe sequence may cycle.",
        "topic": "Hashing"
    },
    {
        "question": "Which data structure is most efficient for implementing a 'First-In, First-Out' (FIFO) buffer?",
        "options": [
            "Stack",
            "Queue",
            "Binary Heap",
            "AVL Tree"
        ],
        "correct": 1,
        "explanation": "A Queue ADT is specifically defined by the FIFO principle, where the first element added is the first to be removed. This is essential for task scheduling and message buffering. ",
        "topic": "Queue ADT"
    },
    {
        "question": "What happens to the 'Path Compression' optimization in Disjoint Sets during a 'Find' operation?",
        "options": [
            "It makes the tree taller to allow for more nodes.",
            "It makes every node on the path point directly to the root, flattening the tree.",
            "It deletes all nodes that are not the root.",
            "It sorts the elements in the set."
        ],
        "correct": 1,
        "explanation": "Path Compression is a 'lazy' optimization. As we traverse from a node to the root, we update each node's parent pointer to point directly to the root. This drastically reduces the height of the tree for all future 'Find' operations, leading to nearly constant-time performance.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "In an 'Adjacency Matrix' for a directed graph, what does a value of 1 at Matrix[3,5] indicate?",
        "options": [
            "There is an edge from vertex 5 to vertex 3.",
            "There is an edge from vertex 3 to vertex 5.",
            "Vertices 3 and 5 are the same node.",
            "The weight of vertex 3 is 5."
        ],
        "correct": 1,
        "explanation": "In an adjacency matrix for directed graphs, the row index 'i' represents the source (from) and the column index 'j' represents the destination (to). Therefore, Matrix[3,5]=1 means a directed edge exists from 3 to 5.",
        "topic": "Adjacency Matrix"
    },
    {
        "question": "Which of the following sorting algorithms is NOT based on the 'Divide and Conquer' strategy?",
        "options": [
            "Merge Sort",
            "Quick Sort",
            "Selection Sort",
            "Heap Sort"
        ],
        "correct": 2,
        "explanation": "Selection sort is a simple comparison sort that repeatedly finds the minimum element from the unsorted part and puts it at the beginning. It does not divide the problem into independent sub-problems. Merge Sort and Quick Sort are the primary examples of Divide and Conquer.",
        "topic": "Sorting Algorithms"
    },
    {
        "question": "In a B-Tree of order 'm', every node except the root must have at least ________ children.",
        "options": [
            "2",
            "m/2",
            "ceil(m/2)",
            "m-1"
        ],
        "correct": 2,
        "explanation": "B-Trees are designed to be at least half-full to maintain balance and efficiency. Every internal node (except the root) must have between ceil(m/2) and m children. ",
        "topic": "B-Trees"
    },
    {
        "question": "What is the time complexity of 'Binary Search' on a sorted linked list?",
        "options": [
            "O(log n)",
            "O(n)",
            "O(n log n)",
            "O(1)"
        ],
        "correct": 1,
        "explanation": "Binary search requires O(1) access to the middle element. Since a linked list only provides O(n) sequential access, you must traverse to the middle in every step. This makes the total time O(n), negating the advantage of the binary search algorithm. This is why binary search is performed on arrays.",
        "topic": "Algorithm Efficiency"
    },
    {
        "question": "The 'Load Factor' () of a hash table is defined as the ratio of ________.",
        "options": [
            "Table size to number of elements.",
            "Number of elements to table size.",
            "Number of collisions to table size.",
            "Number of elements to number of collisions."
        ],
        "correct": 1,
        "explanation": " = n/m. It represents how 'full' the table is. As  increases, the probability of collisions increases, particularly in open addressing schemes.",
        "topic": "Hashing"
    },
    {
        "question": "Which data structure provides O(log n) for search, insert, and delete, and is always perfectly height-balanced?",
        "options": [
            "Binary Search Tree",
            "AVL Tree",
            "Binary Heap",
            "Singly Linked List"
        ],
        "correct": 1,
        "explanation": "AVL trees are self-balancing BSTs where the heights of the two subtrees of any node differ by at most one. This strict balancing ensures O(log n) for all primary operations, unlike standard BSTs which can become O(n).",
        "topic": "AVL Trees"
    },
    {
        "question": "In a 'Max-Heap', where is the second-largest element located?",
        "options": [
            "At the root.",
            "At the very last index of the array.",
            "At either index 2 or index 3 (children of the root).",
            "In the middle of the array."
        ],
        "correct": 2,
        "explanation": "The largest element is at index 1 (the root). According to the heap-order property, the second-largest element must be one of the root's children. Therefore, it will be found at index 2 (left child) or index 3 (right child).",
        "topic": "Heaps"
    },
    {
        "question": "Which specific rotation is used in an AVL tree when a node is inserted into the 'right subtree of the right child'?",
        "options": [
            "Single Left Rotation",
            "Single Right Rotation",
            "Double Left-Right Rotation",
            "Double Right-Left Rotation"
        ],
        "correct": 0,
        "explanation": "This is the RR (Right-Right) case. To fix a right-heavy imbalance in a straight line, a Single Left Rotation is performed around the unbalanced node. ",
        "topic": "AVL Rotations"
    },
    {
        "question": "What is the amortized complexity of 'Build-Heap' for 'n' elements using the bottom-up approach?",
        "options": [
            "O(n log n)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ],
        "correct": 1,
        "explanation": "While it seems like 'n' insertions would take O(n log n), the bottom-up 'buildHeap' algorithm is mathematically proven to take O(n) because the work done decreases as we move toward the root where fewer nodes exist.",
        "topic": "Heap Operations"
    },
    {
        "question": "A graph traversal that visits all vertices at the current depth before moving to the next level is:",
        "options": [
            "Depth First Search (DFS)",
            "Breadth First Search (BFS)",
            "In-order Traversal",
            "Topological Sort"
        ],
        "correct": 1,
        "explanation": "BFS uses a queue to visit neighbors level-by-level. It is the standard algorithm for finding the shortest path in an unweighted graph. ",
        "topic": "Graph Traversals"
    },
    {
        "question": "In a 'Disjoint Set' implemented using an array, if S[i] = -5, what does this value signify?",
        "options": [
            "The element 'i' has a parent at index 5.",
            "The element 'i' is the root of a set containing 5 elements.",
            "The element 'i' is an invalid node.",
            "The set has a height of 5."
        ],
        "correct": 1,
        "explanation": "In 'Union by Size' optimization, the root node stores the negative of the total number of nodes in that set. A value of -5 indicates that 'i' is a representative (root) and there are 5 elements in its group.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "Which of the following is NOT an application of the 'Stack' data structure?",
        "options": [
            "Function call management (Activation Records).",
            "Undo mechanism in text editors.",
            "Arithmetic expression evaluation (Postfix).",
            "Printer spooling (Print Queue)."
        ],
        "correct": 3,
        "explanation": "Printer spooling requires documents to be printed in the order they were received (FIFO), which is the job of a Queue. Stacks are LIFO and are used for function calls and backtracking.",
        "topic": "Stack Applications"
    },
    {
        "question": "Which complexity class describes an algorithm where doubling the input size results in only one additional operation?",
        "options": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ],
        "correct": 2,
        "explanation": "In O(log n), because the base is 2, every time you double 'n', the log(n) value increases by exactly 1. This is characteristic of Binary Search.",
        "topic": "Complexity Classes"
    },
    {
        "question": "A 'Strongly Connected' directed graph is one where ________.",
        "options": [
            "There is an edge between every pair of vertices.",
            "There is a directed path from every vertex to every other vertex.",
            "Every vertex has an outdegree of 1.",
            "The graph is acyclic."
        ],
        "correct": 1,
        "explanation": "Strong connectivity means that you can travel from any node to any other node following the directed edges. If the graph is not strongly connected, it can be broken into 'Strongly Connected Components' (SCCs).",
        "topic": "Graph Theory"
    },
    {
        "question": "In 'Double Hashing', the second hash function h2(k) must never return ________.",
        "options": [
            "A prime number.",
            "Zero.",
            "A value larger than the table size.",
            "A value equal to h1(k)."
        ],
        "correct": 1,
        "explanation": "If h2(k) returns 0, the probe sequence would remain at the same index forever (index + 0, index + 0...), causing an infinite loop. Thus, h2(k) must result in a non-zero step size.",
        "topic": "Double Hashing"
    },
    {
        "question": "What is the 'degree' of a node in a Binary Tree?",
        "options": [
            "The number of its ancestors.",
            "The number of its children (0, 1, or 2).",
            "The height of the tree from that node.",
            "The level of the node starting from root."
        ],
        "correct": 1,
        "explanation": "In tree terminology, 'degree' refers to the number of subtrees (children) a node has. For a binary tree, the maximum degree of any node is 2.",
        "topic": "Tree Terminology"
    },
    {
        "question": "Which of the following is true regarding 'Generic Programming' in C++?",
        "options": [
            "It allows code to work only with integers.",
            "It uses 'templates' to write code independent of data types.",
            "It requires a separate class for every possible data type.",
            "It makes the program run significantly slower than non-generic code."
        ],
        "correct": 1,
        "explanation": "Templates in C++ enable generic programming, allowing a single implementation of a data structure (like Stack or List) to be reused for integers, strings, or custom objects, improving code reusability.",
        "topic": "C++ Templates"
    },
    {
        "question": "According to the final lecture, what is the 'companion' of an algorithm?",
        "options": [
            "The compiler",
            "The data structure",
            "The operating system",
            "The hardware"
        ],
        "correct": 1,
        "explanation": "Lecture 45 concludes by stating that data structures become companions to algorithms. Choosing an algorithm to solve a problem naturally 'brings along' a specific data structure to facilitate that solution.",
        "topic": "Course Summary"
    },
    {
        "question": "In an Adjacency List for a graph with 'V' vertices and 'E' edges, the total number of nodes in all linked lists combined for a directed graph is:",
        "options": [
            "V",
            "E",
            "2E",
            "V + E"
        ],
        "correct": 1,
        "explanation": "In a directed graph, each edge (u, v) is stored exactly once (as a node in the list for vertex u). Thus, the total number of linked list nodes across the entire array is exactly equal to the number of edges.",
        "topic": "Graph Representations"
    },
    {
        "question": "Which hashing method uses a linked list at each table index to resolve collisions?",
        "options": [
            "Linear Probing",
            "Quadratic Probing",
            "Separate Chaining",
            "Double Hashing"
        ],
        "correct": 2,
        "explanation": "Separate Chaining is an 'Open Hashing' technique where each cell in the hash table array points to a linked list of records that hashed to that same index.",
        "topic": "Hashing"
    },
    {
        "question": "What is the time complexity to access the 'top' element of a Stack implemented with a linked list?",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n log n)"
        ],
        "correct": 0,
        "explanation": "In a linked-list stack, the top is the head of the list. Since we maintain a pointer to the head, accessing it is a direct operation that does not depend on the number of elements in the stack.",
        "topic": "Stack Analysis"
    },
    {
        "question": "In a perfect binary tree of height 3, what is the total number of nodes?",
        "options": [
            "7",
            "8",
            "15",
            "31"
        ],
        "correct": 2,
        "explanation": "Formula for total nodes in a perfect binary tree is 2^(h+1) - 1. For height 3: 2^(3+1) - 1 = 16 - 1 = 15.",
        "topic": "Binary Trees"
    },
    {
        "question": "In an undirected graph with 'V' vertices and 'E' edges, what is the sum of the lengths of all adjacency lists in the graph's Adjacency List representation?",
        "options": [
            "E",
            "V",
            "2E",
            "V + E"
        ],
        "correct": 2,
        "explanation": "In an undirected graph, every edge (u, v) is stored twice: once in the adjacency list of vertex 'u' and once in the adjacency list of vertex 'v'. Therefore, the total number of nodes across all linked lists in the representation is exactly twice the number of edges. In a directed graph, each edge is only stored once, making the sum equal to E.",
        "topic": "Graph Representations"
    },
    {
        "question": "Which of the following describes the relationship between the 'height' (h) and the 'number of nodes' (n) in a perfectly balanced Binary Tree?",
        "options": [
            "h = O(n)",
            "h = O(log n)",
            "h = O(n log n)",
            "h = O(1)"
        ],
        "correct": 1,
        "explanation": "A perfectly balanced binary tree (like a complete or perfect tree) ensures that every level is filled before moving to the next. Mathematically, the number of nodes doubles with each level, meaning the height is the logarithm (base 2) of the number of nodes. This logarithmic height is what guarantees the efficiency of search operations in structures like AVL trees. ",
        "topic": "Binary Trees"
    },
    {
        "question": "What is the Big-O complexity of checking if a directed graph contains a cycle using a Topological Sort algorithm (Kahn's Algorithm)?",
        "options": [
            "O(V)",
            "O(E)",
            "O(V + E)",
            "O(V^2)"
        ],
        "correct": 2,
        "explanation": "Topological sorting using Kahn's algorithm involves calculating the indegree of all vertices and processing nodes with indegree 0. Each vertex is visited once, and each edge is 'removed' exactly once. This results in a linear time complexity relative to the size of the graph components. If the process finishes and not all vertices have been sorted, it indicates the presence of a cycle.",
        "topic": "Graph Algorithms"
    },
    {
        "question": "According to Lecture 45 of the CS301 handouts, which data structure is specifically noted for being primarily important from an 'algorithmic' point of view rather than being discussed as a standard ADT in the earlier sections?",
        "options": [
            "Stack",
            "Queue",
            "Graph",
            "Linked List"
        ],
        "correct": 2,
        "explanation": "The final summary in the handouts explicitly points out that while Stacks, Queues, and Trees were covered as core ADTs, Graphs are primarily important from an algorithmic point of view (such as for networking or pathfinding). They are characterized more by the algorithms that run on them than by simple data insertion/deletion. ",
        "topic": "Course Summary"
    },
    {
        "question": "In 'Separate Chaining', if a collision occurs and we use an unordered linked list at each bucket, what is the worst-case complexity for inserting a new element?",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(1) average, O(n) if key must be unique"
        ],
        "correct": 3,
        "explanation": "If the implementation allows duplicate keys, insertion is O(1) as we just add to the head of the list. However, if we must ensure uniqueness, we must scan the entire linked list at that index. In the absolute worst case (where all 'n' elements hash to the same bucket), this scan takes linear time. ",
        "topic": "Hashing"
    },
    {
        "question": "A graph where every vertex has an edge to every other vertex is known as a 'Complete Graph'. How many edges does a directed complete graph with 'n' vertices have (assuming no self-loops)?",
        "options": [
            "n(n-1)/2",
            "n",
            "n(n-1)",
            "n^2"
        ],
        "correct": 2,
        "explanation": "In an undirected complete graph, there are n(n-1)/2 edges. However, in a directed complete graph, every pair of vertices (u, v) must have an edge from u to v AND an edge from v to u. This doubles the total edge count to n(n-1).",
        "topic": "Graph Theory"
    },
    {
        "question": "Which specific approach to hash table collision resolution avoids 'Clustering' by making the probe increment a function of the key?",
        "options": [
            "Linear Probing",
            "Quadratic Probing",
            "Double Hashing",
            "Separate Chaining"
        ],
        "correct": 2,
        "explanation": "Double Hashing uses a second hash function, h2(key), to determine the step size. Because the jump distance is different for different keys (even if they hash to the same primary index), it eliminates both primary and secondary clustering seen in other open-addressing methods. ",
        "topic": "Hashing"
    },
    {
        "question": "In a Max-Heap implemented as an array, where could the 'Third Largest' element be located?",
        "options": [
            "Only at index 3",
            "At either index 2 or 3",
            "Anywhere at Level 1 or Level 2 of the tree (indices 2 through 7)",
            "Always at a leaf node"
        ],
        "correct": 2,
        "explanation": "The largest is at the root (1). The second largest is one of its children (2 or 3). The third largest could be the other child of the root, or one of the children of the second-largest node. Therefore, it can be found at any position in the first three levels of the heap, excluding the root. ",
        "topic": "Heaps"
    },
    {
        "question": "What happens in a B-Tree of order 'm' if a deletion causes a node to have fewer than 'ceil(m/2) - 1' keys?",
        "options": [
            "The tree is immediately converted to a BST.",
            "The node is merged with a sibling or borrows a key from a sibling (Re-distribution).",
            "The key is replaced with a NULL pointer.",
            "The entire level is deleted."
        ],
        "correct": 1,
        "explanation": "B-Trees must maintain a minimum occupancy to ensure the tree stays dense and balanced. If a deletion makes a node too 'empty', the algorithm tries to borrow a key from an adjacent sibling. if the siblings are also at the minimum, the node is merged with a sibling, which may cause a cascading merge toward the root.",
        "topic": "B-Trees"
    },
    {
        "question": "Which of the following is a 'probabilistic' data structure mentioned in the handouts as an alternative to balanced search trees?",
        "options": [
            "AVL Tree",
            "Skip List",
            "Singly Linked List",
            "Min-Heap"
        ],
        "correct": 1,
        "explanation": "Skip Lists are probabilistic because they use randomization (coin flips) to determine node levels. Statistically, they achieve O(log n) performance for search, insert, and delete, providing the same efficiency as AVL trees but with a simpler, pointer-based implementation. ",
        "topic": "Skip Lists"
    },
    {
        "question": "What is the time complexity to find the 'Inorder Predecessor' of a given node in a balanced BST?",
        "options": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(h) where h is the height"
        ],
        "correct": 2,
        "explanation": "In a BST, the inorder predecessor is either the maximum node in the left subtree or an ancestor. Since a balanced BST has a height of log n, the path to find this node is limited by the height, resulting in O(log n) time.",
        "topic": "BST Operations"
    },
    {
        "question": "In an Adjacency Matrix for an undirected graph, if vertex 'i' has a degree of 'k', how many 1s will be in row 'i'?",
        "options": [
            "k",
            "k/2",
            "2k",
            "V"
        ],
        "correct": 0,
        "explanation": "The degree of a vertex in an undirected graph is the number of edges connected to it. Each edge connected to vertex 'i' is represented by a '1' in the i-th row (at the column corresponding to the neighbor). Thus, there are exactly 'k' ones in that row. ",
        "topic": "Graph Representations"
    },
    {
        "question": "Which complexity class represents an algorithm whose execution time increases linearly with the input size?",
        "options": [
            "O(1)",
            "O(n)",
            "O(n^2)",
            "O(log n)"
        ],
        "correct": 1,
        "explanation": "O(n) represents linear growth. This means if you double the number of inputs, the time taken (or number of operations) roughly doubles. Examples include a simple loop through an array or searching for an item in an unordered list.",
        "topic": "Big-O Notation"
    },
    {
        "question": "A 'Strictly Binary Tree' (Full Binary Tree) with 10 internal nodes has how many leaf nodes?",
        "options": [
            "10",
            "11",
            "9",
            "20"
        ],
        "correct": 1,
        "explanation": "In a strictly binary tree (where every node has either 0 or 2 children), the number of leaf nodes (L) is always one more than the number of internal nodes (I). L = I + 1. For 10 internal nodes, L = 11.",
        "topic": "Binary Trees"
    },
    {
        "question": "Which of the following is true for 'Path Compression' in the Union-Find data structure?",
        "options": [
            "It is performed during the Union operation.",
            "It is performed during the Find operation.",
            "It doubles the height of the tree.",
            "It makes the tree strictly binary."
        ],
        "correct": 1,
        "explanation": "Path Compression happens during the Find operation. As the algorithm traverses from a node to the root, it re-links all intermediate nodes to point directly to the root. This 'flattens' the tree, ensuring future searches are faster.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "In the Table ADT, what is the primary disadvantage of using a 'Sorted Array' implementation?",
        "options": [
            "Searching is slow (O(n)).",
            "Insertion and deletion are slow (O(n)) due to shifting elements.",
            "It uses too much memory for pointers.",
            "It cannot be used with Binary Search."
        ],
        "correct": 1,
        "explanation": "While sorted arrays allow O(log n) searching via binary search, adding or removing an element requires shifting every subsequent element to maintain order, which is a costly linear-time operation.",
        "topic": "Table ADT"
    },
    {
        "question": "Which sorting algorithm is known for being 'Stable'?",
        "options": [
            "Quicksort",
            "Heapsort",
            "Insertion Sort",
            "Selection Sort"
        ],
        "correct": 2,
        "explanation": "A stable sort preserves the relative order of elements with equal keys. Insertion sort is stable because it only moves an element past another if it is strictly smaller. Quicksort and Heapsort are generally unstable.",
        "topic": "Sorting"
    },
    {
        "question": "The amortized complexity of 'Build-Heap' for 'n' elements using the bottom-up percolate-down method is:",
        "options": [
            "O(n log n)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ],
        "correct": 1,
        "explanation": "Mathematically, building a heap by calling percolate-down starting from the last non-leaf node up to the root takes linear time O(n). This is because the total distance nodes can 'fall' is bounded by a geometric series that sums to O(n).",
        "topic": "Heaps"
    },
    {
        "question": "In a B-Tree of order 5, what is the maximum number of keys allowed in a single node?",
        "options": [
            "5",
            "4",
            "6",
            "2"
        ],
        "correct": 1,
        "explanation": "The maximum number of children is 'm' (5), and the maximum number of keys is 'm - 1' (4). ",
        "topic": "B-Trees"
    },
    {
        "question": "The time complexity of 'Topological Sort' using Kahn's algorithm is O(V+E). What does 'V' represent?",
        "options": [
            "Total number of edges.",
            "Total number of vertices (nodes).",
            "The value of the largest node.",
            "The version of the algorithm."
        ],
        "correct": 1,
        "explanation": "In graph notation, G=(V, E), where V stands for the set of Vertices and E stands for the set of Edges. Linear graph algorithms like BFS, DFS, and Topo-sort are typically O(V+E).",
        "topic": "Graph Algorithms"
    },
    {
        "question": "Which of the following is NOT a balanced search tree?",
        "options": [
            "AVL Tree",
            "B-Tree",
            "Red-Black Tree",
            "Standard BST"
        ],
        "correct": 3,
        "explanation": "A standard BST has no balancing mechanism. If keys are inserted in sorted order, it degenerates into a linear linked list (O(n)), whereas the others maintain logarithmic height.",
        "topic": "Binary Search Trees"
    },
    {
        "question": "In Huffman Coding, what is the relationship between character frequency and path length from the root?",
        "options": [
            "Higher frequency characters have longer paths.",
            "Higher frequency characters have shorter paths.",
            "All characters have equal path lengths.",
            "Frequency does not affect path length."
        ],
        "correct": 1,
        "explanation": "Huffman's greedy strategy places characters with higher frequencies closer to the root. This results in shorter binary codes (fewer bits) for frequent characters, optimizing overall compression.",
        "topic": "Huffman Coding"
    },
    {
        "question": "If a directed graph has 'n' nodes and no cycles, it is called a:",
        "options": [
            "Complete Graph",
            "Connected Graph",
            "DAG (Directed Acyclic Graph)",
            "Bipartite Graph"
        ],
        "correct": 2,
        "explanation": "A Directed Acyclic Graph (DAG) is the specific term for a digraph without cycles. It is the only type of graph that permits a topological sort.",
        "topic": "Graph Theory"
    },
    {
        "question": "What does a 'Dummy' value at index 0 in a Min-Heap implementation (Sentinel) represent?",
        "options": [
            "The root of the tree.",
            "A value smaller than any possible value in the heap.",
            "The last element of the heap.",
            "An error marker."
        ],
        "correct": 1,
        "explanation": "A sentinel at index 0 (smaller than all other elements) simplifies the 'percolate-up' code because the loop will automatically stop at the root without needing an explicit check for the index '1'.",
        "topic": "Heap Implementation"
    },
    {
        "question": "In an AVL tree, which rotation is required if a new node is inserted into the 'right subtree of the left child' of an unbalanced node?",
        "options": [
            "Single Left Rotation",
            "Single Right Rotation",
            "Double Left-Right (LR) Rotation",
            "Double Right-Left (RL) Rotation"
        ],
        "correct": 2,
        "explanation": "This is a zigzag imbalance (the LR case). It requires a double rotation: first a Left rotation on the child, then a Right rotation on the parent. ",
        "topic": "AVL Trees"
    },
    {
        "question": "The Load Factor () of an open addressing hash table should ideally be kept below ________ to maintain performance.",
        "options": [
            "1.0",
            "0.1",
            "0.5",
            "2.0"
        ],
        "correct": 2,
        "explanation": "Once a probing hash table is more than half full, the probability of collisions and the number of probes needed to find an empty slot increase sharply. Rehashing is usually triggered at  = 0.5.",
        "topic": "Hashing"
    },
    {
        "question": "Which data structure is the 'companion' to a compiler for building the symbol table?",
        "options": [
            "Stack",
            "Hashing",
            "Binary Heap",
            "Linked List"
        ],
        "correct": 1,
        "explanation": "As discussed in Lecture 45, Hashing is the algorithmic procedure used by compilers to efficiently store and look up symbols (identifiers) in constant time.",
        "topic": "Course Summary"
    },
    {
        "question": "In a B-Tree, all leaf nodes must be at:",
        "options": [
            "Level 0",
            "The same level",
            "Variable levels",
            "Depth equal to V"
        ],
        "correct": 1,
        "explanation": "A fundamental constraint of B-Trees is that they are perfectly height-balanced; every path from root to leaf must have exactly the same length.",
        "topic": "B-Trees"
    },
    {
        "question": "Which traversal of a BST visits the nodes in non-decreasing order?",
        "options": [
            "Pre-order",
            "Post-order",
            "In-order",
            "Level-order"
        ],
        "correct": 2,
        "explanation": "In-order traversal visits: Left Subtree -> Root -> Right Subtree. Due to the BST property (Left < Root < Right), this visits nodes in sorted ascending order.",
        "topic": "Tree Traversals"
    },
    {
        "question": "What is the degree of a node in a binary tree?",
        "options": [
            "The number of ancestors.",
            "The number of subtrees (children) it has.",
            "The height of the node.",
            "The total number of nodes in the tree."
        ],
        "correct": 1,
        "explanation": "In tree theory, the degree of a node is the count of its children. For a binary tree, the maximum degree is 2.",
        "topic": "Tree Terminology"
    },
    {
        "question": "Topological sort is only possible for which of the following?",
        "options": [
            "Undirected Graphs",
            "Complete Graphs",
            "Directed Acyclic Graphs (DAGs)",
            "Bipartite Graphs"
        ],
        "correct": 2,
        "explanation": "Only DAGs have a topological sort. Cycles prevent the linear ordering of dependencies.",
        "topic": "Graph Algorithms"
    },
    {
        "question": "Which algorithm is used for finding the shortest path in an unweighted graph?",
        "options": [
            "BFS (Breadth First Search)",
            "DFS (Depth First Search)",
            "Binary Search",
            "Quicksort"
        ],
        "correct": 0,
        "explanation": "BFS explores nodes level by level. In an unweighted graph, the first time you reach a node, it is guaranteed to be via the shortest possible path (least number of edges).",
        "topic": "Graph Traversals"
    },
    {
        "question": "In a Skip List search, you move to the right if the current node's key is ________ than the target key.",
        "options": [
            "Larger",
            "Smaller",
            "Equal",
            "NULL"
        ],
        "correct": 1,
        "explanation": "In a skip list, you traverse the express lanes by moving right as long as the values are smaller than your target. Once you hit a larger value, you drop down one level.",
        "topic": "Skip Lists"
    },
    {
        "question": "What is the time complexity of 'Delete' in a Binary Heap?",
        "options": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "Deletion involves removing the root, replacing it with the last leaf, and percolating down. The distance is limited by the height of the tree (log n).",
        "topic": "Heaps"
    },
    {
        "question": "The Big-O complexity of evaluating a Postfix expression using a stack is:",
        "options": [
            "O(1)",
            "O(n)",
            "O(n^2)",
            "O(log n)"
        ],
        "correct": 1,
        "explanation": "The algorithm scans the expression once (n elements). For each operand, it performs a push; for each operator, it performs two pops and one push. All stack operations are O(1), so total time is linear O(n).",
        "topic": "Stack Applications"
    },
    {
        "question": "In an Adjacency List representation of a Directed Graph with 'V' vertices and 'E' edges, what is the total number of 'Next' pointer fields across all nodes in all linked lists?",
        "options": [
            "V",
            "E",
            "V + E",
            "2E"
        ],
        "correct": 1,
        "explanation": "In a directed graph, each edge (u, v) is represented by exactly one node in the linked list associated with vertex 'u'. Since each node in a linked list contains exactly one 'next' pointer to the subsequent neighbor, the total number of such pointers across the entire structure is equal to the number of edges. For an undirected graph, this would be 2E because each edge is stored twice.",
        "topic": "Graph Representations"
    },
    {
        "question": "Which of the following describes the 'Adjacency Matrix' of a Directed Acyclic Graph (DAG) if the vertices are ordered topologically?",
        "options": [
            "It is a symmetric matrix.",
            "All non-zero entries are located strictly above the main diagonal (Strictly Upper Triangular).",
            "The main diagonal contains only 1s.",
            "It is a sparse matrix with only V edges."
        ],
        "correct": 1,
        "explanation": "If a DAG is topologically sorted, an edge can only exist from a vertex with a lower index to one with a higher index (i < j). In the adjacency matrix, this means Matrix[i,j] can only be 1 if i < j. Consequently, all entries on and below the main diagonal must be zero, making it a strictly upper triangular matrix.",
        "topic": "Graph Algorithms"
    },
    {
        "question": "What is the time complexity to calculate the 'In-degree' of every vertex in a graph represented by an Adjacency Matrix?",
        "options": [
            "O(V)",
            "O(E)",
            "O(V^2)",
            "O(V + E)"
        ],
        "correct": 2,
        "explanation": "To find the in-degree of a single vertex 'j' in an adjacency matrix, you must scan the entire column 'j' (V entries). To do this for all 'V' vertices, you must visit every cell in the V x V matrix, resulting in O(V^2) time complexity. ",
        "topic": "Graph Operations"
    },
    {
        "question": "In the final course summary (Lecture 45), which of the following is described as an important distinction for Graphs compared to other ADTs?",
        "options": [
            "Graphs are always linear structures.",
            "Graphs are primarily important from an algorithmic point of view.",
            "Graphs can only be implemented using arrays.",
            "Graphs are only used in database indexing."
        ],
        "correct": 1,
        "explanation": "The handouts state that while Stacks and Queues are often viewed as basic storage ADTs, Graphs are primarily important from an algorithmic point of view. This means the value of a graph lies in the complex algorithms (like Shortest Path or Minimum Spanning Tree) that operate on it, rather than just simple insertion or retrieval logic.",
        "topic": "Course Summary"
    },
    {
        "question": "Which collision resolution technique in Hashing is specifically designed to avoid 'Primary Clustering' by using non-linear increments?",
        "options": [
            "Linear Probing",
            "Quadratic Probing",
            "Separate Chaining",
            "Rehashing"
        ],
        "correct": 1,
        "explanation": "Primary clustering occurs in Linear Probing because it checks the next sequential index. Quadratic Probing uses an i^2 increment (1, 4, 9...), which allows the algorithm to 'jump' over clusters of occupied cells, though it may still suffer from secondary clustering. ",
        "topic": "Hashing"
    },
    {
        "question": "What is the maximum number of edges in a simple undirected graph with 'n' vertices?",
        "options": [
            "n",
            "n(n-1)",
            "n(n-1) / 2",
            "n^2"
        ],
        "correct": 2,
        "explanation": "In a simple undirected graph, each of the 'n' vertices can be connected to the other 'n-1' vertices. Since the edges are undirected, the edge (u, v) is the same as (v, u), so we divide the total by 2. This is the edge count for a Complete Graph (K_n).",
        "topic": "Graph Theory"
    },
    {
        "question": "Which data structure provides the most efficient average-case performance for a 'Search' operation in a Table ADT?",
        "options": [
            "Linked List",
            "Binary Search Tree",
            "Hash Table",
            "AVL Tree"
        ],
        "correct": 2,
        "explanation": "A well-designed Hash Table provides O(1) average-case search time because the index is calculated directly from the key. While AVL trees provide a guaranteed O(log n), O(1) is theoretically superior for large datasets. ",
        "topic": "Table ADT"
    },
    {
        "question": "In 'Huffman Coding', if we have characters with frequencies: A=10, B=20, C=30, D=40, which character will have the shortest binary code?",
        "options": [
            "A",
            "B",
            "C",
            "D"
        ],
        "correct": 3,
        "explanation": "Huffman Coding uses a greedy strategy where higher-frequency characters are placed closer to the root of the tree. Character 'D' has the highest frequency (40), so it will have the shortest path from the root and, consequently, the shortest bit-code.",
        "topic": "Huffman Coding"
    },
    {
        "question": "What is the height of a 'Complete Binary Tree' with 31 nodes?",
        "options": [
            "4",
            "5",
            "31",
            "15"
        ],
        "correct": 0,
        "explanation": "A complete binary tree with 31 nodes is also a 'Perfect' binary tree (since 31 = 2^5 - 1). Using the formula n = 2^(h+1) - 1, we solve for h: 31 + 1 = 2^(h+1)  32 = 2^5  h+1 = 5  h = 4. ",
        "topic": "Binary Trees"
    },
    {
        "question": "In a B-Tree of order 'm', all internal nodes (except the root) must have at least how many keys?",
        "options": [
            "m / 2",
            "ceil(m / 2)",
            "ceil(m / 2) - 1",
            "m - 1"
        ],
        "correct": 2,
        "explanation": "The B-Tree rule for minimum occupancy is that every node except the root must be at least half-full. A node of order 'm' can have at most m children and m-1 keys. Therefore, it must have at least ceil(m/2) children and ceil(m/2) - 1 keys.",
        "topic": "B-Trees"
    },
    {
        "question": "Which optimization in Disjoint Sets is used to keep the tree 'shallow' by attaching the smaller tree to the root of the larger tree?",
        "options": [
            "Path Compression",
            "Union by Rank (or Size)",
            "Linear Probing",
            "Rehashing"
        ],
        "correct": 1,
        "explanation": "Union by Rank (or Size) prevents the formation of deep, skewed trees by always making the root of the tree with fewer nodes a child of the root of the tree with more nodes. This keeps the tree height logarithmic.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "In 'Double Hashing', if the primary hash is h1(k) and the secondary hash is h2(k), what is the formula for the i-th probe index?",
        "options": [
            "(h1(k) + i + h2(k)) % size",
            "(h1(k) + i * h2(k)) % size",
            "(h1(k) + i^2) % size",
            "(h1(k) * h2(k)) % size"
        ],
        "correct": 1,
        "explanation": "Double Hashing uses h2(k) as the constant step size for a specific key. The probe sequence is (h1(k) + 0*h2(k)), (h1(k) + 1*h2(k)), (h1(k) + 2*h2(k))... all modulo the table size.",
        "topic": "Double Hashing"
    },
    {
        "question": "What is the time complexity to find the 'Maximum' element in a Min-Heap of size 'n'?",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "In a Min-Heap, the minimum is at the root. The maximum element must be one of the leaf nodes. Since there are approximately n/2 leaf nodes, we must perform a linear search through all of them, resulting in O(n) complexity.",
        "topic": "Heap Analysis"
    },
    {
        "question": "According to the course summary, choosing an algorithm to solve a problem often 'brings along' a specific ________.",
        "options": [
            "Hardware requirement",
            "Data structure companion",
            "Programming language",
            "Compiler"
        ],
        "correct": 1,
        "explanation": "Lecture 45 concludes that 'data structure becomes a companion to an algorithm.' For example, solving a shortest-path problem (algorithm) naturally requires a Graph (data structure).",
        "topic": "Course Summary"
    },
    {
        "question": "A graph with no cycles is called:",
        "options": [
            "Complete",
            "Connected",
            "Acyclic",
            "Bipartite"
        ],
        "correct": 2,
        "explanation": "Acyclic simply means 'no cycles'. A Directed Acyclic Graph (DAG) is a specific type used for dependency management.",
        "topic": "Graph Theory"
    },
    {
        "question": "Which sorting algorithm is described as an improvement over Insertion Sort by allowing elements to 'jump' over large distances using a gap?",
        "options": [
            "Bubble Sort",
            "Selection Sort",
            "Shell Sort",
            "Quick Sort"
        ],
        "correct": 2,
        "explanation": "Shell Sort uses a sequence of 'gaps' to perform insertion sorts on sub-lists. This allows elements to move closer to their destination much faster than the single-step shifts in standard Insertion Sort.",
        "topic": "Shell Sort"
    },
    {
        "question": "In an Adjacency Matrix, if Matrix[i,j] = 1, it implies there is an edge from vertex i to vertex j. What does the sum of 'Column j' represent?",
        "options": [
            "Indegree of vertex j",
            "Outdegree of vertex j",
            "Total number of vertices",
            "Number of cycles"
        ],
        "correct": 0,
        "explanation": "Summing Column 'j' counts all entries where 'j' is the destination (the 'to' vertex). This is the definition of Indegree.",
        "topic": "Adjacency Matrix"
    },
    {
        "question": "Which of the following data structures follows the FIFO (First-In, First-Out) principle?",
        "options": [
            "Stack",
            "Queue",
            "Priority Queue",
            "Binary Search Tree"
        ],
        "correct": 1,
        "explanation": "Queues are the standard FIFO data structure. Priority Queues (Option 2) are based on urgency, not just arrival order.",
        "topic": "Queue ADT"
    },
    {
        "question": "In a BST, which traversal results in elements being visited in ascending order?",
        "options": [
            "Pre-order",
            "In-order",
            "Post-order",
            "Level-order"
        ],
        "correct": 1,
        "explanation": "In-order traversal visits: Left Subtree -> Root -> Right Subtree. Due to the BST property, this naturally sorts the data. ",
        "topic": "Tree Traversals"
    },
    {
        "question": "A 'Strongly Connected' directed graph is one where:",
        "options": [
            "Every node is connected to a central root.",
            "A path exists from every vertex to every other vertex.",
            "The graph has no edges.",
            "There are no cycles."
        ],
        "correct": 1,
        "explanation": "Strong connectivity in a digraph means you can get from any node A to any node B following the directions of the edges.",
        "topic": "Graph Theory"
    },
    {
        "question": "The amortized complexity of 'Find' in Disjoint Sets with Path Compression is:",
        "options": [
            "O(1)",
            "O(log n)",
            "Nearly O(1) (Inverse Ackermann function)",
            "O(n)"
        ],
        "correct": 2,
        "explanation": "Path compression flattens the tree so significantly that the operations become nearly constant-time (represented by (n)).",
        "topic": "Disjoint Sets"
    },
    {
        "question": "Which rotation is performed in an AVL tree for the 'Left-Right' (zigzag) case?",
        "options": [
            "Single Left Rotation",
            "Single Right Rotation",
            "Double LR Rotation",
            "Double RL Rotation"
        ],
        "correct": 2,
        "explanation": "An LR imbalance requires a Left rotation on the child followed by a Right rotation on the parent node. ",
        "topic": "AVL Trees"
    },
    {
        "question": "The 'Load Factor'  of a hash table with 100 slots and 70 elements is:",
        "options": [
            "0.3",
            "0.7",
            "1.43",
            "70"
        ],
        "correct": 1,
        "explanation": " = n/m = 70/100 = 0.7.",
        "topic": "Hashing"
    },
    {
        "question": "What is the time complexity of 'Topological Sort' on a graph with V vertices and E edges?",
        "options": [
            "O(V)",
            "O(E)",
            "O(V + E)",
            "O(V^2)"
        ],
        "correct": 2,
        "explanation": "Topological sort (using Kahn's algorithm or DFS) processes each vertex and edge once, making it a linear time algorithm.",
        "topic": "Graph Algorithms"
    },
    {
        "question": "In a Max-Heap, the root node contains the ________ element.",
        "options": [
            "Smallest",
            "Largest",
            "Middle",
            "Last inserted"
        ],
        "correct": 1,
        "explanation": "The Max-Heap property ensures the parent is always greater than or equal to its children, placing the maximum at the root.",
        "topic": "Heaps"
    },
    {
        "question": "Which of the following is true for 'Skip Lists'?",
        "options": [
            "They are a type of Binary Tree.",
            "Search time is O(log n) on average.",
            "They use rotations to balance levels.",
            "They only work for unsorted data."
        ],
        "correct": 1,
        "explanation": "Skip Lists are probabilistic, multi-level linked lists that offer logarithmic search performance on average.",
        "topic": "Skip Lists"
    },
    {
        "question": "A 'Perfect' Binary Tree has how many nodes if its height is 2?",
        "options": [
            "3",
            "7",
            "15",
            "4"
        ],
        "correct": 1,
        "explanation": "Nodes = 2^(h+1) - 1. For h=2, Nodes = 2^(3) - 1 = 7.",
        "topic": "Binary Trees"
    },
    {
        "question": "In Hashing, the 'Division Method' formula is:",
        "options": [
            "h(k) = k * size",
            "h(k) = k % size",
            "h(k) = k / size",
            "h(k) = size % k"
        ],
        "correct": 1,
        "explanation": "The division method maps a key k into the array by taking the remainder of k divided by the table size.",
        "topic": "Hashing"
    },
    {
        "question": "Which data structure is best for checking 'Balanced Parentheses' in an expression?",
        "options": [
            "Queue",
            "Stack",
            "Heap",
            "B-Tree"
        ],
        "correct": 1,
        "explanation": "Stacks are LIFO, making them ideal for matching the last opened parenthesis with the first incoming closing parenthesis.",
        "topic": "Stack Applications"
    },
    {
        "question": "A B-Tree is specifically optimized for which environment?",
        "options": [
            "Small in-memory caches",
            "Recursive functional programming",
            "Secondary storage (disks) and databases",
            "Sorting small arrays"
        ],
        "correct": 2,
        "explanation": "B-Trees have a large branching factor to minimize the height and reduce disk 'seeks' (I/O operations). ",
        "topic": "B-Trees"
    },
    {
        "question": "What is the degree of a leaf node in a tree?",
        "options": [
            "1",
            "0",
            "Variable",
            "2"
        ],
        "correct": 1,
        "explanation": "In tree terminology, degree is the number of children. Leaves have no children, so their degree is 0.",
        "topic": "Tree Terminology"
    },
    {
        "question": "In a Max-Heap, what is the array index of the left child of node 'i'?",
        "options": [
            "2i",
            "2i + 1",
            "i / 2",
            "i + 1"
        ],
        "correct": 0,
        "explanation": "Using 1-based indexing for a complete binary tree, the left child of i is at 2i.",
        "topic": "Heap Implementation"
    },
    {
        "question": "Topological Sort is only possible for graphs that are:",
        "options": [
            "Undirected",
            "Acyclic",
            "Complete",
            "DAGs (Directed Acyclic Graphs)"
        ],
        "correct": 3,
        "explanation": "Topological sorting requires directed edges to determine precedence and an absence of cycles to avoid circular dependencies.",
        "topic": "Graph Algorithms"
    },
    {
        "question": "Which Big-O notation represents logarithmic time?",
        "options": [
            "O(n)",
            "O(n log n)",
            "O(log n)",
            "O(1)"
        ],
        "correct": 2,
        "explanation": "O(log n) is the notation for logarithmic time complexity.",
        "topic": "Complexity Analysis"
    },
    {
        "question": "What is the time complexity to find an element in a balanced BST?",
        "options": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n log n)"
        ],
        "correct": 1,
        "explanation": "A balanced BST (like AVL) ensures the search path is always logarithmic.",
        "topic": "BST Analysis"
    },
    {
        "question": "Which of the following describes the 'Topological Sort' of a Directed Acyclic Graph (DAG)?",
        "options": [
            "A linear ordering of vertices such that for every directed edge (u, v), vertex u comes before v.",
            "A sorting method that arranges vertices based on their degree in descending order.",
            "A traversal that visits nodes level-by-level using a priority queue.",
            "A mechanism to find the shortest path in a weighted undirected graph."
        ],
        "correct": 0,
        "explanation": "Topological sorting is used for scheduling tasks with dependencies. It produces a sequence where all 'prerequisite' nodes (u) appear before the 'dependent' nodes (v). This is only possible in a DAG because cycles would create a circular dependency that cannot be linearized. Options 1, 2, and 3 describe different sorting or traversal logic not applicable to topological ordering.",
        "topic": "Graph Algorithms"
    },
    {
        "question": "What is the time complexity to find the 'Indegree' of all vertices in a directed graph represented by an Adjacency Matrix?",
        "options": [
            "O(V)",
            "O(E)",
            "O(V + E)",
            "O(V^2)"
        ],
        "correct": 3,
        "explanation": "In an adjacency matrix, the 'indegree' of a vertex 'j' is found by summing all the entries in column 'j'. To calculate this for all 'V' vertices, the algorithm must inspect every single cell in the V x V matrix, resulting in O(V^2) complexity. In contrast, an adjacency list would allow this in O(V + E).",
        "topic": "Adjacency Matrix"
    },
    {
        "question": "In the context of topological sorting, what is a vertex with an 'Indegree' of zero?",
        "options": [
            "A vertex that has no outgoing edges.",
            "A vertex that has no incoming edges and can be processed immediately.",
            "A vertex that is part of a cycle.",
            "A leaf node in a binary tree."
        ],
        "correct": 1,
        "explanation": "Kahns algorithm for topological sorting begins by identifying vertices with an indegree of 0. These vertices have no dependencies (no edges pointing to them) and are thus the first candidates to be placed in the sorted sequence. Once processed, their outgoing edges are 'removed' to update the indegrees of their neighbors.",
        "topic": "Graph Algorithms"
    },
    {
        "question": "Which data structure is primarily recommended for finding the 'Shortest Path' in an unweighted graph?",
        "options": [
            "Stack",
            "Queue (using BFS)",
            "Binary Heap",
            "Hash Table"
        ],
        "correct": 1,
        "explanation": "Breadth-First Search (BFS) explores a graph level by level. In an unweighted graph, the first time a vertex is reached via BFS, it is guaranteed to be the shortest path from the source in terms of the number of edges. A Queue is the mandatory ADT to manage this level-order exploration.",
        "topic": "Graph Traversals"
    },
    {
        "question": "According to the course summary in Lecture 45, why are Graphs considered primarily important from an 'algorithmic' point of view?",
        "options": [
            "Because they are the simplest to implement.",
            "Because their value lies in the complex problems they solve, such as networking and pathfinding.",
            "Because they do not require any memory allocation.",
            "Because they are a specific type of Linear Linked List."
        ],
        "correct": 1,
        "explanation": "The final lecture of CS301 emphasizes that unlike basic ADTs like stacks or queues, Graphs are vital because of the algorithms that run on them (like Dijkstra's or Prim's). They are used to model real-world networks where the relationships and paths between data points are the primary focus.",
        "topic": "Course Summary"
    },
    {
        "question": "An undirected graph is 'Connected' if:",
        "options": [
            "Every vertex has a degree of at least 1.",
            "There is a path between every pair of vertices.",
            "It is a complete graph.",
            "It has no cycles."
        ],
        "correct": 1,
        "explanation": "Connectivity in an undirected graph means that no vertex is isolated; you can travel from any node to any other node through a sequence of edges. A graph that is not connected is divided into multiple 'Connected Components'.",
        "topic": "Graph Theory"
    },
    {
        "question": "In an Adjacency List for a directed graph, the sum of the lengths of all linked lists is exactly equal to:",
        "options": [
            "V",
            "2E",
            "E",
            "V + E"
        ],
        "correct": 2,
        "explanation": "In a directed graph, each edge (u, v) is stored exactly oncein the list for vertex 'u'. Therefore, the total number of nodes in all linked lists equals the number of edges 'E'. For an undirected graph, this sum would be '2E' because each edge is stored for both endpoints.",
        "topic": "Adjacency List"
    },
    {
        "question": "Which of the following growth rates is the least efficient for large input sizes?",
        "options": [
            "O(n log n)",
            "O(n^2)",
            "O(2^n)",
            "O(n!)"
        ],
        "correct": 3,
        "explanation": "O(n!) or Factorial growth is the most explosive and least efficient, often seen in brute-force solutions to problems like the Traveling Salesperson Problem. O(2^n) is exponential, but still grows slower than factorial for large n.",
        "topic": "Complexity Analysis"
    },
    {
        "question": "What is the primary disadvantage of using an Adjacency Matrix for a 'Sparse' graph?",
        "options": [
            "Finding if an edge exists takes O(V).",
            "It consumes O(V^2) space, wasting memory on zeros.",
            "It cannot store edge weights.",
            "It cannot represent directed graphs."
        ],
        "correct": 1,
        "explanation": "A sparse graph has few edges (E << V^2). An adjacency matrix allocates a square of memory (V x V) regardless of edges. If most entries are 0, this is highly inefficient. Adjacency lists are preferred in such cases as they only store existing edges.",
        "topic": "Graph Representations"
    },
    {
        "question": "In 'Huffman Coding', the resulting binary tree is a ________.",
        "options": [
            "Binary Search Tree",
            "Strictly Binary Tree",
            "AVL Tree",
            "B-Tree"
        ],
        "correct": 1,
        "explanation": "A Huffman tree is a strictly binary tree (or full binary tree), meaning every internal node has exactly two children. This structure is required to ensure that every character (leaf) has a unique prefix-free binary code.",
        "topic": "Huffman Coding"
    },
    {
        "question": "Which sorting algorithm is characterized by an 'In-place' O(1) space complexity and an average-case O(n log n) time complexity?",
        "options": [
            "Merge Sort",
            "Quick Sort",
            "Insertion Sort",
            "Bubble Sort"
        ],
        "correct": 1,
        "explanation": "Quick Sort is an in-place algorithm as it sorts the data within the original array with only O(log n) stack space for recursion (ignoring the input size). Merge Sort (Option 0) is not in-place as it requires an O(n) auxiliary array.",
        "topic": "Sorting"
    },
    {
        "question": "In a B-Tree of order 'm', the root node must have at least ________ children if it is not a leaf.",
        "options": [
            "m/2",
            "ceil(m/2)",
            "2",
            "m-1"
        ],
        "correct": 2,
        "explanation": "The root of a B-Tree is the only node exempt from the 'half-full' rule. While other internal nodes must have at least ceil(m/2) children, the root only needs a minimum of 2 children to establish a tree structure.",
        "topic": "B-Trees"
    },
    {
        "question": "The process of 'Path Compression' in Disjoint Sets is typically implemented during which operation?",
        "options": [
            "Union",
            "Find",
            "Initialization",
            "Delete"
        ],
        "correct": 1,
        "explanation": "Path compression flattens the tree by making all nodes along the search path point directly to the root. This is performed during the 'Find' operation as it traverses up to the representative, speeding up future searches for those nodes.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "Which hash table collision resolution technique is most likely to suffer from 'Primary Clustering'?",
        "options": [
            "Linear Probing",
            "Quadratic Probing",
            "Double Hashing",
            "Separate Chaining"
        ],
        "correct": 0,
        "explanation": "Linear probing checks the next sequential slot. This causes occupied slots to clump together into 'clusters'. As these clusters grow, the probability of hashing into them increases, further degrading performance.",
        "topic": "Hashing"
    },
    {
        "question": "What is the maximum number of children in a node of a B-Tree of order 5?",
        "options": [
            "4",
            "5",
            "6",
            "2"
        ],
        "correct": 1,
        "explanation": "In a B-Tree, the 'order' m defines the maximum number of children a node can have. Therefore, a node of order 5 can have at most 5 children and 4 keys.",
        "topic": "B-Trees"
    },
    {
        "question": "In a 'Max-Heap', where is the minimum element located?",
        "options": [
            "At the root.",
            "At index 1.",
            "In one of the leaf nodes.",
            "In the middle of the array."
        ],
        "correct": 2,
        "explanation": "The Max-Heap property ensures the parent is larger than the children. This means the minimum value can never be a parent. It must reside in one of the leaf nodes (the bottom level of the tree).",
        "topic": "Heaps"
    },
    {
        "question": "What is the height of a perfect binary tree with 7 nodes?",
        "options": [
            "2",
            "3",
            "7",
            "1"
        ],
        "correct": 0,
        "explanation": "Using the formula n = 2^(h+1) - 1 for a perfect tree: 7 = 2^(h+1) - 1 => 8 = 2^(h+1) => 2^3 = 2^(h+1). Thus h+1 = 3, and height h = 2.",
        "topic": "Binary Trees"
    },
    {
        "question": "Which data structure is a 'probabilistic alternative' to balanced search trees?",
        "options": [
            "Hash Table",
            "Skip List",
            "Threaded Binary Tree",
            "B-Tree"
        ],
        "correct": 1,
        "explanation": "Skip Lists use randomization (coin flips) to determine the levels of nodes. Statistically, they achieve O(log n) search and update times similar to AVL trees but are often easier to implement without complex rotations.",
        "topic": "Skip Lists"
    },
    {
        "question": "In a directed graph, a 'Cycle' is a path that:",
        "options": [
            "Visits every node exactly once.",
            "Starts and ends at the same vertex.",
            "Has no edges.",
            "Connects two different components."
        ],
        "correct": 1,
        "explanation": "A cycle is a path of length at least 1 that starts and ends at the same vertex. A graph without such paths is 'Acyclic'.",
        "topic": "Graph Theory"
    },
    {
        "question": "Which traversal of a Binary Search Tree (BST) visits nodes in ascending sorted order?",
        "options": [
            "Pre-order",
            "Post-order",
            "In-order",
            "Level-order"
        ],
        "correct": 2,
        "explanation": "In-order traversal visits: Left Subtree -> Root -> Right Subtree. Since a BST is defined by Left < Root < Right, this traversal visits keys in non-decreasing order.",
        "topic": "Tree Traversals"
    },
    {
        "question": "The 'Load Factor'  in a hash table is defined as:",
        "options": [
            "m / n",
            "n / m",
            "n + m",
            "n * m"
        ],
        "correct": 1,
        "explanation": "Load factor  is the ratio of the number of elements (n) to the table size (m). It is used to measure how full the table is and determine when to rehash.",
        "topic": "Hashing"
    },
    {
        "question": "In a Max-Heap, the parent of a node at index 'i' (1-based indexing) is at:",
        "options": [
            "2i",
            "2i + 1",
            "floor(i / 2)",
            "i - 1"
        ],
        "correct": 2,
        "explanation": "Using the array representation of a complete binary tree, if a child is at index i, its parent is at floor(i/2).",
        "topic": "Heaps"
    },
    {
        "question": "Which rotation is required in an AVL tree when the 'Left-Right' (LR) case occurs?",
        "options": [
            "Single Right Rotation",
            "Single Left Rotation",
            "Double LR Rotation",
            "Double RL Rotation"
        ],
        "correct": 2,
        "explanation": "An LR imbalance occurs when a node is added to the right subtree of the left child. It requires a double rotation: first a Left rotation on the child, then a Right rotation on the parent.",
        "topic": "AVL Trees"
    },
    {
        "question": "What is the time complexity to find an element in an AVL tree?",
        "options": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "Because AVL trees are strictly balanced, their height is always logarithmic (h  1.44 log n). A search takes time proportional to the height, making it O(log n).",
        "topic": "AVL Analysis"
    },
    {
        "question": "In 'Double Hashing', the step size is determined by:",
        "options": [
            "A constant value (usually 1).",
            "A second hash function h2(key).",
            "The value of the previous hash.",
            "The current number of collisions."
        ],
        "correct": 1,
        "explanation": "To eliminate secondary clustering, double hashing uses a second hash function to determine the jump distance. This ensures that even if two keys collide at the first index, they will likely follow different probe sequences.",
        "topic": "Double Hashing"
    },
    {
        "question": "The Standard Template Library (STL) 'vector' is a dynamic implementation of a ________.",
        "options": [
            "Linked List",
            "Stack",
            "Array",
            "Queue"
        ],
        "correct": 2,
        "explanation": "A vector is a template class that manages a contiguous block of memory (an array) that can resize itself automatically when it becomes full.",
        "topic": "STL Containers"
    },
    {
        "question": "What is the degree of a node in a Binary Tree?",
        "options": [
            "Number of ancestors.",
            "Number of children (0, 1, or 2).",
            "Total number of nodes.",
            "Height of the tree."
        ],
        "correct": 1,
        "explanation": "The degree of a node is defined as the number of subtrees (children) attached to it.",
        "topic": "Tree Terminology"
    },
    {
        "question": "Which of the following is NOT a linear data structure?",
        "options": [
            "Stack",
            "Queue",
            "Linked List",
            "Graph"
        ],
        "correct": 3,
        "explanation": "Graphs and Trees are non-linear data structures because they allow for hierarchical or networked relationships rather than a simple sequential order.",
        "topic": "Data Structure Classification"
    },
    {
        "question": "In a B-Tree, all leaf nodes must be at:",
        "options": [
            "Level 0.",
            "The same level.",
            "The first level.",
            "Variable depths."
        ],
        "correct": 1,
        "explanation": "A B-Tree is perfectly balanced. One of its strict properties is that every leaf node in the tree must be at the exact same depth from the root.",
        "topic": "B-Trees"
    },
    {
        "question": "A complete binary tree with 10 nodes has how many leaf nodes?",
        "options": [
            "5",
            "6",
            "4",
            "10"
        ],
        "correct": 0,
        "explanation": "In a complete binary tree with n nodes, the leaves start from index floor(n/2)+1 to n. For n=10, floor(10/2)+1 = 6. Nodes at indices 6, 7, 8, 9, 10 are leaves. Total = 5.",
        "topic": "Binary Tree Properties"
    },
    {
        "question": "Which operation is used to merge two disjoint sets into one?",
        "options": [
            "Find",
            "Union",
            "Split",
            "Search"
        ],
        "correct": 1,
        "explanation": "The 'Union' operation takes two elements, finds their respective set representatives (roots), and makes one root the child of the other.",
        "topic": "Disjoint Sets"
    },
    {
        "question": "What is the result of the 'Division Method' for hashing if the key is 25 and table size is 7?",
        "options": [
            "3",
            "4",
            "25",
            "1"
        ],
        "correct": 1,
        "explanation": "h(k) = k % m => 25 % 7 = 4.",
        "topic": "Hashing"
    },
    {
        "question": "A graph whose vertices can be divided into two disjoint sets such that no two vertices within the same set are adjacent is called a ________.",
        "options": [
            "Complete Graph",
            "Bipartite Graph",
            "Directed Graph",
            "Connected Graph"
        ],
        "correct": 1,
        "explanation": "Bipartite graphs are used to model relationships between two different types of objects, like students and courses.",
        "topic": "Graph Types"
    },
    {
        "question": "What is the time complexity to insert an element into an AVL tree?",
        "options": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ],
        "correct": 2,
        "explanation": "Insertion in an AVL tree involves finding the spot (O(log n)), inserting the node, and potentially rotating up the path (O(log n)). Total time is logarithmic.",
        "topic": "AVL Analysis"
    },
    {
        "question": "According to Lecture 45, what is the 'companion' of an algorithm?",
        "options": [
            "The Compiler",
            "The User",
            "The Data Structure",
            "The Code"
        ],
        "correct": 2,
        "explanation": "As noted in the final course review, 'data structure becomes a companion to an algorithm.' Choosing an algorithm to solve a problem naturally forces the choice of an appropriate data structure.",
        "topic": "Course Summary"
    }
]